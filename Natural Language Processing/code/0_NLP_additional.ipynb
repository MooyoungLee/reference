{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating extra NLP features\n",
    "\n",
    "- **Spell error ratio** using 'autocorrect' package\n",
    "- **Polarity** and **subjectivity** from TextBlob\n",
    "- Train Wikipedia comments:  **['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']**\n",
    "- Readability: **Flesch-Kincaid** and **Automated Readability Index**\n",
    "- **Spam** classfication probability is added using a Naive Bayes model trained w/1600-spam-corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files\n",
    "\n",
    "### Previous data set w/ Stanford NLP features will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import spell\n",
    "from textblob import TextBlob\n",
    "import re, string\n",
    "from readability_score.calculators.fleschkincaid import *\n",
    "from readability_score.calculators.ari import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "# try:\n",
    "#     sys.getwindowsversion()\n",
    "# except AttributeError:\n",
    "#     isWindows = False\n",
    "# else:\n",
    "#     isWindows = True\n",
    "#use more RAM\n",
    "# if isWindows:\n",
    "#     import win32api,win32process,win32con\n",
    "#     pid = win32api.GetCurrentProcessId()\n",
    "#     handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, True, pid)\n",
    "#     win32process.SetPriorityClass(handle, win32process.HIGH_PRIORITY_CLASS)\n",
    "regular_reviews = pd.read_csv('data/reg_reviews_NLP.csv')\n",
    "not_recommended_reviews= pd.read_csv('data/not_reviews_NLP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230530"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(regular_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27057"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_recommended_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>friends</th>\n",
       "      <th>photos</th>\n",
       "      <th>rating</th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>numSentence</th>\n",
       "      <th>numWords</th>\n",
       "      <th>totSentiment</th>\n",
       "      <th>avgSentiment</th>\n",
       "      <th>Sfreq0</th>\n",
       "      <th>...</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>readability_FK</th>\n",
       "      <th>readability_AR</th>\n",
       "      <th>spam</th>\n",
       "      <th>toxic.1</th>\n",
       "      <th>severe_toxic.1</th>\n",
       "      <th>obscene.1</th>\n",
       "      <th>threat.1</th>\n",
       "      <th>insult.1</th>\n",
       "      <th>identity_hate.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "      <td>27057.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.113058</td>\n",
       "      <td>12.326829</td>\n",
       "      <td>4.122815</td>\n",
       "      <td>339.546993</td>\n",
       "      <td>17.022212</td>\n",
       "      <td>5.193961</td>\n",
       "      <td>62.696714</td>\n",
       "      <td>10.962856</td>\n",
       "      <td>2.299413</td>\n",
       "      <td>0.059208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>8.843109</td>\n",
       "      <td>9.733636</td>\n",
       "      <td>0.539512</td>\n",
       "      <td>0.033635</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.008331</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>142.656716</td>\n",
       "      <td>282.878244</td>\n",
       "      <td>1.286370</td>\n",
       "      <td>186.130745</td>\n",
       "      <td>57.924208</td>\n",
       "      <td>4.921907</td>\n",
       "      <td>75.617481</td>\n",
       "      <td>9.043593</td>\n",
       "      <td>0.694986</td>\n",
       "      <td>0.272098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>4.404453</td>\n",
       "      <td>4.938015</td>\n",
       "      <td>0.151680</td>\n",
       "      <td>0.083904</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.041856</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.025203</td>\n",
       "      <td>0.001806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086098</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.439331</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.520460</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.000613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.622585</td>\n",
       "      <td>0.027332</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>24931.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>665.000000</td>\n",
       "      <td>2835.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>942.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184950</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>0.999493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.247176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022564</td>\n",
       "      <td>0.984059</td>\n",
       "      <td>0.184952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            friends        photos        rating  restaurant_id       reviews  \\\n",
       "count  27057.000000  27057.000000  27057.000000   27057.000000  27057.000000   \n",
       "mean      33.113058     12.326829      4.122815     339.546993     17.022212   \n",
       "std      142.656716    282.878244      1.286370     186.130745     57.924208   \n",
       "min        0.000000      0.000000      1.000000       0.000000      1.000000   \n",
       "25%        0.000000      0.000000      4.000000     207.000000      2.000000   \n",
       "50%        0.000000      0.000000      5.000000     326.000000      5.000000   \n",
       "75%       11.000000      1.000000      5.000000     512.000000     11.000000   \n",
       "max     5000.000000  24931.000000      5.000000     665.000000   2835.000000   \n",
       "\n",
       "        numSentence      numWords  totSentiment  avgSentiment        Sfreq0  \\\n",
       "count  27057.000000  27057.000000  27057.000000  27057.000000  27057.000000   \n",
       "mean       5.193961     62.696714     10.962856      2.299413      0.059208   \n",
       "std        4.921907     75.617481      9.043593      0.694986      0.272098   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        2.000000     19.000000      5.000000      1.833333      0.000000   \n",
       "50%        4.000000     39.000000      9.000000      2.333333      0.000000   \n",
       "75%        6.000000     78.000000     14.000000      3.000000      0.000000   \n",
       "max      101.000000    942.000000    202.000000      4.000000      5.000000   \n",
       "\n",
       "            ...         identity_hate  readability_FK  readability_AR  \\\n",
       "count       ...          27057.000000    27057.000000    27057.000000   \n",
       "mean        ...              0.000897        8.843109        9.733636   \n",
       "std         ...              0.001806        4.404453        4.938015   \n",
       "min         ...              0.000030        0.000000        0.000000   \n",
       "25%         ...              0.000439        7.000000        7.000000   \n",
       "50%         ...              0.000613        9.000000        9.000000   \n",
       "75%         ...              0.000954       10.000000       11.000000   \n",
       "max         ...              0.184950      437.000000      208.000000   \n",
       "\n",
       "               spam       toxic.1  severe_toxic.1     obscene.1      threat.1  \\\n",
       "count  27057.000000  27057.000000    27057.000000  27057.000000  27057.000000   \n",
       "mean       0.539512      0.033635        0.000657      0.008331      0.000220   \n",
       "std        0.151680      0.083904        0.002269      0.041856      0.000334   \n",
       "min        0.086098      0.000010        0.000027      0.000055      0.000033   \n",
       "25%        0.439331      0.008095        0.000316      0.002861      0.000121   \n",
       "50%        0.520460      0.014025        0.000434      0.004017      0.000156   \n",
       "75%        0.622585      0.027332        0.000629      0.006080      0.000230   \n",
       "max        0.999493      1.000000        0.247176      1.000000      0.022564   \n",
       "\n",
       "           insult.1  identity_hate.1  \n",
       "count  27057.000000     27057.000000  \n",
       "mean       0.008097         0.000897  \n",
       "std        0.025203         0.001806  \n",
       "min        0.000080         0.000030  \n",
       "25%        0.002902         0.000439  \n",
       "50%        0.004393         0.000613  \n",
       "75%        0.007328         0.000954  \n",
       "max        0.984059         0.184952  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_recommended_reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spellErrorRatio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filter for punctuations\n",
    "punctuation = re.compile(r'[0-9]')\n",
    "\n",
    "# Create a regular expression tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Get the list of stop words \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def spellErrorRatio(input_text):\n",
    "\n",
    "    # Remove the numbers\n",
    "    input_text = punctuation.sub(\"\",input_text)\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "    # Remove the stop words \n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "    \n",
    "    # Check spelling error ratio\n",
    "    if len(tokens)>1:\n",
    "        cntWrong = 0\n",
    "        for i, item in enumerate(tokens):\n",
    "            if(tokens[i]!=spell(item)):\n",
    "                cntWrong += 1\n",
    "    #             print(tokens[i], spell(item))  # check which word is wrong.\n",
    "        return (cntWrong/len(tokens))\n",
    "    else:\n",
    "        return (np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate spell error ratio on both files and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hours processed for one file: 3.2670867806010775\n",
      "total hours processed: 3.5838604907857046\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "regular_reviews['spellErrorRatio'] = regular_reviews['review'].apply(lambda x: spellErrorRatio(x))\n",
    "regular_reviews.to_csv('data/regular_reviews_spell.csv', index=False)\n",
    "print('hours processed for one file:', (time.time()-t0)/3600)\n",
    "\n",
    "not_recommended_reviews['spellErrorRatio'] = not_recommended_reviews['review'].apply(lambda x: spellErrorRatio(x))\n",
    "not_recommended_reviews.to_csv('data/not_reviews_spell.csv', index=False)\n",
    "print('total hours processed:', (time.time()-t0)/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Features using 'Text Blob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for regular review file\n",
    "# regular_reviews['TB_polarity'] = regular_reviews.review.apply(lambda x:TextBlob(x).polarity)\n",
    "# regular_reviews['TB_subjectivity'] = regular_reviews.review.apply(lambda x:TextBlob(x).subjectivity)\n",
    "# regular_reviews.to_csv('data/regular_reviews_TB.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for not-recommended review file\n",
    "# not_recommended_reviews['TB_polarity'] = not_recommended_reviews.review.apply(lambda x:TextBlob(x).polarity)\n",
    "# not_recommended_reviews['TB_subjectivity'] = not_recommended_reviews.review.apply(lambda x:TextBlob(x).subjectivity)\n",
    "# not_recommended_reviews.to_csv('data/not_reviews_TB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "### Source: Kaggle/ 'Toxic Comment Classification Challenge'\n",
    "Kaggle objective: Detecting toxic comments that are rude, disrespectful or otherwise likely to make someone leave a discussion.\n",
    "\n",
    "Kaggle supporter: Conversation AI team by Jigsaw and Google.\n",
    "\n",
    "### Train data: Wikipedia comments that are labeled by human raters for toxic behavior.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate           none  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "train = pd.read_csv('data/train_Toxic.csv')\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[label_cols].max(axis=1)\n",
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting existing columns \n",
    "regular_reviews.drop(label_cols, inplace = True, axis=1)\n",
    "not_recommended_reviews.drop(label_cols, inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  none  \n",
       "0             0        0       0       0              0     1  \n",
       "1             0        0       0       0              0     1  \n",
       "2             0        0       0       0              0     1  \n",
       "3             0        0       0       0              0     1  \n",
       "4             0        0       0       0              0     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "def pr(x,y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "def get_mdl(x,y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(x,1,y) / pr(x,0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model cross-validation w/ Stratified k-fold (within Toxic data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Stratified 3-Ford Cross-Validation for  toxic \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98     48093\n",
      "          1       0.90      0.69      0.78      5098\n",
      "\n",
      "avg / total       0.96      0.96      0.96     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98     48092\n",
      "          1       0.90      0.69      0.78      5098\n",
      "\n",
      "avg / total       0.96      0.96      0.96     53190\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98     48092\n",
      "          1       0.91      0.68      0.78      5098\n",
      "\n",
      "avg / total       0.96      0.96      0.96     53190\n",
      "\n",
      "Model Stratified 3-Ford Cross-Validation for  severe_toxic \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     52659\n",
      "          1       0.48      0.26      0.34       532\n",
      "\n",
      "avg / total       0.99      0.99      0.99     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     52659\n",
      "          1       0.51      0.28      0.36       532\n",
      "\n",
      "avg / total       0.99      0.99      0.99     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     52658\n",
      "          1       0.58      0.27      0.37       531\n",
      "\n",
      "avg / total       0.99      0.99      0.99     53189\n",
      "\n",
      "Model Stratified 3-Ford Cross-Validation for  obscene \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     50374\n",
      "          1       0.88      0.71      0.79      2817\n",
      "\n",
      "avg / total       0.98      0.98      0.98     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     50374\n",
      "          1       0.89      0.70      0.78      2816\n",
      "\n",
      "avg / total       0.98      0.98      0.98     53190\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99     50374\n",
      "          1       0.89      0.72      0.79      2816\n",
      "\n",
      "avg / total       0.98      0.98      0.98     53190\n",
      "\n",
      "Model Stratified 3-Ford Cross-Validation for  threat \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     53031\n",
      "          1       0.73      0.23      0.34       160\n",
      "\n",
      "avg / total       1.00      1.00      1.00     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     53031\n",
      "          1       0.60      0.26      0.36       159\n",
      "\n",
      "avg / total       1.00      1.00      1.00     53190\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     53031\n",
      "          1       0.87      0.29      0.43       159\n",
      "\n",
      "avg / total       1.00      1.00      1.00     53190\n",
      "\n",
      "Model Stratified 3-Ford Cross-Validation for  insult \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     50565\n",
      "          1       0.80      0.59      0.68      2626\n",
      "\n",
      "avg / total       0.97      0.97      0.97     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     50565\n",
      "          1       0.80      0.58      0.67      2626\n",
      "\n",
      "avg / total       0.97      0.97      0.97     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99     50564\n",
      "          1       0.84      0.57      0.68      2625\n",
      "\n",
      "avg / total       0.97      0.97      0.97     53189\n",
      "\n",
      "Model Stratified 3-Ford Cross-Validation for  identity_hate \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     52722\n",
      "          1       0.65      0.26      0.37       469\n",
      "\n",
      "avg / total       0.99      0.99      0.99     53191\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     52722\n",
      "          1       0.70      0.25      0.37       468\n",
      "\n",
      "avg / total       0.99      0.99      0.99     53190\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     52722\n",
      "          1       0.66      0.26      0.38       468\n",
      "\n",
      "avg / total       0.99      0.99      0.99     53190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "\n",
    "    X = train[COMMENT]\n",
    "    y = train[j]\n",
    "    skf.get_n_splits(X,y)\n",
    "\n",
    "    print('Model Stratified 3-Ford Cross-Validation for ', j,'\\n')\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train0, X_test0 = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train = vec.fit_transform(X_train0)\n",
    "        X_test = vec.transform(X_test0)\n",
    "\n",
    "        m,r = get_mdl(X_train, y_train)\n",
    "        preds = m.predict_proba(X_test.multiply(r))[:,1]\n",
    "\n",
    "        print(classification_report(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Model w/ all Toxic Train Data and Apply to Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Fit to recommended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "n = train.shape[0]\n",
    "\n",
    "x = vec.fit_transform(train[COMMENT])\n",
    "\n",
    "test_x = vec.transform(regular_reviews['review'])\n",
    "\n",
    "preds = np.zeros((len(regular_reviews), len(label_cols)))\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_mdl(x, train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "    \n",
    "regular_reviews = pd.concat([regular_reviews, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "# regular_reviews.to_csv('data/regular_NBSVM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Fit to not-recommended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "test_x = vec.transform(not_recommended_reviews['review'])\n",
    "\n",
    "preds = np.zeros((len(not_recommended_reviews), len(label_cols)))\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    m,r = get_mdl(x, train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "\n",
    "not_recommended_reviews = pd.concat([not_recommended_reviews, pd.DataFrame(preds, columns = label_cols)], axis=1)\n",
    "# not_recommended_reviews.to_csv('data/not_reviews_NBSVM.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Readability\n",
    "\n",
    "There are multiple readability score formulas as shown below.  They utilize count values of words, sentence, cyllables, and/or characters.  Each formular varies slightly, and gives out grade level who can read easily.  \n",
    "\n",
    "\n",
    " - Flesch-Kincaid\n",
    " - Coleman-Liau\n",
    " - Dale-Chall\n",
    " - SMOG\n",
    " - Automated Readability Index\n",
    " - Flesch Reading Ease (does not have min_age)\n",
    "\n",
    "source <https://github.com/wimmuskee/readability-score>\n",
    "\n",
    "### Readabiltiy Score Calculation\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Automated_readability_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. For recommended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_reviews['readability_FK'] = regular_reviews.review.apply(lambda x:FleschKincaid(x).min_age)\n",
    "regular_reviews['readability_AR'] = regular_reviews.review.apply(lambda x:ARI(x).min_age)\n",
    "regular_reviews.to_csv('data/reg_reviews_Readability.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. For not-recommended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_recommended_reviews['readability_FK'] = not_recommended_reviews.review.apply(lambda x:FleschKincaid(x).min_age)\n",
    "not_recommended_reviews['readability_AR'] = not_recommended_reviews.review.apply(lambda x:ARI(x).min_age)\n",
    "not_recommended_reviews.to_csv('data/not_reviews_Readability.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spam classification\n",
    "\n",
    "Spam Corpus Data : http://myleott.com/op-spam.html\n",
    "\n",
    "\n",
    "## Objective :  Classifying Yelp review into spam index [1=spam, 0=not spam].\n",
    "\n",
    "\n",
    "## Method: \n",
    "\n",
    "1. The raw training data set is processed to have a train and a valid data set.  \n",
    "Code to process: <https://drive.google.com/drive/folders/1VkS0TkjoeQp-vLnXQsw25YeUEiIcbANe>\n",
    "\n",
    "2. Logistic regression model will be tested with the corpus data itself to check the accuracy of model.\n",
    "\n",
    "3. Merge corpus train and test set to model with all corpus data set (1600 recordings)\n",
    "\n",
    "4. The trained model will be used to classify spam from the Yelp data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_SpamCorpus.csv')\n",
    "valid = pd.read_csv('data/valid_SpamCorpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append corpus data sets\n",
    "trainAll = train.append(valid, ignore_index=True)\n",
    "len(trainAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling/Validation within Spam Corpus Data Set\n",
    "\n",
    "##  <span style=\"color:blue\">Crete a bag of words representation, as a term document matrix. \n",
    "##  <span style=\"color:blue\">Use ngrams, as suggested in the NBSVM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic naive bayes feature equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(x, y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a model for one dependent at a time:\n",
    "This function is designed for multiple dependent features/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(x, y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(x,1,y) / pr(x,0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "      <th>fold</th>\n",
       "      <th>file</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>positive_polarity</td>\n",
       "      <td>deceptive_from_MTurk</td>\n",
       "      <td>2</td>\n",
       "      <td>d_talbott_9.txt</td>\n",
       "      <td>excellent staff and customer service, very cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>positive_polarity</td>\n",
       "      <td>deceptive_from_MTurk</td>\n",
       "      <td>2</td>\n",
       "      <td>d_talbott_8.txt</td>\n",
       "      <td>my stay at this hotel was one of the best i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>positive_polarity</td>\n",
       "      <td>deceptive_from_MTurk</td>\n",
       "      <td>2</td>\n",
       "      <td>d_affinia_20.txt</td>\n",
       "      <td>we just got back from a trip to chicago for my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>positive_polarity</td>\n",
       "      <td>deceptive_from_MTurk</td>\n",
       "      <td>2</td>\n",
       "      <td>d_hardrock_18.txt</td>\n",
       "      <td>i have to say that the hard rock hotel in chic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>positive_polarity</td>\n",
       "      <td>deceptive_from_MTurk</td>\n",
       "      <td>2</td>\n",
       "      <td>d_hardrock_19.txt</td>\n",
       "      <td>my husband and i recently stayed at the hard r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class           polarity                source  fold               file  \\\n",
       "0      0  positive_polarity  deceptive_from_MTurk     2    d_talbott_9.txt   \n",
       "1      0  positive_polarity  deceptive_from_MTurk     2    d_talbott_8.txt   \n",
       "2      0  positive_polarity  deceptive_from_MTurk     2   d_affinia_20.txt   \n",
       "3      0  positive_polarity  deceptive_from_MTurk     2  d_hardrock_18.txt   \n",
       "4      0  positive_polarity  deceptive_from_MTurk     2  d_hardrock_19.txt   \n",
       "\n",
       "                                              review  \n",
       "0  excellent staff and customer service, very cle...  \n",
       "1  my stay at this hotel was one of the best i ha...  \n",
       "2  we just got back from a trip to chicago for my...  \n",
       "3  i have to say that the hard rock hotel in chic...  \n",
       "4  my husband and i recently stayed at the hard r...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model cross-validation w/ Stratified k-fold (within spam corpus data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Stratified 3-Ford Cross-Validation:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.89      0.88       267\n",
      "          1       0.89      0.87      0.88       267\n",
      "\n",
      "avg / total       0.88      0.88      0.88       534\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.89      0.87       267\n",
      "          1       0.88      0.84      0.86       267\n",
      "\n",
      "avg / total       0.87      0.87      0.87       534\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.90      0.87       266\n",
      "          1       0.89      0.84      0.86       266\n",
      "\n",
      "avg / total       0.87      0.87      0.87       532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 1)\n",
    "\n",
    "X = trainAll['review']\n",
    "y = trainAll['class']\n",
    "skf.get_n_splits(X,y)\n",
    "\n",
    "print('Model Stratified 3-Ford Cross-Validation:')\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train0, X_test0 = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = vec.fit_transform(X_train0)\n",
    "    X_test = vec.transform(X_test0)\n",
    "    \n",
    "    m,r = get_mdl(X_train, y_train)\n",
    "    preds = m.predict_proba(X_test.multiply(r))[:,1]\n",
    "\n",
    "    print(classification_report(y_test, np.round(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Modeling w/ all 1600 spam corpus reviews to apply to Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = trainAll.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "x = vec.fit_transform(trainAll.review)\n",
    "\n",
    "m,r = get_mdl(x,trainAll['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a. Prediction for recommended reveiws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vec.transform(regular_reviews['review'])\n",
    "regular_reviews['spam'] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "regular_reviews.to_csv('data/reg_reviews_NLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b. Predictions for not-recommended reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = vec.transform(not_recommended_reviews['review'])\n",
    "not_recommended_reviews['spam'] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "not_recommended_reviews.to_csv('data/not_reviews_NLP.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
