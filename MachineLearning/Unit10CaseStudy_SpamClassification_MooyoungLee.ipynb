{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit10 Case Study: Spam Classification<a name=\"Top\"></a>\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">MSDS7333-4023\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">7/23/2018\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">Mooyoung Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Please use hyperlinks !\n",
    "    \n",
    "# Table of Contents<a name=\"toc\"></a>\n",
    "\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Data](#data)\n",
    "3. [Background](#Background)\n",
    "4. [Evaluation Metrix](#eval)\n",
    "5. [Method](#method)\n",
    "6. [Results](#Result)\n",
    "7. [Conclusion](#Conclusion)\n",
    "8. [Reference Code](#code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> **The objective of this study is to compare the performance of multiple classification algorithms in classifying emails as spam or ham.**\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> Spam corpus data from Spam Assassin website is used for both model training and evaluation.  **Naive Bayes, CART, and XGBoost models were used to classify spam emails and compare the performance metrix.**\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> **Extra attention was paid to the sampling method** between simple random sampling(SRS) and stratified sampling(STR) since the corpus data amounts were not balanced.  **Statistical tests are performed to see if there is a difference in classification performance with two different sampling methods.**  \n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> The first Naive Bayes model utilized only a Bag of Words(BOW) from email body text.  The later CART and XGBoost models were used [29 features](#variable) that are consist of email metadata and simple text mining data. The log likelihood ratio found from the first Naive Bayes model is added to the 29 features to run a XGBoost model to improve performance.  \n",
    "\n",
    "\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data <a name='data'></a>\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">The original source of the spam corpus is SpamAssassin website. 5 corpus directories were existing, ['easy_ham' 'easy_ham_2' 'hard_ham' 'spam' 'spam_2'].  The word 'ham' represent non-spam emails.  The number of emails in each directory were 5051, 1400, 500, 1000, and 1397 accordingly. The name of the corpus folder indicated that there are some difference in the contents among folders.  <span style=\"color:blue\">**Only 7.1%(=500/6951) of non-spam emails were belong to 'hard_ham' corpus and 92.8% of the non-spam emails were belong to 'easy_ham'.**</span>  If the 'hard_ham' contains very distinct data compared to 'easy_ham', then stratified sampling is a better sampling scheme than drawing SRS sample from a flattened group containing all 'ham'.\n",
    "\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">For the Naive Bayes mode, only the email body texts are processed to determine the probabilies of being a spam. The body text is cleaned by removing punctuations and stopwords. Processes of data extraction from email and cleaning to create features are borrowed from the textbook listed below.   \n",
    "\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">For the CART and XGBoost models, 29 features are used.  The code to generate all the 29 features are reused from textbook. The details of features are shown on the below table. \n",
    "\n",
    "\n",
    "\n",
    "**TextBook**: Chapter 3 from [\"Data Science in R A Case Studies Approach to Computational Reasoning and Problem Solving\"](http://rdatasciencecases.org/)\n",
    "\n",
    "**Textbook data**: [SpamAssasin Corpus](http://www.rdatasciencecases.org/Spam/)\n",
    "\n",
    "**Original data source**: [SpamAssassin](http://spamassassin.apache.org)\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 align='center'>Variable Definition Table</h3><a name=\"variable\"></a>\n",
    "\n",
    "\n",
    "|Variable |Type |Definition|\n",
    "|------------------------|---------------------------|-----------------------------|\n",
    "|isRe |logical |TRUE if Re: appears at the start of the subject.|\n",
    "|numLines |integer |Number of lines in the body of the message.|\n",
    "|bodyCharCt |integer |Number of characters in the body of the message.|\n",
    "|underscore |logical |TRUE if email address in the From field of the header contains an underscore.|\n",
    "|subExcCt |integer |Number of exclamation marks in the subject.|\n",
    "|subQuesCt |integer |Number of question marks in the subject.|\n",
    "|numAtt |integer |Number of attachments in the message.|\n",
    "|priority |logical |TRUE if a Priority key is present in the header.|\n",
    "|numRec |numeric |Number of recipients of the message, including CCs.|\n",
    "|perCaps |numeric |Percentage of capitals among all letters in the message body, excluding attachments.|\n",
    "|isInReplyTo |logical |TRUE if the In-Reply-To key is present in the header.|\n",
    "|sortedRec |logical |TRUE if the recipients’ email addresses are sorted.|\n",
    "|subPunc |logical |TRUE if words in the subject have punctuation or numbers embedded in them, e.g., w!se.|\n",
    "|hour |numeric |Hour of the day in the Date field.|\n",
    "|multipartText |logical |TRUE if the MIME type is multipart/text.|\n",
    "|hasImages |logical |TRUE if the message contains images.|\n",
    "|isPGPsigned |logical |TRUE if the message contains a PGP signature.|\n",
    "|perHTML |numeric |Percentage of characters in HTML tags in the message body in comparison to all characters.|\n",
    "|subSpamWords |logical |TRUE if the subject contains one of the words in a spam word vector.|\n",
    "|subBlanks |numeric |Percentage of blanks in the subject.|\n",
    "|noHost |logical |TRUE if there is no hostname in the Message-Id key in the header.|\n",
    "|numEnd |logical |TRUE if the email sender’s address (before the @) ends in a number.|\n",
    "|isYelling |logical |TRUE if the subject is all capital letters.|\n",
    "|forwards |numeric |Number of forward symbols in a line of the body, e.g.,>>> xxx contains 3 forwards.|\n",
    "|isOrigMsg |logical |TRUE if the message body contains the phrase original message.|\n",
    "|isDear |logical |TRUE if the message body contains the word dear.|\n",
    "|isWrote |logical |TRUE if the message contains the phrase wrote:.|\n",
    "|avgWordLen |numeric |The average length of the words in a message.|\n",
    "|numDlr |numeric |Number of dollar signs in the message body.|\n",
    "\n",
    "[Go back](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background  <a name='Background'></a>\n",
    "\n",
    "## **Naive Bayes :**\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> Naive Bayes classifier use a bag of word. A bag of word can be created from train data set using all email recordings.  The order of word is not considered so it is called Naive.  It only counts the frequency of the word and use the frequency to calculate the likelihood of being spam.  Applying log to the likelyhood of being spam changes product process into sum while providing a better statistical property [Textbook].  Therefore, the log-likelyhood-ratio(LLR) is used to calculate the probabilities of being spam.  \n",
    "\n",
    "\n",
    "## **CART :**\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> CART represent Classification And Regression Tree. It is a decision tree based algorithm that can be used both classification and regression. [More Info](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29)\n",
    "\n",
    "## **XGBoost :**\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> XGBoost is a gradient boosted decision trees that is focused on computational speed and model performance. [Three main component of XGBoost are gradient boosting algorithm, stochastic gradient boosting, and regularized gradient boosting.](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)  Gradient boosting can be controlled by the learning rate.  Stochastic gradient boosting is related to the hyperparameters such as sub-sampling and column split levels.  L1 and L2 are the regularization inputs. \n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics  <a name='eval'></a>\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> For spam classification task, false positive rate should be minimized because the cost to label regular emails as spam and lose the communications will be high.  The cost to the false negative which is allowing to receive some spam email will be acceptable comparing to the cost of losing regular emails.  Thus, **Type I Error or Precision should be the primary interest for the implementation of a spam classification model.**  \n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> However, Type I Error can become zero by adjusting a model hyperparameter (e.g. Tau-value for log-odd-ratio) with having a large Type II Error.  Therefore, **F1 score or Accuracy values have to be used to compare the performance of models during a model selection stage.**  Accuracy is an ratio of correct prediction.  F1-score is an weighted average of recall and precision.  **F1-score is better than Accuracy for our application** since the distribution of class(Spam:Ham ~ 1:2) is not even and also the cost of false positive and false negatives are different.  \n",
    "\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\"> Belows are the equations for evaluation metrics. These metrics will be calculated for every classifier to make sure all other metrics are reasonable beside the F1-score, the metrics to choose a final classifier. \n",
    "\n",
    "$$Precision = \\frac{TP}{TP+FP}$$<br>\n",
    "$$Recall = \\frac{TP}{TP+FN}$$<br>\n",
    "$$F1 = \\frac{2*Precision*Recall}{Precision+Recall}$$<br>\n",
    "$$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$$<br>\n",
    "$$Type I Error = \\frac{FP}{FP+TN}$$<br>\n",
    "$$Type II Error = \\frac{FN}{FN+TP}$$<br>\n",
    "\n",
    "*Where,* \n",
    "> *TP = True Positive,*<br>\n",
    "  *FP = False Positive,*<br>\n",
    "  *TN = True Negative,*<br> \n",
    "  *FN = False Negative.*<br>\n",
    "        \n",
    "        \n",
    "[Equation Reference](https://en.wikipedia.org/wiki/Precision_and_recall)<br>\n",
    "[Metrics Applications 1](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)\n",
    "[Metrics Applications 2](http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method <a name='method'></a>\n",
    "\n",
    "**1. Naive Bayes Model**\n",
    "\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[Log-likelyhood-ratio example](#tau) shows the effect of Type I and II errors by selecting tau-value.\n",
    "- <span style=\"color:black; font-size: 1.3em;\">Naive Bayes models are compared with SRS and Stratified samplings.  The train and test data sets are splited with 66% and 33% accordingly.  For each sampling scheme, modeling and performance evaluation process is repeated 10 times.  \n",
    "- <span style=\"color:black; font-size: 1.3em;\">[A function](#str) is wrote to draw SRS and Stratify samples.\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[A function](#fun_naive) is generated to iteratively model and collect performance metrics. Tau value is set to have Type I error not greater than 0.01.\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[T-test](#ttest) is performed to check the mean difference in LLR statistics and performance metrics.\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[Classifier performance](#eval_naive) outputs are generated. k-fold cross-validation is not used for this model.  Instead, 10-time random samples are used to evaluate.  \n",
    "    \n",
    "**2. CART**\n",
    "\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[CART model](#cart) is generated using rpart library method and the cp parameter, which helps to prune trees, is optimized using caret library. The range of cp parameter search was 0 to 0.01 by 0.0005 increment.  3-fold cross-validation is used for this process.\n",
    "    \n",
    "**3. XGBoost**\n",
    "\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[XGBoost model](#xg) is also optimized using the caret library.  Below parameters values are used to find the best combination to yield the best F1 score. 3-fold cross-validation is used for this process.\n",
    "    \n",
    "     - nrounds = c(100,150,200), \n",
    "     - max_depth = c(7,9,11,13), \n",
    "     - eta = c(0.01,0.1,0.3), \n",
    "     - gamma=c(0,1,3), \n",
    "     - min_child_weight=c(1,3,5)\n",
    "     \n",
    "**4. XGBoost w/ additional LLR feature**\n",
    "\n",
    "- <span style=\"color:black; font-size: 1.3em;\">The LLR model is generated only based on the training data set. Then the LLR values for the test and train data sets are calculated using the LLR model.  \n",
    "- <span style=\"color:black; font-size: 1.3em;\">The LLR value is added as a new feature for each train and test data sets that originally include 29 features. Thus, the final train/test sets had 30 features at the end.\n",
    "- <span style=\"color:black; font-size: 1.3em;\">[XGBoost model](#xgllr) was generated using the total 30 input features. \n",
    "- <span style=\"color:black; font-size: 1.3em;\">3-fold cross-validation is performed.\n",
    "\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result <a name=\"Result\"></a>\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">The [t-test](#ttest) showed that there is no significant difference in LLR distribution statistics and model performance metrics. \n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">The [performance metrics of all four classifiers](#resultAll) are compared.  The highest F1 score, 0.986, was acheived by XGBoost model without LLR.  The second highest F1 score was from XGBoost model with additional LLR feature.  The Naive Bayes and CART models showed F1 score of 0.963 and 0.960 accordingly, which is 2% behind XGBoost models.  XGBoost w/ additional LLR feature showed the lowest Type I and Type II errors.  \n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion<a name=\"Conclusion\"></a>\n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">Even though the corpus data recordings are not balanced and also contains differnet difficulties to detect as spam, <span style=\"color:blue\">there was no significant evidence (w/ 0.05 of alpha value) to change the Naive Bayes model performance by changing sampling scheme from SRS to Statified.</span>  Thus, the rest of the model evaluation was done with SRS samples without stratification by the source folders.  \n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">XGBoost classifier showed higher F1 score by 0.02 than the Naive Bayes and CART models.  <span style=\"color:blue\">By adding Log-Likelyhood-Ratio(LLR) feature into the XGBoost model, the precision, accuracy, and Type I and II errors become superior than any other models.</span> \n",
    "\n",
    "<span style=\"color:black; font-size: 1.3em;\">To control the spam emails, precision need to be higher or Type I error has to be very low.  The XGBoost model with LLR feature gives 0.003 of Type I error.  This means that 3 non-spam emails out of a thousand non-spam email will be falsely classified as spam.  Those 3 email will go into a spam folder and the recipient may never see the email.  0.003 of Type I error value was the lowest error rate among other models and while providing the lowest Type II error.  However,  <span style=\"color:blue\">the decision to implement this model should be made carefully by considering the cost of false positive rate to the business that is adopting this model.  \n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Code <a name=\"code\"></a>\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1-A: Data Preperation for Naive Bayes Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working directory structure\n",
    "\n",
    "Place all spam directories under the 'messages' directory to make easy to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>''</li>\n",
       "\t<li>'.ipynb_checkpoints'</li>\n",
       "\t<li>'Data'</li>\n",
       "\t<li>'messages'</li>\n",
       "\t<li>'messages/easy_ham'</li>\n",
       "\t<li>'messages/easy_ham_2'</li>\n",
       "\t<li>'messages/hard_ham'</li>\n",
       "\t<li>'messages/spam'</li>\n",
       "\t<li>'messages/spam_2'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item ''\n",
       "\\item '.ipynb\\_checkpoints'\n",
       "\\item 'Data'\n",
       "\\item 'messages'\n",
       "\\item 'messages/easy\\_ham'\n",
       "\\item 'messages/easy\\_ham\\_2'\n",
       "\\item 'messages/hard\\_ham'\n",
       "\\item 'messages/spam'\n",
       "\\item 'messages/spam\\_2'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. ''\n",
       "2. '.ipynb_checkpoints'\n",
       "3. 'Data'\n",
       "4. 'messages'\n",
       "5. 'messages/easy_ham'\n",
       "6. 'messages/easy_ham_2'\n",
       "7. 'messages/hard_ham'\n",
       "8. 'messages/spam'\n",
       "9. 'messages/spam_2'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"\"                    \".ipynb_checkpoints\"  \"Data\"               \n",
       "[4] \"messages\"            \"messages/easy_ham\"   \"messages/easy_ham_2\"\n",
       "[7] \"messages/hard_ham\"   \"messages/spam\"       \"messages/spam_2\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spamPath = \"./\"\n",
    "list.dirs(spamPath, full.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 different spam corpus directories are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'easy_ham'</li>\n",
       "\t<li>'easy_ham_2'</li>\n",
       "\t<li>'hard_ham'</li>\n",
       "\t<li>'spam'</li>\n",
       "\t<li>'spam_2'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'easy\\_ham'\n",
       "\\item 'easy\\_ham\\_2'\n",
       "\\item 'hard\\_ham'\n",
       "\\item 'spam'\n",
       "\\item 'spam\\_2'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'easy_ham'\n",
       "2. 'easy_ham_2'\n",
       "3. 'hard_ham'\n",
       "4. 'spam'\n",
       "5. 'spam_2'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"easy_ham\"   \"easy_ham_2\" \"hard_ham\"   \"spam\"       \"spam_2\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list.files(path = paste(spamPath, \"messages\", sep = .Platform$file.sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The email recordings are named with a number and MD5 checksum value that is generated using the contents of each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'00001.7848dde101aa985090474a91ec93fcf0'</li>\n",
       "\t<li>'00002.d94f1b97e48ed3b553b3508d116e6a09'</li>\n",
       "\t<li>'00003.2ee33bc6eacdb11f38d052c44819ba6c'</li>\n",
       "\t<li>'00004.eac8de8d759b7e74154f142194282724'</li>\n",
       "\t<li>'00005.57696a39d7d84318ce497886896bf90d'</li>\n",
       "\t<li>'00006.5ab5620d3d7c6c0db76234556a16f6c1'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '00001.7848dde101aa985090474a91ec93fcf0'\n",
       "\\item '00002.d94f1b97e48ed3b553b3508d116e6a09'\n",
       "\\item '00003.2ee33bc6eacdb11f38d052c44819ba6c'\n",
       "\\item '00004.eac8de8d759b7e74154f142194282724'\n",
       "\\item '00005.57696a39d7d84318ce497886896bf90d'\n",
       "\\item '00006.5ab5620d3d7c6c0db76234556a16f6c1'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '00001.7848dde101aa985090474a91ec93fcf0'\n",
       "2. '00002.d94f1b97e48ed3b553b3508d116e6a09'\n",
       "3. '00003.2ee33bc6eacdb11f38d052c44819ba6c'\n",
       "4. '00004.eac8de8d759b7e74154f142194282724'\n",
       "5. '00005.57696a39d7d84318ce497886896bf90d'\n",
       "6. '00006.5ab5620d3d7c6c0db76234556a16f6c1'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"00001.7848dde101aa985090474a91ec93fcf0\"\n",
       "[2] \"00002.d94f1b97e48ed3b553b3508d116e6a09\"\n",
       "[3] \"00003.2ee33bc6eacdb11f38d052c44819ba6c\"\n",
       "[4] \"00004.eac8de8d759b7e74154f142194282724\"\n",
       "[5] \"00005.57696a39d7d84318ce497886896bf90d\"\n",
       "[6] \"00006.5ab5620d3d7c6c0db76234556a16f6c1\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(list.files(path = paste(spamPath,'messages', 'spam', sep=.Platform$file.sep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of files\n",
    "\n",
    "### corpus directory may contain extra system log files so the actual recordings are one or two less than the file count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of all corpus data :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "9353"
      ],
      "text/latex": [
       "9353"
      ],
      "text/markdown": [
       "9353"
      ],
      "text/plain": [
       "[1] 9353"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of each corpus data :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>.//messages/easy_ham</dt>\n",
       "\t\t<dd>5052</dd>\n",
       "\t<dt>.//messages/easy_ham_2</dt>\n",
       "\t\t<dd>1401</dd>\n",
       "\t<dt>.//messages/hard_ham</dt>\n",
       "\t\t<dd>501</dd>\n",
       "\t<dt>.//messages/spam</dt>\n",
       "\t\t<dd>1001</dd>\n",
       "\t<dt>.//messages/spam_2</dt>\n",
       "\t\t<dd>1398</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[.//messages/easy\\textbackslash{}\\_ham] 5052\n",
       "\\item[.//messages/easy\\textbackslash{}\\_ham\\textbackslash{}\\_2] 1401\n",
       "\\item[.//messages/hard\\textbackslash{}\\_ham] 501\n",
       "\\item[.//messages/spam] 1001\n",
       "\\item[.//messages/spam\\textbackslash{}\\_2] 1398\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       ".//messages/easy_ham\n",
       ":   5052.//messages/easy_ham_2\n",
       ":   1401.//messages/hard_ham\n",
       ":   501.//messages/spam\n",
       ":   1001.//messages/spam_2\n",
       ":   1398\n",
       "\n"
      ],
      "text/plain": [
       "  .//messages/easy_ham .//messages/easy_ham_2   .//messages/hard_ham \n",
       "                  5052                   1401                    501 \n",
       "      .//messages/spam     .//messages/spam_2 \n",
       "                  1001                   1398 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirNames = list.files(path = paste(spamPath, \"messages\", \n",
    "                      sep = .Platform$file.sep))\n",
    "\n",
    "cat('Total number of all corpus data :\\n')\n",
    "length(list.files(paste(spamPath, \"messages\", dirNames, \n",
    "                        sep = .Platform$file.sep)))\n",
    "\n",
    "cat('\\n\\nNumber of each corpus data :\\n')\n",
    "sapply(paste(spamPath, \"messages\", dirNames, \n",
    "             sep = .Platform$file.sep), \n",
    "       function(dir) length(list.files(dir)) )\n",
    "\n",
    "fullDirNames = paste(spamPath, \"messages\", dirNames, \n",
    "                     sep = .Platform$file.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert body text into odd ratio using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "splitMessage = function(msg) {\n",
    "  splitPoint = match(\"\", msg)\n",
    "  header = msg[1:(splitPoint-1)]\n",
    "  body = msg[ -(1:splitPoint) ]\n",
    "  return(list(header = header, body = body))\n",
    "}\n",
    "\n",
    "getBoundary = function(header) {\n",
    "  boundaryIdx = grep(\"boundary=\", header)\n",
    "  boundary = gsub('\"', \"\", header[boundaryIdx])\n",
    "  gsub(\".*boundary= *([^;]*);?.*\", \"\\\\1\", boundary)\n",
    "}\n",
    "\n",
    "dropAttach = function(body, boundary){\n",
    "  \n",
    "  bString = paste(\"--\", boundary, sep = \"\")\n",
    "  bStringLocs = which(bString == body)\n",
    "  \n",
    "  if (length(bStringLocs) <= 1) return(body)\n",
    "  \n",
    "  eString = paste(\"--\", boundary, \"--\", sep = \"\")\n",
    "  eStringLoc = which(eString == body)\n",
    "  if (length(eStringLoc) == 0) \n",
    "    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])\n",
    "  \n",
    "  n = length(body)\n",
    "  if (eStringLoc < n) \n",
    "     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), \n",
    "                    ( (eStringLoc + 1) : n )) ] )\n",
    "  \n",
    "  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])\n",
    "}\n",
    "\n",
    "library(tm)\n",
    "stopWords = stopwords()\n",
    "cleanSW = tolower(gsub(\"[[:punct:]0-9[:blank:]]+\", \" \", stopWords))\n",
    "SWords = unlist(strsplit(cleanSW, \"[[:blank:]]+\"))\n",
    "SWords = SWords[ nchar(SWords) > 1 ]\n",
    "stopWords = unique(SWords)\n",
    "\n",
    "cleanText =\n",
    "function(msg)   {\n",
    "  tolower(gsub(\"[[:punct:]0-9[:space:][:blank:]]+\", \" \", msg))\n",
    "}\n",
    "\n",
    "findMsgWords = \n",
    "function(msg, stopWords) {\n",
    " if(is.null(msg))\n",
    "  return(character())\n",
    "\n",
    " words = unique(unlist(strsplit(cleanText(msg), \"[[:blank:]\\t]+\")))\n",
    " \n",
    " # drop empty and 1 letter words\n",
    " words = words[ nchar(words) > 1]\n",
    " words = words[ !( words %in% stopWords) ]\n",
    " invisible(words)\n",
    "}\n",
    "\n",
    "\n",
    "processAllWords = function(dirName, stopWords)\n",
    "{\n",
    "       # read all files in the directory\n",
    "  fileNames = list.files(dirName, full.names = TRUE)\n",
    "       # drop files that are not email, i.e., cmds\n",
    "  notEmail = grep(\"cmds$\", fileNames)\n",
    "  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]\n",
    "\n",
    "  messages = lapply(fileNames, readLines, encoding = \"latin1\")\n",
    "  \n",
    "       # split header and body\n",
    "  emailSplit = lapply(messages, splitMessage)\n",
    "       # put body and header in own lists\n",
    "  bodyList = lapply(emailSplit, function(msg) msg$body)\n",
    "  headerList = lapply(emailSplit, function(msg) msg$header)\n",
    "  rm(emailSplit)\n",
    "  \n",
    "       # determine which messages have attachments\n",
    "  hasAttach = sapply(headerList, function(header) {\n",
    "    CTloc = grep(\"Content-Type\", header)\n",
    "    if (length(CTloc) == 0) return(0)\n",
    "    multi = grep(\"multi\", tolower(header[CTloc])) \n",
    "    if (length(multi) == 0) return(0)\n",
    "    multi\n",
    "  })\n",
    "  \n",
    "  hasAttach = which(hasAttach > 0)\n",
    "  \n",
    "       # find boundary strings for messages with attachments\n",
    "  boundaries = sapply(headerList[hasAttach], getBoundary)\n",
    "  \n",
    "       # drop attachments from message body\n",
    "  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], \n",
    "                               boundaries, SIMPLIFY = FALSE)\n",
    "  \n",
    "       # extract words from body\n",
    "  msgWordsList = lapply(bodyList, findMsgWords, stopWords)\n",
    "  \n",
    "  invisible(msgWordsList)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/hard_ham/00228.0eaef7857bbbf3ebf5edbbdae2b30493'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/hard_ham/0231.7c6cc716ce3f3bfad7130dd3c8d7b072'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/hard_ham/0250.7c6cc716ce3f3bfad7130dd3c8d7b072'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/spam/00136.faa39d8e816c70f23b4bb8758d8a74f0'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/spam/0143.260a940290dcb61f9327b224a368d4af'\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages in each directory:\n",
      " 5051 1400 500 1000 1397"
     ]
    }
   ],
   "source": [
    "msgWordsList = lapply(fullDirNames, processAllWords, \n",
    "                      stopWords = stopWords) \n",
    "\n",
    "numMsgs = sapply(msgWordsList, length)\n",
    "cat('Number of messages in each directory:\\n',numMsgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Spam Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)  # label Spam and Ham by the folder names\n",
    "numFolder = rep(c(1,2,3,4,5), numMsgs)   # label folder numbers for stratification sampling purpose\n",
    "\n",
    "msgWordsList = unlist(msgWordsList, recursive = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Train/Test data split w/ SRS or Folder-Stratification<a name=\"str\"></a>\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = function(msgWordsList = msgWordsList, stratify = FALSE, test_size = 3, random_state = 418910){\n",
    "    \n",
    "    # stratify[True] will sample data from each folder proportionally to their recording size\n",
    "    # stratify[False] will samlple data from the combined Spam and Ham folders. \n",
    "    # test_size is the denominator of total recordings.  For example, 3 means 1/3 of all data will be used for testing.\n",
    "    # random_state is the seed number\n",
    "    \n",
    "    set.seed(random_state)\n",
    "    numEmail = length(isSpam)\n",
    "    numSpam = sum(isSpam)\n",
    "    numHam = numEmail - numSpam\n",
    "    numSpamTest = numSpam/test_size\n",
    "    numHamTest = numHam/test_size\n",
    "    \n",
    "    if (stratify == FALSE){\n",
    "        \n",
    "        # SRS sample from each Spam and Ham groups  \n",
    "        testSpamIdx = sample(numSpam, size = floor(numSpamTest))\n",
    "        testHamIdx = sample(numHam, size = floor(numHamTest))\n",
    "\n",
    "        testMsgWords = c((msgWordsList[isSpam])[testSpamIdx],\n",
    "                         (msgWordsList[!isSpam])[testHamIdx] )\n",
    "        trainMsgWords = c((msgWordsList[isSpam])[ - testSpamIdx], \n",
    "                          (msgWordsList[!isSpam])[ - testHamIdx])\n",
    "\n",
    "        testIsSpam = rep(c(TRUE, FALSE), \n",
    "                         c(length(testSpamIdx), length(testHamIdx)))\n",
    "        trainIsSpam = rep(c(TRUE, FALSE), \n",
    "                         c(numSpam - length(testSpamIdx), \n",
    "                           numHam - length(testHamIdx)))\n",
    "    }else{\n",
    "        # Stratified sample from each corpus folder (5-folder in this case)\n",
    "        numTestEachFolder = floor(c(numMsgs[1:3]/numHam*numHamTest, numMsgs[4:5]/numSpam*numSpamTest))\n",
    "        testID_eachFolder = mapply(sample, numMsgs, numTestEachFolder)    # sample numbers for each folder\n",
    "        numFolderCumsum = c(0,cumsum(numMsgs)[1:4])                       # cumulative sum of each folder total recordings\n",
    "        \n",
    "        testID = unlist(sapply(1:5, function(x) testID_eachFolder[[x]]+numFolderCumsum[x]))   # test sample numbers\n",
    "        testMsgWords = msgWordsList[testID]\n",
    "        trainMsgWords = msgWordsList[- testID]   \n",
    "        testIsSpam = isSpam[testID]\n",
    "        trainIsSpam = isSpam[- testID]   \n",
    "    }\n",
    "    \n",
    "    return(list(testMsgWords, trainMsgWords, testIsSpam, trainIsSpam))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to compute Log Odd ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeFreqs =\n",
    "function(wordsList, spam, bow = unique(unlist(wordsList)))\n",
    "{\n",
    "   # create a matrix for spam, ham, and log odds\n",
    "  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), \n",
    "                     dimnames = list(c(\"spam\", \"ham\", \n",
    "                                        \"presentLogOdds\", \n",
    "                                        \"absentLogOdds\"),  bow))\n",
    "\n",
    "   # For each spam message, add 1 to counts for words in message\n",
    "  counts.spam = table(unlist(lapply(wordsList[spam], unique)))\n",
    "  wordTable[\"spam\", names(counts.spam)] = counts.spam + .5\n",
    "\n",
    "   # Similarly for ham messages\n",
    "  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  \n",
    "  wordTable[\"ham\", names(counts.ham)] = counts.ham + .5  \n",
    "\n",
    "\n",
    "   # Find the total number of spam and ham\n",
    "  numSpam = sum(spam)\n",
    "  numHam = length(spam) - numSpam\n",
    "\n",
    "   # Prob(word|spam) and Prob(word | ham)\n",
    "  wordTable[\"spam\", ] = wordTable[\"spam\", ]/(numSpam + .5)\n",
    "  wordTable[\"ham\", ] = wordTable[\"ham\", ]/(numHam + .5)\n",
    "  \n",
    "   # log odds\n",
    "  wordTable[\"presentLogOdds\", ] = \n",
    "     log(wordTable[\"spam\",]) - log(wordTable[\"ham\", ])\n",
    "  wordTable[\"absentLogOdds\", ] = \n",
    "     log((1 - wordTable[\"spam\", ])) - log((1 -wordTable[\"ham\", ]))\n",
    "\n",
    "  invisible(wordTable)\n",
    "}\n",
    "\n",
    "computeMsgLLR = function(words, freqTable) \n",
    "{\n",
    "       # Discards words not in training data.\n",
    "  words = words[!is.na(match(words, colnames(freqTable)))]\n",
    "\n",
    "       # Find which words are present\n",
    "  present = colnames(freqTable) %in% words\n",
    "\n",
    "  sum(freqTable[\"presentLogOdds\", present]) +\n",
    "    sum(freqTable[\"absentLogOdds\", !present])\n",
    "}\n",
    "\n",
    "typeIErrorRate = \n",
    "function(tau, llrVals, spam)\n",
    "{\n",
    "  classify = llrVals > tau\n",
    "  sum(classify & !spam)/sum(!spam)\n",
    "}\n",
    "\n",
    "typeIErrorRates = \n",
    "function(llrVals, isSpam) \n",
    "{\n",
    "  o = order(llrVals)\n",
    "  llrVals =  llrVals[o]\n",
    "  isSpam = isSpam[o]\n",
    "\n",
    "  idx = which(!isSpam)\n",
    "  N = length(idx)\n",
    "  list(error = (N:1)/N, values = llrVals[idx])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Metrix Function\n",
    "\n",
    "Below MLmetrics outputs were not matching w/ theoreticall equations so a function to calculate evaulation metrics are generated.\n",
    "\n",
    "library(MLmetrics)<br>\n",
    "f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])<br>\n",
    "p <- Precision(y_pred = data$pred, y_true = data$obs, positive = lev[1])<br>\n",
    "r <- Recall(y_pred = data$pred, y_true = data$obs, positive = lev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = function(y_pred, y_true, tau){\n",
    "    tp = sum(y_pred ==1 & y_true ==1)\n",
    "    fp = sum(y_pred ==1 & y_true ==0)\n",
    "    tn = sum(y_pred ==0 & y_true ==0)\n",
    "    fn = sum(y_pred ==0 & y_true ==1)\n",
    "    \n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "    \n",
    "    # Type I&II Errors (Dr.Slater)\n",
    "    t1e<-fp/(fp+tn)\n",
    "    t2e<-fn/(fn+tp)\n",
    "    \n",
    "    return(data.frame(Precision = precision, Recall = recall, \n",
    "                      F1 = f1, Accuracy = accuracy, \n",
    "                      Type_I_Error = t1e, Type_II_Error = t2e,\n",
    "                     Tau = tau))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation of Type I and II Errors w/ Tau value <a name=\"tau\"></a>\n",
    "\n",
    "\n",
    "## <span style=\"color:blue\">Below plot shows that the Type I error can be decreased while Type II error increase as the tau value moves from left to right. \n",
    "    \n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTestSplit = train_test_split(msgWordsList = msgWordsList, stratify = FALSE, test_size = 3)\n",
    "\n",
    "testMsgWords = trainTestSplit[[1]]\n",
    "trainMsgWords = trainTestSplit[[2]]\n",
    "testIsSpam = trainTestSplit[[3]]\n",
    "trainIsSpam = trainTestSplit[[4]]\n",
    "\n",
    "# Log Likelihood Ratio w/ Naive Bayes\n",
    "trainTable = computeFreqs(trainMsgWords, trainIsSpam)\n",
    "testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLLR = sapply(trainMsgWords, computeMsgLLR, trainTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeIErrorRates = function(llrVals, isSpam) {\n",
    "    o = order(llrVals)\n",
    "    llrVals =  llrVals[o]\n",
    "    isSpam = isSpam[o]\n",
    "\n",
    "    idx = which(!isSpam)\n",
    "    N = length(idx)\n",
    "    list(error = (N:1)/N, values = llrVals[idx])\n",
    "}\n",
    "\n",
    "typeIIErrorRates = function(llrVals, isSpam) {\n",
    "    \n",
    "    o = order(llrVals)\n",
    "    llrVals =  llrVals[o]\n",
    "    isSpam = isSpam[o]\n",
    "\n",
    "    idx = which(isSpam)\n",
    "    N = length(idx)\n",
    "    list(error = (1:(N))/N, values = llrVals[idx])\n",
    "}  \n",
    "\n",
    "xI = typeIErrorRates(testLLR, testIsSpam)\n",
    "xII = typeIIErrorRates(testLLR, testIsSpam)\n",
    "tau01 = round(min(xI$values[xI$error <= 0.01]))\n",
    "t2 = max(xII$error[ xII$values < tau01 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAulBMVEUAAABNTU1Nr0poaGh8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\nCRBASIAAQgIEEBIggJAAAf8f7mIkf1PrkhMAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(RColorBrewer)\n",
    "cols = brewer.pal(9, \"Set1\")[c(3, 4, 5)]\n",
    "plot(xII$error ~ xII$values,  type = \"l\", col = cols[1], lwd = 3,\n",
    "     xlim = c(-300, 250), ylim = c(0, 1),\n",
    "     xlab = \"Log Likelihood Ratio Values\", ylab=\"Error Rate\")\n",
    "points(xI$error ~ xI$values, type = \"l\", col = cols[2], lwd = 3)\n",
    "legend(x = 50, y = 0.4, fill = c(cols[2], cols[1]),\n",
    "       legend = c(\"Classify Ham as Spam\", \n",
    "                  \"Classify Spam as Ham\"), cex = 0.8,\n",
    "       bty = \"n\")\n",
    "abline(h=0.01, col =\"grey\", lwd = 3, lty = 2)\n",
    "text(-250, 0.05, pos = 4, \"Type I Error = 0.01\", col = cols[2])\n",
    "\n",
    "mtext(tau01, side = 1, line = 0.5, at = tau01, col = cols[3])\n",
    "segments(x0 = tau01, y0 = -.50, x1 = tau01, y1 = t2, \n",
    "         lwd = 2, col = \"grey\")\n",
    "text(tau01 + 20, 0.05, pos = 4,\n",
    "     paste(\"Type II Error = \", round(t2, digits = 2)), \n",
    "     col = cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Training and Testing (x10 iteration) <a name=\"fun_naive\"></a>\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTest10 = function(stratify = FALSE){\n",
    "for (i in 1:10){\n",
    "    # Train/Test sample number split\n",
    "    trainTestSplit = train_test_split(msgWordsList = msgWordsList, stratify = stratify, test_size = 3, random_state = i)\n",
    "\n",
    "    testMsgWords = trainTestSplit[[1]]\n",
    "    trainMsgWords = trainTestSplit[[2]]\n",
    "    testIsSpam = trainTestSplit[[3]]\n",
    "    trainIsSpam = trainTestSplit[[4]]\n",
    "\n",
    "    # Log Likelihood Ratio w/ Naive Bayes\n",
    "    trainTable = computeFreqs(trainMsgWords, trainIsSpam)\n",
    "    testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)\n",
    "\n",
    "    # Summary of Log Ratio for Ham and Spam\n",
    "    LLR_summary = tapply(testLLR, testIsSpam, summary)\n",
    "\n",
    "\n",
    "    # determine Tau; Set to max allowable Type 1 error rate as 1%\n",
    "    xI = typeIErrorRates(testLLR, testIsSpam)\n",
    "    xII = typeIIErrorRates(testLLR, testIsSpam)\n",
    "    tau = round(min(xI$values[xI$error <= 0.01]))\n",
    "    \n",
    "    # Prediction values\n",
    "    y_pred = testLLR > tau\n",
    "\n",
    "    # Evalueation Metricx\n",
    "    Eval_Metrics = metrics(y_pred, testIsSpam, tau)\n",
    "\n",
    "    # Output collection for 10 iterations\n",
    "    if (i==1){\n",
    "        LLR_summary_TRUE = LLR_summary$'TRUE'\n",
    "        LLR_summary_FALSE = LLR_summary$'FALSE'\n",
    "        Metrics = Eval_Metrics\n",
    "        }else{\n",
    "            LLR_summary_TRUE = rbind(LLR_summary_TRUE, LLR_summary$'TRUE')\n",
    "            LLR_summary_FALSE = rbind(LLR_summary_FALSE, LLR_summary$'FALSE')\n",
    "            Metrics = rbind(Metrics, Eval_Metrics)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return(list(LLR_summary_TRUE, LLR_summary_FALSE, Metrics))\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_SRS = TrainTest10(stratify = FALSE)\n",
    "Result_STR = TrainTest10(stratify = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Random Sample into Naive Bayes <a name=\"Result_SRS\"></a>\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<thead><tr><th></th><th scope=col>Min.</th><th scope=col>1st Qu.</th><th scope=col>Median</th><th scope=col>Mean</th><th scope=col>3rd Qu.</th><th scope=col>Max.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>LLR_summary_TRUE</th><td> -80.10191 </td><td>-4.90272881</td><td>41.87438   </td><td> 99.88098  </td><td>122.7432   </td><td> 4583.167  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -79.90948 </td><td>-3.37990119</td><td>42.36449   </td><td>106.76617  </td><td>127.6823   </td><td> 4619.376  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -67.85804 </td><td>11.93074908</td><td>52.94551   </td><td>143.66126  </td><td>129.7910   </td><td>23494.327  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -64.47736 </td><td> 0.07749401</td><td>49.63625   </td><td>131.62484  </td><td>124.3412   </td><td>23534.333  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -72.82860 </td><td>-5.81229965</td><td>38.26452   </td><td> 94.70565  </td><td>110.9181   </td><td> 3469.296  </td></tr>\n",
       "\t<tr><th scope=row></th><td>-209.00863 </td><td> 5.72272453</td><td>50.03919   </td><td>125.57868  </td><td>121.2183   </td><td>23538.402  </td></tr>\n",
       "\t<tr><th scope=row></th><td>-219.06962 </td><td> 0.97213669</td><td>46.89009   </td><td>134.67741  </td><td>123.5320   </td><td>23532.423  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -67.25014 </td><td> 0.37288960</td><td>48.68154   </td><td>122.90621  </td><td>123.7185   </td><td>23591.940  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -73.89127 </td><td>-4.51361330</td><td>40.92245   </td><td> 96.49177  </td><td>114.9617   </td><td> 4627.187  </td></tr>\n",
       "\t<tr><th scope=row></th><td> -68.16454 </td><td>-2.76284266</td><td>40.76886   </td><td>103.73217  </td><td>117.9462   </td><td> 3393.786  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th></th><th scope=col>Min.</th><th scope=col>1st Qu.</th><th scope=col>Median</th><th scope=col>Mean</th><th scope=col>3rd Qu.</th><th scope=col>Max.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>LLR_summary_FALSE</th><td> -647.4824</td><td>-134.7026 </td><td>-109.7846 </td><td>-125.0911 </td><td>-90.10563 </td><td>698.378719</td></tr>\n",
       "\t<tr><th scope=row></th><td>-1035.9110</td><td>-134.0461 </td><td>-108.0262 </td><td>-124.9029 </td><td>-87.98946 </td><td>121.112408</td></tr>\n",
       "\t<tr><th scope=row></th><td> -770.3451</td><td>-127.8988 </td><td>-101.4215 </td><td>-118.1032 </td><td>-81.30104 </td><td> 64.410255</td></tr>\n",
       "\t<tr><th scope=row></th><td>-1317.5062</td><td>-130.8595 </td><td>-105.7273 </td><td>-122.0087 </td><td>-86.24114 </td><td>  8.362279</td></tr>\n",
       "\t<tr><th scope=row></th><td> -792.8637</td><td>-138.4605 </td><td>-112.0371 </td><td>-128.1616 </td><td>-93.57327 </td><td>740.026356</td></tr>\n",
       "\t<tr><th scope=row></th><td>-1050.1936</td><td>-128.7832 </td><td>-101.2164 </td><td>-117.3889 </td><td>-79.57209 </td><td>122.080749</td></tr>\n",
       "\t<tr><th scope=row></th><td>-1036.3454</td><td>-130.9569 </td><td>-102.7758 </td><td>-119.3544 </td><td>-82.07620 </td><td> 65.493018</td></tr>\n",
       "\t<tr><th scope=row></th><td> -990.4283</td><td>-132.9178 </td><td>-106.3486 </td><td>-121.5322 </td><td>-86.64966 </td><td>726.688267</td></tr>\n",
       "\t<tr><th scope=row></th><td> -937.1652</td><td>-135.0941 </td><td>-108.4102 </td><td>-124.6133 </td><td>-88.04073 </td><td> 71.897336</td></tr>\n",
       "\t<tr><th scope=row></th><td>-1246.9290</td><td>-132.5154 </td><td>-107.2790 </td><td>-123.4871 </td><td>-88.77522 </td><td>132.603149</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>Precision</th><th scope=col>Recall</th><th scope=col>F1</th><th scope=col>Accuracy</th><th scope=col>Type_I_Error</th><th scope=col>Type_II_Error</th><th scope=col>Tau</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9705882  </td><td>0.9499374  </td><td>0.9601518  </td><td>0.9797818  </td><td>0.009926629</td><td>0.05006258 </td><td>-42        </td></tr>\n",
       "\t<tr><td>0.9713168  </td><td>0.9324155  </td><td>0.9514687  </td><td>0.9756098  </td><td>0.009495037</td><td>0.06758448 </td><td>-36        </td></tr>\n",
       "\t<tr><td>0.9718121  </td><td>0.9061327  </td><td>0.9378238  </td><td>0.9691913  </td><td>0.009063444</td><td>0.09386733 </td><td>-20        </td></tr>\n",
       "\t<tr><td>0.9752781  </td><td>0.9874844  </td><td>0.9813433  </td><td>0.9903723  </td><td>0.008631852</td><td>0.01251564 </td><td>-53        </td></tr>\n",
       "\t<tr><td>0.9713930  </td><td>0.9774718  </td><td>0.9744230  </td><td>0.9868421  </td><td>0.009926629</td><td>0.02252816 </td><td>-57        </td></tr>\n",
       "\t<tr><td>0.9724656  </td><td>0.9724656  </td><td>0.9724656  </td><td>0.9858793  </td><td>0.009495037</td><td>0.02753442 </td><td>-40        </td></tr>\n",
       "\t<tr><td>0.9733840  </td><td>0.9612015  </td><td>0.9672544  </td><td>0.9833119  </td><td>0.009063444</td><td>0.03879850 </td><td>-41        </td></tr>\n",
       "\t<tr><td>0.9719745  </td><td>0.9549437  </td><td>0.9633838  </td><td>0.9813864  </td><td>0.009495037</td><td>0.04505632 </td><td>-44        </td></tr>\n",
       "\t<tr><td>0.9702073  </td><td>0.9374218  </td><td>0.9535328  </td><td>0.9765725  </td><td>0.009926629</td><td>0.06257822 </td><td>-37        </td></tr>\n",
       "\t<tr><td>0.9719745  </td><td>0.9549437  </td><td>0.9633838  </td><td>0.9813864  </td><td>0.009495037</td><td>0.04505632 </td><td>-42        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{r|llllll}\n",
       "  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max.\\\\\n",
       "\\hline\n",
       "\tLLR\\_summary\\_TRUE &  -80.10191  & -4.90272881 & 41.87438    &  99.88098   & 122.7432    &  4583.167  \\\\\n",
       "\t &  -79.90948  & -3.37990119 & 42.36449    & 106.76617   & 127.6823    &  4619.376  \\\\\n",
       "\t &  -67.85804  & 11.93074908 & 52.94551    & 143.66126   & 129.7910    & 23494.327  \\\\\n",
       "\t &  -64.47736  &  0.07749401 & 49.63625    & 131.62484   & 124.3412    & 23534.333  \\\\\n",
       "\t &  -72.82860  & -5.81229965 & 38.26452    &  94.70565   & 110.9181    &  3469.296  \\\\\n",
       "\t & -209.00863  &  5.72272453 & 50.03919    & 125.57868   & 121.2183    & 23538.402  \\\\\n",
       "\t & -219.06962  &  0.97213669 & 46.89009    & 134.67741   & 123.5320    & 23532.423  \\\\\n",
       "\t &  -67.25014  &  0.37288960 & 48.68154    & 122.90621   & 123.7185    & 23591.940  \\\\\n",
       "\t &  -73.89127  & -4.51361330 & 40.92245    &  96.49177   & 114.9617    &  4627.187  \\\\\n",
       "\t &  -68.16454  & -2.76284266 & 40.76886    & 103.73217   & 117.9462    &  3393.786  \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|llllll}\n",
       "  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max.\\\\\n",
       "\\hline\n",
       "\tLLR\\_summary\\_FALSE &  -647.4824 & -134.7026  & -109.7846  & -125.0911  & -90.10563  & 698.378719\\\\\n",
       "\t & -1035.9110 & -134.0461  & -108.0262  & -124.9029  & -87.98946  & 121.112408\\\\\n",
       "\t &  -770.3451 & -127.8988  & -101.4215  & -118.1032  & -81.30104  &  64.410255\\\\\n",
       "\t & -1317.5062 & -130.8595  & -105.7273  & -122.0087  & -86.24114  &   8.362279\\\\\n",
       "\t &  -792.8637 & -138.4605  & -112.0371  & -128.1616  & -93.57327  & 740.026356\\\\\n",
       "\t & -1050.1936 & -128.7832  & -101.2164  & -117.3889  & -79.57209  & 122.080749\\\\\n",
       "\t & -1036.3454 & -130.9569  & -102.7758  & -119.3544  & -82.07620  &  65.493018\\\\\n",
       "\t &  -990.4283 & -132.9178  & -106.3486  & -121.5322  & -86.64966  & 726.688267\\\\\n",
       "\t &  -937.1652 & -135.0941  & -108.4102  & -124.6133  & -88.04073  &  71.897336\\\\\n",
       "\t & -1246.9290 & -132.5154  & -107.2790  & -123.4871  & -88.77522  & 132.603149\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|lllllll}\n",
       " Precision & Recall & F1 & Accuracy & Type\\_I\\_Error & Type\\_II\\_Error & Tau\\\\\n",
       "\\hline\n",
       "\t 0.9705882   & 0.9499374   & 0.9601518   & 0.9797818   & 0.009926629 & 0.05006258  & -42        \\\\\n",
       "\t 0.9713168   & 0.9324155   & 0.9514687   & 0.9756098   & 0.009495037 & 0.06758448  & -36        \\\\\n",
       "\t 0.9718121   & 0.9061327   & 0.9378238   & 0.9691913   & 0.009063444 & 0.09386733  & -20        \\\\\n",
       "\t 0.9752781   & 0.9874844   & 0.9813433   & 0.9903723   & 0.008631852 & 0.01251564  & -53        \\\\\n",
       "\t 0.9713930   & 0.9774718   & 0.9744230   & 0.9868421   & 0.009926629 & 0.02252816  & -57        \\\\\n",
       "\t 0.9724656   & 0.9724656   & 0.9724656   & 0.9858793   & 0.009495037 & 0.02753442  & -40        \\\\\n",
       "\t 0.9733840   & 0.9612015   & 0.9672544   & 0.9833119   & 0.009063444 & 0.03879850  & -41        \\\\\n",
       "\t 0.9719745   & 0.9549437   & 0.9633838   & 0.9813864   & 0.009495037 & 0.04505632  & -44        \\\\\n",
       "\t 0.9702073   & 0.9374218   & 0.9535328   & 0.9765725   & 0.009926629 & 0.06257822  & -37        \\\\\n",
       "\t 0.9719745   & 0.9549437   & 0.9633838   & 0.9813864   & 0.009495037 & 0.04505632  & -42        \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| <!--/--> | Min. | 1st Qu. | Median | Mean | 3rd Qu. | Max. | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| LLR_summary_TRUE |  -80.10191  | -4.90272881 | 41.87438    |  99.88098   | 122.7432    |  4583.167   | \n",
       "|  |  -79.90948  | -3.37990119 | 42.36449    | 106.76617   | 127.6823    |  4619.376   | \n",
       "|  |  -67.85804  | 11.93074908 | 52.94551    | 143.66126   | 129.7910    | 23494.327   | \n",
       "|  |  -64.47736  |  0.07749401 | 49.63625    | 131.62484   | 124.3412    | 23534.333   | \n",
       "|  |  -72.82860  | -5.81229965 | 38.26452    |  94.70565   | 110.9181    |  3469.296   | \n",
       "|  | -209.00863  |  5.72272453 | 50.03919    | 125.57868   | 121.2183    | 23538.402   | \n",
       "|  | -219.06962  |  0.97213669 | 46.89009    | 134.67741   | 123.5320    | 23532.423   | \n",
       "|  |  -67.25014  |  0.37288960 | 48.68154    | 122.90621   | 123.7185    | 23591.940   | \n",
       "|  |  -73.89127  | -4.51361330 | 40.92245    |  96.49177   | 114.9617    |  4627.187   | \n",
       "|  |  -68.16454  | -2.76284266 | 40.76886    | 103.73217   | 117.9462    |  3393.786   | \n",
       "\n",
       "\n",
       "\n",
       "2. \n",
       "| <!--/--> | Min. | 1st Qu. | Median | Mean | 3rd Qu. | Max. | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| LLR_summary_FALSE |  -647.4824 | -134.7026  | -109.7846  | -125.0911  | -90.10563  | 698.378719 | \n",
       "|  | -1035.9110 | -134.0461  | -108.0262  | -124.9029  | -87.98946  | 121.112408 | \n",
       "|  |  -770.3451 | -127.8988  | -101.4215  | -118.1032  | -81.30104  |  64.410255 | \n",
       "|  | -1317.5062 | -130.8595  | -105.7273  | -122.0087  | -86.24114  |   8.362279 | \n",
       "|  |  -792.8637 | -138.4605  | -112.0371  | -128.1616  | -93.57327  | 740.026356 | \n",
       "|  | -1050.1936 | -128.7832  | -101.2164  | -117.3889  | -79.57209  | 122.080749 | \n",
       "|  | -1036.3454 | -130.9569  | -102.7758  | -119.3544  | -82.07620  |  65.493018 | \n",
       "|  |  -990.4283 | -132.9178  | -106.3486  | -121.5322  | -86.64966  | 726.688267 | \n",
       "|  |  -937.1652 | -135.0941  | -108.4102  | -124.6133  | -88.04073  |  71.897336 | \n",
       "|  | -1246.9290 | -132.5154  | -107.2790  | -123.4871  | -88.77522  | 132.603149 | \n",
       "\n",
       "\n",
       "\n",
       "3. \n",
       "Precision | Recall | F1 | Accuracy | Type_I_Error | Type_II_Error | Tau | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.9705882   | 0.9499374   | 0.9601518   | 0.9797818   | 0.009926629 | 0.05006258  | -42         | \n",
       "| 0.9713168   | 0.9324155   | 0.9514687   | 0.9756098   | 0.009495037 | 0.06758448  | -36         | \n",
       "| 0.9718121   | 0.9061327   | 0.9378238   | 0.9691913   | 0.009063444 | 0.09386733  | -20         | \n",
       "| 0.9752781   | 0.9874844   | 0.9813433   | 0.9903723   | 0.008631852 | 0.01251564  | -53         | \n",
       "| 0.9713930   | 0.9774718   | 0.9744230   | 0.9868421   | 0.009926629 | 0.02252816  | -57         | \n",
       "| 0.9724656   | 0.9724656   | 0.9724656   | 0.9858793   | 0.009495037 | 0.02753442  | -40         | \n",
       "| 0.9733840   | 0.9612015   | 0.9672544   | 0.9833119   | 0.009063444 | 0.03879850  | -41         | \n",
       "| 0.9719745   | 0.9549437   | 0.9633838   | 0.9813864   | 0.009495037 | 0.04505632  | -44         | \n",
       "| 0.9702073   | 0.9374218   | 0.9535328   | 0.9765725   | 0.009926629 | 0.06257822  | -37         | \n",
       "| 0.9719745   | 0.9549437   | 0.9633838   | 0.9813864   | 0.009495037 | 0.04505632  | -42         | \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "                       Min.     1st Qu.   Median      Mean  3rd Qu.      Max.\n",
       "LLR_summary_TRUE  -80.10191 -4.90272881 41.87438  99.88098 122.7432  4583.167\n",
       "                  -79.90948 -3.37990119 42.36449 106.76617 127.6823  4619.376\n",
       "                  -67.85804 11.93074908 52.94551 143.66126 129.7910 23494.327\n",
       "                  -64.47736  0.07749401 49.63625 131.62484 124.3412 23534.333\n",
       "                  -72.82860 -5.81229965 38.26452  94.70565 110.9181  3469.296\n",
       "                 -209.00863  5.72272453 50.03919 125.57868 121.2183 23538.402\n",
       "                 -219.06962  0.97213669 46.89009 134.67741 123.5320 23532.423\n",
       "                  -67.25014  0.37288960 48.68154 122.90621 123.7185 23591.940\n",
       "                  -73.89127 -4.51361330 40.92245  96.49177 114.9617  4627.187\n",
       "                  -68.16454 -2.76284266 40.76886 103.73217 117.9462  3393.786\n",
       "\n",
       "[[2]]\n",
       "                        Min.   1st Qu.    Median      Mean   3rd Qu.       Max.\n",
       "LLR_summary_FALSE  -647.4824 -134.7026 -109.7846 -125.0911 -90.10563 698.378719\n",
       "                  -1035.9110 -134.0461 -108.0262 -124.9029 -87.98946 121.112408\n",
       "                   -770.3451 -127.8988 -101.4215 -118.1032 -81.30104  64.410255\n",
       "                  -1317.5062 -130.8595 -105.7273 -122.0087 -86.24114   8.362279\n",
       "                   -792.8637 -138.4605 -112.0371 -128.1616 -93.57327 740.026356\n",
       "                  -1050.1936 -128.7832 -101.2164 -117.3889 -79.57209 122.080749\n",
       "                  -1036.3454 -130.9569 -102.7758 -119.3544 -82.07620  65.493018\n",
       "                   -990.4283 -132.9178 -106.3486 -121.5322 -86.64966 726.688267\n",
       "                   -937.1652 -135.0941 -108.4102 -124.6133 -88.04073  71.897336\n",
       "                  -1246.9290 -132.5154 -107.2790 -123.4871 -88.77522 132.603149\n",
       "\n",
       "[[3]]\n",
       "   Precision    Recall        F1  Accuracy Type_I_Error Type_II_Error Tau\n",
       "1  0.9705882 0.9499374 0.9601518 0.9797818  0.009926629    0.05006258 -42\n",
       "2  0.9713168 0.9324155 0.9514687 0.9756098  0.009495037    0.06758448 -36\n",
       "3  0.9718121 0.9061327 0.9378238 0.9691913  0.009063444    0.09386733 -20\n",
       "4  0.9752781 0.9874844 0.9813433 0.9903723  0.008631852    0.01251564 -53\n",
       "5  0.9713930 0.9774718 0.9744230 0.9868421  0.009926629    0.02252816 -57\n",
       "6  0.9724656 0.9724656 0.9724656 0.9858793  0.009495037    0.02753442 -40\n",
       "7  0.9733840 0.9612015 0.9672544 0.9833119  0.009063444    0.03879850 -41\n",
       "8  0.9719745 0.9549437 0.9633838 0.9813864  0.009495037    0.04505632 -44\n",
       "9  0.9702073 0.9374218 0.9535328 0.9765725  0.009926629    0.06257822 -37\n",
       "10 0.9719745 0.9549437 0.9633838 0.9813864  0.009495037    0.04505632 -42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Result_SRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sample into Naive Bayes <a name=\"Result_STR></a>\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<thead><tr><th></th><th scope=col>Min.</th><th scope=col>1st Qu.</th><th scope=col>Median</th><th scope=col>Mean</th><th scope=col>3rd Qu.</th><th scope=col>Max.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>LLR_summary_TRUE</th><td> -64.83709</td><td>15.0726805</td><td>59.07358  </td><td>119.50210 </td><td>135.2279  </td><td> 4649.010 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -85.51464</td><td>-7.4732290</td><td>42.23265  </td><td> 87.96905 </td><td>120.3682  </td><td> 1075.130 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-186.16040</td><td> 8.1291441</td><td>50.53009  </td><td>143.45602 </td><td>128.2131  </td><td>23481.197 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -68.28194</td><td> 8.5755695</td><td>56.37585  </td><td>151.47235 </td><td>137.1026  </td><td>23544.162 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -60.36812</td><td>13.1555625</td><td>57.90939  </td><td>110.04449 </td><td>132.6392  </td><td> 3560.873 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-191.76304</td><td>-0.7838034</td><td>50.91973  </td><td>105.70637 </td><td>127.3404  </td><td> 4630.565 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -66.13492</td><td> 0.5222096</td><td>45.51293  </td><td>104.15367 </td><td>125.6264  </td><td> 3591.619 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -77.28183</td><td> 1.6182141</td><td>45.16942  </td><td>107.31890 </td><td>126.3658  </td><td> 4551.144 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -67.07898</td><td> 7.0466481</td><td>53.08017  </td><td>132.48405 </td><td>133.1050  </td><td>23477.631 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -82.89271</td><td>-9.4357976</td><td>37.91926  </td><td> 95.30955 </td><td>112.5865  </td><td> 4672.140 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th></th><th scope=col>Min.</th><th scope=col>1st Qu.</th><th scope=col>Median</th><th scope=col>Mean</th><th scope=col>3rd Qu.</th><th scope=col>Max.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>LLR_summary_FALSE</th><td> -719.7324</td><td>-121.8402 </td><td> -93.49664</td><td>-110.9428 </td><td>-72.40613 </td><td>124.77430 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -698.5905</td><td>-140.4652 </td><td>-112.13980</td><td>-127.7353 </td><td>-91.49748 </td><td>709.97978 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -792.4196</td><td>-128.6790 </td><td>-100.57389</td><td>-117.0467 </td><td>-80.43293 </td><td>109.74143 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-1333.1049</td><td>-127.8229 </td><td>-100.87420</td><td>-118.1450 </td><td>-82.43001 </td><td> 79.68799 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-1067.9660</td><td>-119.4601 </td><td> -92.71254</td><td>-109.9996 </td><td>-73.61048 </td><td>143.21547 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-1374.7659</td><td>-135.2505 </td><td>-107.81797</td><td>-124.8884 </td><td>-87.67496 </td><td> 97.78870 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-1000.6829</td><td>-136.1519 </td><td>-108.77304</td><td>-124.6155 </td><td>-89.04934 </td><td>673.06148 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-1393.3057</td><td>-135.2406 </td><td>-107.26059</td><td>-124.8080 </td><td>-85.86994 </td><td> 98.47553 </td></tr>\n",
       "\t<tr><th scope=row></th><td>-1024.5525</td><td>-130.3093 </td><td>-102.70555</td><td>-119.2402 </td><td>-82.75255 </td><td>103.74321 </td></tr>\n",
       "\t<tr><th scope=row></th><td> -695.6576</td><td>-138.3926 </td><td>-111.50300</td><td>-128.6987 </td><td>-91.71044 </td><td> 73.99310 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>Precision</th><th scope=col>Recall</th><th scope=col>F1</th><th scope=col>Accuracy</th><th scope=col>Type_I_Error</th><th scope=col>Type_II_Error</th><th scope=col>Tau</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9723618  </td><td>0.9699248  </td><td>0.9711418  </td><td>0.9852233  </td><td>0.009503240</td><td>0.03007519 </td><td>-33        </td></tr>\n",
       "\t<tr><td>0.9696970  </td><td>0.9223058  </td><td>0.9454078  </td><td>0.9726951  </td><td>0.009935205</td><td>0.07769424 </td><td>-38        </td></tr>\n",
       "\t<tr><td>0.9719745  </td><td>0.9561404  </td><td>0.9639924  </td><td>0.9816897  </td><td>0.009503240</td><td>0.04385965 </td><td>-36        </td></tr>\n",
       "\t<tr><td>0.9690722  </td><td>0.9423559  </td><td>0.9555273  </td><td>0.9775137  </td><td>0.010367171</td><td>0.05764411 </td><td>-31        </td></tr>\n",
       "\t<tr><td>0.9707379  </td><td>0.9561404  </td><td>0.9633838  </td><td>0.9813685  </td><td>0.009935205</td><td>0.04385965 </td><td>-30        </td></tr>\n",
       "\t<tr><td>0.9693593  </td><td>0.8721805  </td><td>0.9182058  </td><td>0.9601670  </td><td>0.009503240</td><td>0.12781955 </td><td>-26        </td></tr>\n",
       "\t<tr><td>0.9707751  </td><td>0.9573935  </td><td>0.9640379  </td><td>0.9816897  </td><td>0.009935205</td><td>0.04260652 </td><td>-40        </td></tr>\n",
       "\t<tr><td>0.9716495  </td><td>0.9448622  </td><td>0.9580686  </td><td>0.9787986  </td><td>0.009503240</td><td>0.05513784 </td><td>-38        </td></tr>\n",
       "\t<tr><td>0.9714286  </td><td>0.9799499  </td><td>0.9756706  </td><td>0.9874719  </td><td>0.009935205</td><td>0.02005013 </td><td>-50        </td></tr>\n",
       "\t<tr><td>0.9678616  </td><td>0.9812030  </td><td>0.9744866  </td><td>0.9868294  </td><td>0.011231102</td><td>0.01879699 </td><td>-57        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{r|llllll}\n",
       "  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max.\\\\\n",
       "\\hline\n",
       "\tLLR\\_summary\\_TRUE &  -64.83709 & 15.0726805 & 59.07358   & 119.50210  & 135.2279   &  4649.010 \\\\\n",
       "\t &  -85.51464 & -7.4732290 & 42.23265   &  87.96905  & 120.3682   &  1075.130 \\\\\n",
       "\t & -186.16040 &  8.1291441 & 50.53009   & 143.45602  & 128.2131   & 23481.197 \\\\\n",
       "\t &  -68.28194 &  8.5755695 & 56.37585   & 151.47235  & 137.1026   & 23544.162 \\\\\n",
       "\t &  -60.36812 & 13.1555625 & 57.90939   & 110.04449  & 132.6392   &  3560.873 \\\\\n",
       "\t & -191.76304 & -0.7838034 & 50.91973   & 105.70637  & 127.3404   &  4630.565 \\\\\n",
       "\t &  -66.13492 &  0.5222096 & 45.51293   & 104.15367  & 125.6264   &  3591.619 \\\\\n",
       "\t &  -77.28183 &  1.6182141 & 45.16942   & 107.31890  & 126.3658   &  4551.144 \\\\\n",
       "\t &  -67.07898 &  7.0466481 & 53.08017   & 132.48405  & 133.1050   & 23477.631 \\\\\n",
       "\t &  -82.89271 & -9.4357976 & 37.91926   &  95.30955  & 112.5865   &  4672.140 \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|llllll}\n",
       "  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max.\\\\\n",
       "\\hline\n",
       "\tLLR\\_summary\\_FALSE &  -719.7324 & -121.8402  &  -93.49664 & -110.9428  & -72.40613  & 124.77430 \\\\\n",
       "\t &  -698.5905 & -140.4652  & -112.13980 & -127.7353  & -91.49748  & 709.97978 \\\\\n",
       "\t &  -792.4196 & -128.6790  & -100.57389 & -117.0467  & -80.43293  & 109.74143 \\\\\n",
       "\t & -1333.1049 & -127.8229  & -100.87420 & -118.1450  & -82.43001  &  79.68799 \\\\\n",
       "\t & -1067.9660 & -119.4601  &  -92.71254 & -109.9996  & -73.61048  & 143.21547 \\\\\n",
       "\t & -1374.7659 & -135.2505  & -107.81797 & -124.8884  & -87.67496  &  97.78870 \\\\\n",
       "\t & -1000.6829 & -136.1519  & -108.77304 & -124.6155  & -89.04934  & 673.06148 \\\\\n",
       "\t & -1393.3057 & -135.2406  & -107.26059 & -124.8080  & -85.86994  &  98.47553 \\\\\n",
       "\t & -1024.5525 & -130.3093  & -102.70555 & -119.2402  & -82.75255  & 103.74321 \\\\\n",
       "\t &  -695.6576 & -138.3926  & -111.50300 & -128.6987  & -91.71044  &  73.99310 \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|lllllll}\n",
       " Precision & Recall & F1 & Accuracy & Type\\_I\\_Error & Type\\_II\\_Error & Tau\\\\\n",
       "\\hline\n",
       "\t 0.9723618   & 0.9699248   & 0.9711418   & 0.9852233   & 0.009503240 & 0.03007519  & -33        \\\\\n",
       "\t 0.9696970   & 0.9223058   & 0.9454078   & 0.9726951   & 0.009935205 & 0.07769424  & -38        \\\\\n",
       "\t 0.9719745   & 0.9561404   & 0.9639924   & 0.9816897   & 0.009503240 & 0.04385965  & -36        \\\\\n",
       "\t 0.9690722   & 0.9423559   & 0.9555273   & 0.9775137   & 0.010367171 & 0.05764411  & -31        \\\\\n",
       "\t 0.9707379   & 0.9561404   & 0.9633838   & 0.9813685   & 0.009935205 & 0.04385965  & -30        \\\\\n",
       "\t 0.9693593   & 0.8721805   & 0.9182058   & 0.9601670   & 0.009503240 & 0.12781955  & -26        \\\\\n",
       "\t 0.9707751   & 0.9573935   & 0.9640379   & 0.9816897   & 0.009935205 & 0.04260652  & -40        \\\\\n",
       "\t 0.9716495   & 0.9448622   & 0.9580686   & 0.9787986   & 0.009503240 & 0.05513784  & -38        \\\\\n",
       "\t 0.9714286   & 0.9799499   & 0.9756706   & 0.9874719   & 0.009935205 & 0.02005013  & -50        \\\\\n",
       "\t 0.9678616   & 0.9812030   & 0.9744866   & 0.9868294   & 0.011231102 & 0.01879699  & -57        \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| <!--/--> | Min. | 1st Qu. | Median | Mean | 3rd Qu. | Max. | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| LLR_summary_TRUE |  -64.83709 | 15.0726805 | 59.07358   | 119.50210  | 135.2279   |  4649.010  | \n",
       "|  |  -85.51464 | -7.4732290 | 42.23265   |  87.96905  | 120.3682   |  1075.130  | \n",
       "|  | -186.16040 |  8.1291441 | 50.53009   | 143.45602  | 128.2131   | 23481.197  | \n",
       "|  |  -68.28194 |  8.5755695 | 56.37585   | 151.47235  | 137.1026   | 23544.162  | \n",
       "|  |  -60.36812 | 13.1555625 | 57.90939   | 110.04449  | 132.6392   |  3560.873  | \n",
       "|  | -191.76304 | -0.7838034 | 50.91973   | 105.70637  | 127.3404   |  4630.565  | \n",
       "|  |  -66.13492 |  0.5222096 | 45.51293   | 104.15367  | 125.6264   |  3591.619  | \n",
       "|  |  -77.28183 |  1.6182141 | 45.16942   | 107.31890  | 126.3658   |  4551.144  | \n",
       "|  |  -67.07898 |  7.0466481 | 53.08017   | 132.48405  | 133.1050   | 23477.631  | \n",
       "|  |  -82.89271 | -9.4357976 | 37.91926   |  95.30955  | 112.5865   |  4672.140  | \n",
       "\n",
       "\n",
       "\n",
       "2. \n",
       "| <!--/--> | Min. | 1st Qu. | Median | Mean | 3rd Qu. | Max. | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| LLR_summary_FALSE |  -719.7324 | -121.8402  |  -93.49664 | -110.9428  | -72.40613  | 124.77430  | \n",
       "|  |  -698.5905 | -140.4652  | -112.13980 | -127.7353  | -91.49748  | 709.97978  | \n",
       "|  |  -792.4196 | -128.6790  | -100.57389 | -117.0467  | -80.43293  | 109.74143  | \n",
       "|  | -1333.1049 | -127.8229  | -100.87420 | -118.1450  | -82.43001  |  79.68799  | \n",
       "|  | -1067.9660 | -119.4601  |  -92.71254 | -109.9996  | -73.61048  | 143.21547  | \n",
       "|  | -1374.7659 | -135.2505  | -107.81797 | -124.8884  | -87.67496  |  97.78870  | \n",
       "|  | -1000.6829 | -136.1519  | -108.77304 | -124.6155  | -89.04934  | 673.06148  | \n",
       "|  | -1393.3057 | -135.2406  | -107.26059 | -124.8080  | -85.86994  |  98.47553  | \n",
       "|  | -1024.5525 | -130.3093  | -102.70555 | -119.2402  | -82.75255  | 103.74321  | \n",
       "|  |  -695.6576 | -138.3926  | -111.50300 | -128.6987  | -91.71044  |  73.99310  | \n",
       "\n",
       "\n",
       "\n",
       "3. \n",
       "Precision | Recall | F1 | Accuracy | Type_I_Error | Type_II_Error | Tau | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.9723618   | 0.9699248   | 0.9711418   | 0.9852233   | 0.009503240 | 0.03007519  | -33         | \n",
       "| 0.9696970   | 0.9223058   | 0.9454078   | 0.9726951   | 0.009935205 | 0.07769424  | -38         | \n",
       "| 0.9719745   | 0.9561404   | 0.9639924   | 0.9816897   | 0.009503240 | 0.04385965  | -36         | \n",
       "| 0.9690722   | 0.9423559   | 0.9555273   | 0.9775137   | 0.010367171 | 0.05764411  | -31         | \n",
       "| 0.9707379   | 0.9561404   | 0.9633838   | 0.9813685   | 0.009935205 | 0.04385965  | -30         | \n",
       "| 0.9693593   | 0.8721805   | 0.9182058   | 0.9601670   | 0.009503240 | 0.12781955  | -26         | \n",
       "| 0.9707751   | 0.9573935   | 0.9640379   | 0.9816897   | 0.009935205 | 0.04260652  | -40         | \n",
       "| 0.9716495   | 0.9448622   | 0.9580686   | 0.9787986   | 0.009503240 | 0.05513784  | -38         | \n",
       "| 0.9714286   | 0.9799499   | 0.9756706   | 0.9874719   | 0.009935205 | 0.02005013  | -50         | \n",
       "| 0.9678616   | 0.9812030   | 0.9744866   | 0.9868294   | 0.011231102 | 0.01879699  | -57         | \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "                       Min.    1st Qu.   Median      Mean  3rd Qu.      Max.\n",
       "LLR_summary_TRUE  -64.83709 15.0726805 59.07358 119.50210 135.2279  4649.010\n",
       "                  -85.51464 -7.4732290 42.23265  87.96905 120.3682  1075.130\n",
       "                 -186.16040  8.1291441 50.53009 143.45602 128.2131 23481.197\n",
       "                  -68.28194  8.5755695 56.37585 151.47235 137.1026 23544.162\n",
       "                  -60.36812 13.1555625 57.90939 110.04449 132.6392  3560.873\n",
       "                 -191.76304 -0.7838034 50.91973 105.70637 127.3404  4630.565\n",
       "                  -66.13492  0.5222096 45.51293 104.15367 125.6264  3591.619\n",
       "                  -77.28183  1.6182141 45.16942 107.31890 126.3658  4551.144\n",
       "                  -67.07898  7.0466481 53.08017 132.48405 133.1050 23477.631\n",
       "                  -82.89271 -9.4357976 37.91926  95.30955 112.5865  4672.140\n",
       "\n",
       "[[2]]\n",
       "                        Min.   1st Qu.     Median      Mean   3rd Qu.      Max.\n",
       "LLR_summary_FALSE  -719.7324 -121.8402  -93.49664 -110.9428 -72.40613 124.77430\n",
       "                   -698.5905 -140.4652 -112.13980 -127.7353 -91.49748 709.97978\n",
       "                   -792.4196 -128.6790 -100.57389 -117.0467 -80.43293 109.74143\n",
       "                  -1333.1049 -127.8229 -100.87420 -118.1450 -82.43001  79.68799\n",
       "                  -1067.9660 -119.4601  -92.71254 -109.9996 -73.61048 143.21547\n",
       "                  -1374.7659 -135.2505 -107.81797 -124.8884 -87.67496  97.78870\n",
       "                  -1000.6829 -136.1519 -108.77304 -124.6155 -89.04934 673.06148\n",
       "                  -1393.3057 -135.2406 -107.26059 -124.8080 -85.86994  98.47553\n",
       "                  -1024.5525 -130.3093 -102.70555 -119.2402 -82.75255 103.74321\n",
       "                   -695.6576 -138.3926 -111.50300 -128.6987 -91.71044  73.99310\n",
       "\n",
       "[[3]]\n",
       "   Precision    Recall        F1  Accuracy Type_I_Error Type_II_Error Tau\n",
       "1  0.9723618 0.9699248 0.9711418 0.9852233  0.009503240    0.03007519 -33\n",
       "2  0.9696970 0.9223058 0.9454078 0.9726951  0.009935205    0.07769424 -38\n",
       "3  0.9719745 0.9561404 0.9639924 0.9816897  0.009503240    0.04385965 -36\n",
       "4  0.9690722 0.9423559 0.9555273 0.9775137  0.010367171    0.05764411 -31\n",
       "5  0.9707379 0.9561404 0.9633838 0.9813685  0.009935205    0.04385965 -30\n",
       "6  0.9693593 0.8721805 0.9182058 0.9601670  0.009503240    0.12781955 -26\n",
       "7  0.9707751 0.9573935 0.9640379 0.9816897  0.009935205    0.04260652 -40\n",
       "8  0.9716495 0.9448622 0.9580686 0.9787986  0.009503240    0.05513784 -38\n",
       "9  0.9714286 0.9799499 0.9756706 0.9874719  0.009935205    0.02005013 -50\n",
       "10 0.9678616 0.9812030 0.9744866 0.9868294  0.011231102    0.01879699 -57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Result_STR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent t-test <a name=\"ttest\"></a>\n",
    "\n",
    "[Back to Method](#method)\n",
    "\n",
    "[Back to Result](#Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test w/ LLR statistics for spam emails between SRS and Stratified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Min.</th><th scope=col>1st Qu.</th><th scope=col>Median</th><th scope=col>Mean</th><th scope=col>3rd Qu.</th><th scope=col>Max.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>statistic</th><td>-0.2107331</td><td>-1.239668 </td><td>-1.699614 </td><td>0.03024803</td><td>-2.096873 </td><td>0.9288854 </td></tr>\n",
       "\t<tr><th scope=row>parameter</th><td>17.43035</td><td>15.74501</td><td>16.20618</td><td>17.57633</td><td>16.94718</td><td>17.919  </td></tr>\n",
       "\t<tr><th scope=row>p.value</th><td>0.8355395 </td><td>0.2332577 </td><td>0.1083136 </td><td>0.9762101 </td><td>0.05131808</td><td>0.3652933 </td></tr>\n",
       "\t<tr><th scope=row>conf.int</th><td>-57.43389, 46.98471     </td><td>-10.502775, 2.758257    </td><td>-10.407011, 1.139852    </td><td>-17.88881, 18.41053     </td><td>-12.38409544, 0.03957233</td><td>-5195.284, 13425.437    </td></tr>\n",
       "\t<tr><th scope=row>estimate</th><td>-100.25596, -95.03137</td><td>-0.2295392, 3.6427199</td><td>45.23873, 49.87231   </td><td>116.0025, 115.7417   </td><td>121.6852, 127.8575   </td><td>13838.424, 9723.347  </td></tr>\n",
       "\t<tr><th scope=row>null.value</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>alternative</th><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td></tr>\n",
       "\t<tr><th scope=row>method</th><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td></tr>\n",
       "\t<tr><th scope=row>data.name</th><td>Result_SRS[[1]][, x] and Result_STR[[1]][, x]</td><td>Result_SRS[[1]][, x] and Result_STR[[1]][, x]</td><td>Result_SRS[[1]][, x] and Result_STR[[1]][, x]</td><td>Result_SRS[[1]][, x] and Result_STR[[1]][, x]</td><td>Result_SRS[[1]][, x] and Result_STR[[1]][, x]</td><td>Result_SRS[[1]][, x] and Result_STR[[1]][, x]</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max.\\\\\n",
       "\\hline\n",
       "\tstatistic & -0.2107331 & -1.239668  & -1.699614  & 0.03024803 & -2.096873  & 0.9288854 \\\\\n",
       "\tparameter & 17.43035 & 15.74501 & 16.20618 & 17.57633 & 16.94718 & 17.919  \\\\\n",
       "\tp.value & 0.8355395  & 0.2332577  & 0.1083136  & 0.9762101  & 0.05131808 & 0.3652933 \\\\\n",
       "\tconf.int & -57.43389, 46.98471      & -10.502775, 2.758257     & -10.407011, 1.139852     & -17.88881, 18.41053      & -12.38409544, 0.03957233 & -5195.284, 13425.437    \\\\\n",
       "\testimate & -100.25596, -95.03137 & -0.2295392, 3.6427199 & 45.23873, 49.87231    & 116.0025, 115.7417    & 121.6852, 127.8575    & 13838.424, 9723.347  \\\\\n",
       "\tnull.value & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\talternative & two.sided & two.sided & two.sided & two.sided & two.sided & two.sided\\\\\n",
       "\tmethod & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test\\\\\n",
       "\tdata.name & Result\\_SRS{[}{[}1{]}{]}{[}, x{]} and Result\\_STR{[}{[}1{]}{]}{[}, x{]} & Result\\_SRS{[}{[}1{]}{]}{[}, x{]} and Result\\_STR{[}{[}1{]}{]}{[}, x{]} & Result\\_SRS{[}{[}1{]}{]}{[}, x{]} and Result\\_STR{[}{[}1{]}{]}{[}, x{]} & Result\\_SRS{[}{[}1{]}{]}{[}, x{]} and Result\\_STR{[}{[}1{]}{]}{[}, x{]} & Result\\_SRS{[}{[}1{]}{]}{[}, x{]} and Result\\_STR{[}{[}1{]}{]}{[}, x{]} & Result\\_SRS{[}{[}1{]}{]}{[}, x{]} and Result\\_STR{[}{[}1{]}{]}{[}, x{]}\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Min. | 1st Qu. | Median | Mean | 3rd Qu. | Max. | \n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| statistic | -0.2107331 | -1.239668  | -1.699614  | 0.03024803 | -2.096873  | 0.9288854  | \n",
       "| parameter | 17.43035 | 15.74501 | 16.20618 | 17.57633 | 16.94718 | 17.919   | \n",
       "| p.value | 0.8355395  | 0.2332577  | 0.1083136  | 0.9762101  | 0.05131808 | 0.3652933  | \n",
       "| conf.int | -57.43389, 46.98471      | -10.502775, 2.758257     | -10.407011, 1.139852     | -17.88881, 18.41053      | -12.38409544, 0.03957233 | -5195.284, 13425.437     | \n",
       "| estimate | -100.25596, -95.03137 | -0.2295392, 3.6427199 | 45.23873, 49.87231    | 116.0025, 115.7417    | 121.6852, 127.8575    | 13838.424, 9723.347   | \n",
       "| null.value | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| alternative | two.sided | two.sided | two.sided | two.sided | two.sided | two.sided | \n",
       "| method | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | \n",
       "| data.name | Result_SRS[[1]][, x] and Result_STR[[1]][, x] | Result_SRS[[1]][, x] and Result_STR[[1]][, x] | Result_SRS[[1]][, x] and Result_STR[[1]][, x] | Result_SRS[[1]][, x] and Result_STR[[1]][, x] | Result_SRS[[1]][, x] and Result_STR[[1]][, x] | Result_SRS[[1]][, x] and Result_STR[[1]][, x] | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            Min.                                         \n",
       "statistic   -0.2107331                                   \n",
       "parameter   17.43035                                     \n",
       "p.value     0.8355395                                    \n",
       "conf.int    -57.43389, 46.98471                          \n",
       "estimate    -100.25596, -95.03137                        \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[1]][, x] and Result_STR[[1]][, x]\n",
       "            1st Qu.                                      \n",
       "statistic   -1.239668                                    \n",
       "parameter   15.74501                                     \n",
       "p.value     0.2332577                                    \n",
       "conf.int    -10.502775, 2.758257                         \n",
       "estimate    -0.2295392, 3.6427199                        \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[1]][, x] and Result_STR[[1]][, x]\n",
       "            Median                                       \n",
       "statistic   -1.699614                                    \n",
       "parameter   16.20618                                     \n",
       "p.value     0.1083136                                    \n",
       "conf.int    -10.407011, 1.139852                         \n",
       "estimate    45.23873, 49.87231                           \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[1]][, x] and Result_STR[[1]][, x]\n",
       "            Mean                                         \n",
       "statistic   0.03024803                                   \n",
       "parameter   17.57633                                     \n",
       "p.value     0.9762101                                    \n",
       "conf.int    -17.88881, 18.41053                          \n",
       "estimate    116.0025, 115.7417                           \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[1]][, x] and Result_STR[[1]][, x]\n",
       "            3rd Qu.                                      \n",
       "statistic   -2.096873                                    \n",
       "parameter   16.94718                                     \n",
       "p.value     0.05131808                                   \n",
       "conf.int    -12.38409544, 0.03957233                     \n",
       "estimate    121.6852, 127.8575                           \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[1]][, x] and Result_STR[[1]][, x]\n",
       "            Max.                                         \n",
       "statistic   0.9288854                                    \n",
       "parameter   17.919                                       \n",
       "p.value     0.3652933                                    \n",
       "conf.int    -5195.284, 13425.437                         \n",
       "estimate    13838.424, 9723.347                          \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[1]][, x] and Result_STR[[1]][, x]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttest_LLR_TRUE = sapply(1:6, function(x) t.test(Result_SRS[[1]][,x], Result_STR[[1]][,x], alpha=0.05))\n",
    "colnames(ttest_LLR_TRUE) = c('Min.','1st Qu.','Median','Mean','3rd Qu.','Max.')\n",
    "ttest_LLR_TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test w/ LLR statistics for ham(non-spam) emails between SRS and Stratified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Min.</th><th scope=col>1st Qu.</th><th scope=col>Median</th><th scope=col>Mean</th><th scope=col>3rd Qu.</th><th scope=col>Max.</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>statistic</th><td>0.2489318 </td><td>-0.5210641</td><td>-1.021098 </td><td>-0.7848173</td><td>-1.055579 </td><td>0.4265713 </td></tr>\n",
       "\t<tr><th scope=row>parameter</th><td>16.5414 </td><td>12.5294 </td><td>13.51201</td><td>13.52562</td><td>15.22834</td><td>17.18175</td></tr>\n",
       "\t<tr><th scope=row>p.value</th><td>0.8064781</td><td>0.611408 </td><td>0.3251428</td><td>0.4460899</td><td>0.3076202</td><td>0.6749884</td></tr>\n",
       "\t<tr><th scope=row>conf.int</th><td>-206.5245, 261.6461</td><td>-6.515635, 3.991153</td><td>-7.821697, 2.787797</td><td>-6.931179, 3.226517</td><td>-8.111659, 2.733625</td><td>-211.5246, 318.8429</td></tr>\n",
       "\t<tr><th scope=row>estimate</th><td>-982.517, -1010.078 </td><td>-132.6235, -131.3612</td><td>-106.3027, -103.7857</td><td>-122.4643, -120.6120</td><td>-86.43244, -83.74343</td><td>275.1053, 221.4461  </td></tr>\n",
       "\t<tr><th scope=row>null.value</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>alternative</th><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td></tr>\n",
       "\t<tr><th scope=row>method</th><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td></tr>\n",
       "\t<tr><th scope=row>data.name</th><td>Result_SRS[[2]][, x] and Result_STR[[2]][, x]</td><td>Result_SRS[[2]][, x] and Result_STR[[2]][, x]</td><td>Result_SRS[[2]][, x] and Result_STR[[2]][, x]</td><td>Result_SRS[[2]][, x] and Result_STR[[2]][, x]</td><td>Result_SRS[[2]][, x] and Result_STR[[2]][, x]</td><td>Result_SRS[[2]][, x] and Result_STR[[2]][, x]</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max.\\\\\n",
       "\\hline\n",
       "\tstatistic & 0.2489318  & -0.5210641 & -1.021098  & -0.7848173 & -1.055579  & 0.4265713 \\\\\n",
       "\tparameter & 16.5414  & 12.5294  & 13.51201 & 13.52562 & 15.22834 & 17.18175\\\\\n",
       "\tp.value & 0.8064781 & 0.611408  & 0.3251428 & 0.4460899 & 0.3076202 & 0.6749884\\\\\n",
       "\tconf.int & -206.5245, 261.6461 & -6.515635, 3.991153 & -7.821697, 2.787797 & -6.931179, 3.226517 & -8.111659, 2.733625 & -211.5246, 318.8429\\\\\n",
       "\testimate & -982.517, -1010.078  & -132.6235, -131.3612 & -106.3027, -103.7857 & -122.4643, -120.6120 & -86.43244, -83.74343 & 275.1053, 221.4461  \\\\\n",
       "\tnull.value & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\talternative & two.sided & two.sided & two.sided & two.sided & two.sided & two.sided\\\\\n",
       "\tmethod & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test\\\\\n",
       "\tdata.name & Result\\_SRS{[}{[}2{]}{]}{[}, x{]} and Result\\_STR{[}{[}2{]}{]}{[}, x{]} & Result\\_SRS{[}{[}2{]}{]}{[}, x{]} and Result\\_STR{[}{[}2{]}{]}{[}, x{]} & Result\\_SRS{[}{[}2{]}{]}{[}, x{]} and Result\\_STR{[}{[}2{]}{]}{[}, x{]} & Result\\_SRS{[}{[}2{]}{]}{[}, x{]} and Result\\_STR{[}{[}2{]}{]}{[}, x{]} & Result\\_SRS{[}{[}2{]}{]}{[}, x{]} and Result\\_STR{[}{[}2{]}{]}{[}, x{]} & Result\\_SRS{[}{[}2{]}{]}{[}, x{]} and Result\\_STR{[}{[}2{]}{]}{[}, x{]}\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Min. | 1st Qu. | Median | Mean | 3rd Qu. | Max. | \n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| statistic | 0.2489318  | -0.5210641 | -1.021098  | -0.7848173 | -1.055579  | 0.4265713  | \n",
       "| parameter | 16.5414  | 12.5294  | 13.51201 | 13.52562 | 15.22834 | 17.18175 | \n",
       "| p.value | 0.8064781 | 0.611408  | 0.3251428 | 0.4460899 | 0.3076202 | 0.6749884 | \n",
       "| conf.int | -206.5245, 261.6461 | -6.515635, 3.991153 | -7.821697, 2.787797 | -6.931179, 3.226517 | -8.111659, 2.733625 | -211.5246, 318.8429 | \n",
       "| estimate | -982.517, -1010.078  | -132.6235, -131.3612 | -106.3027, -103.7857 | -122.4643, -120.6120 | -86.43244, -83.74343 | 275.1053, 221.4461   | \n",
       "| null.value | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| alternative | two.sided | two.sided | two.sided | two.sided | two.sided | two.sided | \n",
       "| method | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | \n",
       "| data.name | Result_SRS[[2]][, x] and Result_STR[[2]][, x] | Result_SRS[[2]][, x] and Result_STR[[2]][, x] | Result_SRS[[2]][, x] and Result_STR[[2]][, x] | Result_SRS[[2]][, x] and Result_STR[[2]][, x] | Result_SRS[[2]][, x] and Result_STR[[2]][, x] | Result_SRS[[2]][, x] and Result_STR[[2]][, x] | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            Min.                                         \n",
       "statistic   0.2489318                                    \n",
       "parameter   16.5414                                      \n",
       "p.value     0.8064781                                    \n",
       "conf.int    -206.5245, 261.6461                          \n",
       "estimate    -982.517, -1010.078                          \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[2]][, x] and Result_STR[[2]][, x]\n",
       "            1st Qu.                                      \n",
       "statistic   -0.5210641                                   \n",
       "parameter   12.5294                                      \n",
       "p.value     0.611408                                     \n",
       "conf.int    -6.515635, 3.991153                          \n",
       "estimate    -132.6235, -131.3612                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[2]][, x] and Result_STR[[2]][, x]\n",
       "            Median                                       \n",
       "statistic   -1.021098                                    \n",
       "parameter   13.51201                                     \n",
       "p.value     0.3251428                                    \n",
       "conf.int    -7.821697, 2.787797                          \n",
       "estimate    -106.3027, -103.7857                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[2]][, x] and Result_STR[[2]][, x]\n",
       "            Mean                                         \n",
       "statistic   -0.7848173                                   \n",
       "parameter   13.52562                                     \n",
       "p.value     0.4460899                                    \n",
       "conf.int    -6.931179, 3.226517                          \n",
       "estimate    -122.4643, -120.6120                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[2]][, x] and Result_STR[[2]][, x]\n",
       "            3rd Qu.                                      \n",
       "statistic   -1.055579                                    \n",
       "parameter   15.22834                                     \n",
       "p.value     0.3076202                                    \n",
       "conf.int    -8.111659, 2.733625                          \n",
       "estimate    -86.43244, -83.74343                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[2]][, x] and Result_STR[[2]][, x]\n",
       "            Max.                                         \n",
       "statistic   0.4265713                                    \n",
       "parameter   17.18175                                     \n",
       "p.value     0.6749884                                    \n",
       "conf.int    -211.5246, 318.8429                          \n",
       "estimate    275.1053, 221.4461                           \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[2]][, x] and Result_STR[[2]][, x]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttest_LLR_FALSE = sapply(1:6, function(x) t.test(Result_SRS[[2]][,x], Result_STR[[2]][,x], alpha=0.05))\n",
    "colnames(ttest_LLR_FALSE) = c('Min.','1st Qu.','Median','Mean','3rd Qu.','Max.')\n",
    "ttest_LLR_FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test w/ evaluation metrics between SRS and Stratified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Precision</th><th scope=col>Recall</th><th scope=col>F1</th><th scope=col>Accuracy</th><th scope=col>Type_I_Error</th><th scope=col>Type_II_Error</th><th scope=col>Tau</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>statistic</th><td>2.386749  </td><td>0.410984  </td><td>0.5274041 </td><td>0.5257885 </td><td>-2.218921 </td><td>-0.410984 </td><td>-0.7618766</td></tr>\n",
       "\t<tr><th scope=row>parameter</th><td>17.99998</td><td>16.60962</td><td>16.62666</td><td>16.81568</td><td>17.14343</td><td>16.60962</td><td>17.93871</td></tr>\n",
       "\t<tr><th scope=row>p.value</th><td>0.02818236</td><td>0.686338  </td><td>0.60488   </td><td>0.6059003 </td><td>0.0402691 </td><td>0.686338  </td><td>0.4560344 </td></tr>\n",
       "\t<tr><th scope=row>conf.int</th><td>0.0001853436, 0.0029100128  </td><td>-0.02152665, 0.03191903     </td><td>-0.01061803, 0.01767970     </td><td>-0.005093135, 0.008470520   </td><td>-9.425972e-04, -2.405827e-05</td><td>-0.03191903, 0.02152665     </td><td>-12.402184, 5.802184        </td></tr>\n",
       "\t<tr><th scope=row>estimate</th><td>0.9720394, 0.9704917    </td><td>0.9534418, 0.9482456    </td><td>0.9625231, 0.9589923    </td><td>0.9810334, 0.9793447    </td><td>0.009451877, 0.009935205</td><td>0.04655820, 0.05175439  </td><td>-41.2, -37.9            </td></tr>\n",
       "\t<tr><th scope=row>null.value</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>alternative</th><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td><td>two.sided</td></tr>\n",
       "\t<tr><th scope=row>method</th><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td><td>Welch Two Sample t-test</td></tr>\n",
       "\t<tr><th scope=row>data.name</th><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td><td>Result_SRS[[3]][, x] and Result_STR[[3]][, x]</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       "  & Precision & Recall & F1 & Accuracy & Type\\_I\\_Error & Type\\_II\\_Error & Tau\\\\\n",
       "\\hline\n",
       "\tstatistic & 2.386749   & 0.410984   & 0.5274041  & 0.5257885  & -2.218921  & -0.410984  & -0.7618766\\\\\n",
       "\tparameter & 17.99998 & 16.60962 & 16.62666 & 16.81568 & 17.14343 & 16.60962 & 17.93871\\\\\n",
       "\tp.value & 0.02818236 & 0.686338   & 0.60488    & 0.6059003  & 0.0402691  & 0.686338   & 0.4560344 \\\\\n",
       "\tconf.int & 0.0001853436, 0.0029100128   & -0.02152665, 0.03191903      & -0.01061803, 0.01767970      & -0.005093135, 0.008470520    & -9.425972e-04, -2.405827e-05 & -0.03191903, 0.02152665      & -12.402184, 5.802184        \\\\\n",
       "\testimate & 0.9720394, 0.9704917     & 0.9534418, 0.9482456     & 0.9625231, 0.9589923     & 0.9810334, 0.9793447     & 0.009451877, 0.009935205 & 0.04655820, 0.05175439   & -41.2, -37.9            \\\\\n",
       "\tnull.value & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\talternative & two.sided & two.sided & two.sided & two.sided & two.sided & two.sided & two.sided\\\\\n",
       "\tmethod & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test & Welch Two Sample t-test\\\\\n",
       "\tdata.name & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]} & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]} & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]} & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]} & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]} & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]} & Result\\_SRS{[}{[}3{]}{]}{[}, x{]} and Result\\_STR{[}{[}3{]}{]}{[}, x{]}\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Precision | Recall | F1 | Accuracy | Type_I_Error | Type_II_Error | Tau | \n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| statistic | 2.386749   | 0.410984   | 0.5274041  | 0.5257885  | -2.218921  | -0.410984  | -0.7618766 | \n",
       "| parameter | 17.99998 | 16.60962 | 16.62666 | 16.81568 | 17.14343 | 16.60962 | 17.93871 | \n",
       "| p.value | 0.02818236 | 0.686338   | 0.60488    | 0.6059003  | 0.0402691  | 0.686338   | 0.4560344  | \n",
       "| conf.int | 0.0001853436, 0.0029100128   | -0.02152665, 0.03191903      | -0.01061803, 0.01767970      | -0.005093135, 0.008470520    | -9.425972e-04, -2.405827e-05 | -0.03191903, 0.02152665      | -12.402184, 5.802184         | \n",
       "| estimate | 0.9720394, 0.9704917     | 0.9534418, 0.9482456     | 0.9625231, 0.9589923     | 0.9810334, 0.9793447     | 0.009451877, 0.009935205 | 0.04655820, 0.05175439   | -41.2, -37.9             | \n",
       "| null.value | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
       "| alternative | two.sided | two.sided | two.sided | two.sided | two.sided | two.sided | two.sided | \n",
       "| method | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | Welch Two Sample t-test | \n",
       "| data.name | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | Result_SRS[[3]][, x] and Result_STR[[3]][, x] | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            Precision                                    \n",
       "statistic   2.386749                                     \n",
       "parameter   17.99998                                     \n",
       "p.value     0.02818236                                   \n",
       "conf.int    0.0001853436, 0.0029100128                   \n",
       "estimate    0.9720394, 0.9704917                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]\n",
       "            Recall                                       \n",
       "statistic   0.410984                                     \n",
       "parameter   16.60962                                     \n",
       "p.value     0.686338                                     \n",
       "conf.int    -0.02152665, 0.03191903                      \n",
       "estimate    0.9534418, 0.9482456                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]\n",
       "            F1                                           \n",
       "statistic   0.5274041                                    \n",
       "parameter   16.62666                                     \n",
       "p.value     0.60488                                      \n",
       "conf.int    -0.01061803, 0.01767970                      \n",
       "estimate    0.9625231, 0.9589923                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]\n",
       "            Accuracy                                     \n",
       "statistic   0.5257885                                    \n",
       "parameter   16.81568                                     \n",
       "p.value     0.6059003                                    \n",
       "conf.int    -0.005093135, 0.008470520                    \n",
       "estimate    0.9810334, 0.9793447                         \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]\n",
       "            Type_I_Error                                 \n",
       "statistic   -2.218921                                    \n",
       "parameter   17.14343                                     \n",
       "p.value     0.0402691                                    \n",
       "conf.int    -9.425972e-04, -2.405827e-05                 \n",
       "estimate    0.009451877, 0.009935205                     \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]\n",
       "            Type_II_Error                                \n",
       "statistic   -0.410984                                    \n",
       "parameter   16.60962                                     \n",
       "p.value     0.686338                                     \n",
       "conf.int    -0.03191903, 0.02152665                      \n",
       "estimate    0.04655820, 0.05175439                       \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]\n",
       "            Tau                                          \n",
       "statistic   -0.7618766                                   \n",
       "parameter   17.93871                                     \n",
       "p.value     0.4560344                                    \n",
       "conf.int    -12.402184, 5.802184                         \n",
       "estimate    -41.2, -37.9                                 \n",
       "null.value  0                                            \n",
       "alternative two.sided                                    \n",
       "method      Welch Two Sample t-test                      \n",
       "data.name   Result_SRS[[3]][, x] and Result_STR[[3]][, x]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttest_metrics = sapply(1:7, function(x) t.test(Result_SRS[[3]][,x], Result_STR[[3]][,x], alpha=0.05))\n",
    "colnames(ttest_metrics) = c('Precision', 'Recall', 'F1', 'Accuracy','Type_I_Error','Type_II_Error','Tau')\n",
    "ttest_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Performance: Naive Bayes Model  <a name=\"eval_naive\"></a>\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average performance output w/ SRS Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Mean</th><th scope=col>StDev</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Precision</th><td>  0.972</td><td>0.001  </td></tr>\n",
       "\t<tr><th scope=row>Recall</th><td>  0.953</td><td>0.024  </td></tr>\n",
       "\t<tr><th scope=row>F1</th><td>  0.963</td><td>0.013  </td></tr>\n",
       "\t<tr><th scope=row>Accuracy</th><td>  0.981</td><td>0.006  </td></tr>\n",
       "\t<tr><th scope=row>Type_I_Error</th><td>  0.009</td><td>0.000  </td></tr>\n",
       "\t<tr><th scope=row>Type_II_Error</th><td>  0.047</td><td>0.024  </td></tr>\n",
       "\t<tr><th scope=row>Tau</th><td>-41.200</td><td>9.964  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Mean & StDev\\\\\n",
       "\\hline\n",
       "\tPrecision &   0.972 & 0.001  \\\\\n",
       "\tRecall &   0.953 & 0.024  \\\\\n",
       "\tF1 &   0.963 & 0.013  \\\\\n",
       "\tAccuracy &   0.981 & 0.006  \\\\\n",
       "\tType\\_I\\_Error &   0.009 & 0.000  \\\\\n",
       "\tType\\_II\\_Error &   0.047 & 0.024  \\\\\n",
       "\tTau & -41.200 & 9.964  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Mean | StDev | \n",
       "|---|---|---|---|---|---|---|\n",
       "| Precision |   0.972 | 0.001   | \n",
       "| Recall |   0.953 | 0.024   | \n",
       "| F1 |   0.963 | 0.013   | \n",
       "| Accuracy |   0.981 | 0.006   | \n",
       "| Type_I_Error |   0.009 | 0.000   | \n",
       "| Type_II_Error |   0.047 | 0.024   | \n",
       "| Tau | -41.200 | 9.964   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "              Mean    StDev\n",
       "Precision       0.972 0.001\n",
       "Recall          0.953 0.024\n",
       "F1              0.963 0.013\n",
       "Accuracy        0.981 0.006\n",
       "Type_I_Error    0.009 0.000\n",
       "Type_II_Error   0.047 0.024\n",
       "Tau           -41.200 9.964"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_metrics = cbind(apply(Result_SRS[[3]], 2, mean), apply(Result_SRS[[3]], 2, sd))\n",
    "colnames(avg_metrics) = c('Mean','StDev')\n",
    "round(avg_metrics,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average performance output w/ Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Mean</th><th scope=col>StDev</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Precision</th><td>  0.970</td><td>0.001  </td></tr>\n",
       "\t<tr><th scope=row>Recall</th><td>  0.948</td><td>0.032  </td></tr>\n",
       "\t<tr><th scope=row>F1</th><td>  0.959</td><td>0.017  </td></tr>\n",
       "\t<tr><th scope=row>Accuracy</th><td>  0.979</td><td>0.008  </td></tr>\n",
       "\t<tr><th scope=row>Type_I_Error</th><td>  0.010</td><td>0.001  </td></tr>\n",
       "\t<tr><th scope=row>Type_II_Error</th><td>  0.052</td><td>0.032  </td></tr>\n",
       "\t<tr><th scope=row>Tau</th><td>-37.900</td><td>9.398  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Mean & StDev\\\\\n",
       "\\hline\n",
       "\tPrecision &   0.970 & 0.001  \\\\\n",
       "\tRecall &   0.948 & 0.032  \\\\\n",
       "\tF1 &   0.959 & 0.017  \\\\\n",
       "\tAccuracy &   0.979 & 0.008  \\\\\n",
       "\tType\\_I\\_Error &   0.010 & 0.001  \\\\\n",
       "\tType\\_II\\_Error &   0.052 & 0.032  \\\\\n",
       "\tTau & -37.900 & 9.398  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Mean | StDev | \n",
       "|---|---|---|---|---|---|---|\n",
       "| Precision |   0.970 | 0.001   | \n",
       "| Recall |   0.948 | 0.032   | \n",
       "| F1 |   0.959 | 0.017   | \n",
       "| Accuracy |   0.979 | 0.008   | \n",
       "| Type_I_Error |   0.010 | 0.001   | \n",
       "| Type_II_Error |   0.052 | 0.032   | \n",
       "| Tau | -37.900 | 9.398   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "              Mean    StDev\n",
       "Precision       0.970 0.001\n",
       "Recall          0.948 0.032\n",
       "F1              0.959 0.017\n",
       "Accuracy        0.979 0.008\n",
       "Type_I_Error    0.010 0.001\n",
       "Type_II_Error   0.052 0.032\n",
       "Tau           -37.900 9.398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_metrics = cbind(apply(Result_STR[[3]], 2, mean), apply(Result_STR[[3]], 2, sd))\n",
    "colnames(avg_metrics) = c('Mean','StDev')\n",
    "round(avg_metrics,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART model example via rpart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processAttach = function(body, contentType){\n",
    "\n",
    "  n = length(body)\n",
    "  boundary = getBoundary(contentType)\n",
    " \n",
    "  bString = paste(\"--\", boundary, sep = \"\")\n",
    "  bStringLocs = which(bString == body)\n",
    "  eString = paste(\"--\", boundary, \"--\", sep = \"\")\n",
    "  eStringLoc = which(eString == body)\n",
    "  \n",
    "  if (length(eStringLoc) == 0) eStringLoc = n\n",
    "  if (length(bStringLocs) <= 1) {\n",
    "    attachLocs = NULL\n",
    "    msgLastLine = n\n",
    "    if (length(bStringLocs) == 0) bStringLocs = 0\n",
    "  } else {\n",
    "    attachLocs = c(bStringLocs[ -1 ],  eStringLoc)\n",
    "    msgLastLine = bStringLocs[2] - 1\n",
    "  }\n",
    "  \n",
    "  msg = body[ (bStringLocs[1] + 1) : msgLastLine] \n",
    "  if ( eStringLoc < n )\n",
    "    msg = c(msg, body[ (eStringLoc + 1) : n ])\n",
    "  \n",
    "  if ( !is.null(attachLocs) ) {\n",
    "    attachLens = diff(attachLocs, lag = 1) \n",
    "    attachTypes = mapply(function(begL, endL) {\n",
    "      CTloc = grep(\"^[Cc]ontent-[Tt]ype\", body[ (begL + 1) : (endL - 1)])\n",
    "      if ( length(CTloc) == 0 ) {\n",
    "        MIMEType = NA\n",
    "      } else {\n",
    "        CTval = body[ begL + CTloc[1] ]\n",
    "        CTval = gsub('\"', \"\", CTval )\n",
    "        MIMEType = sub(\" *[Cc]ontent-[Tt]ype: *([^;]*);?.*\", \"\\\\1\", CTval)   \n",
    "      }\n",
    "      return(MIMEType)\n",
    "    }, attachLocs[-length(attachLocs)], attachLocs[-1])\n",
    "  }\n",
    "  \n",
    "  if (is.null(attachLocs)) return(list(body = msg, attachDF = NULL) )\n",
    "  return(list(body = msg, \n",
    "             attachDF = data.frame(aLen = attachLens, \n",
    "                                     aType = unlist(attachTypes),\n",
    "                                     stringsAsFactors = FALSE)))                                \n",
    "}                       \n",
    "\n",
    "readEmail = function(dirName) {\n",
    "       # retrieve the names of files in directory\n",
    "  fileNames = list.files(dirName, full.names = TRUE)\n",
    "       # drop files that are not email\n",
    "  notEmail = grep(\"cmds$\", fileNames)\n",
    "  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]\n",
    "\n",
    "       # read all files in the directory\n",
    "  lapply(fileNames, readLines, encoding = \"latin1\")\n",
    "}\n",
    "\n",
    "\n",
    "processAllEmail = function(dirName, isSpam = FALSE)\n",
    "{\n",
    "       # read all files in the directory\n",
    "  messages = readEmail(dirName)\n",
    "  fileNames = names(messages)\n",
    "  n = length(messages)\n",
    "  \n",
    "       # split header from body\n",
    "  eSplit = lapply(messages, splitMessage)\n",
    "  rm(messages)\n",
    "\n",
    "       # process header as named character vector\n",
    "  headerList = lapply(eSplit, function(msg) \n",
    "                                 processHeader(msg$header))\n",
    "  \n",
    "       # extract content-type key\n",
    "  contentTypes = sapply(headerList, function(header) \n",
    "                                       header[\"Content-Type\"])\n",
    "  \n",
    "       # extract the body\n",
    "  bodyList = lapply(eSplit, function(msg) msg$body)\n",
    "  rm(eSplit)\n",
    "\n",
    "       # which email have attachments\n",
    "  hasAttach = grep(\"^ *multi\", tolower(contentTypes))\n",
    "\n",
    "       # get summary stats for attachments and the shorter body\n",
    "  attList = mapply(processAttach, bodyList[hasAttach], \n",
    "                   contentTypes[hasAttach], SIMPLIFY = FALSE)\n",
    "  \n",
    "  bodyList[hasAttach] = lapply(attList, function(attEl) \n",
    "                                           attEl$body)\n",
    " \n",
    "  attachInfo = vector(\"list\", length = n )\n",
    "  attachInfo[ hasAttach ] = lapply(attList, \n",
    "                                  function(attEl) attEl$attachDF)\n",
    " \n",
    "       # prepare return structure\n",
    "  emailList = mapply(function(header, body, attach, isSpam) {\n",
    "                       list(isSpam = isSpam, header = header, \n",
    "                            body = body, attach = attach)\n",
    "                     },\n",
    "                     headerList, bodyList, attachInfo, \n",
    "                     rep(isSpam, n), SIMPLIFY = FALSE )\n",
    "  names(emailList) = fileNames\n",
    "  \n",
    "  invisible(emailList)\n",
    "}\n",
    "                                   \n",
    "processHeader = function(header)\n",
    "{\n",
    "       # modify the first line to create a key:value pair\n",
    "  header[1] = sub(\"^From\", \"Top-From:\", header[1])\n",
    "  \n",
    "  headerMat = read.dcf(textConnection(header), all = TRUE)\n",
    "  headerVec = unlist(headerMat)\n",
    "  \n",
    "  dupKeys = sapply(headerMat, function(x) length(unlist(x)))\n",
    "  names(headerVec) = rep(colnames(headerMat), dupKeys)\n",
    "  \n",
    "  return(headerVec)\n",
    "}\n",
    "                   \n",
    "SpamCheckWords =\n",
    "  c(\"viagra\", \"pounds\", \"free\", \"weight\", \"guarantee\", \"million\", \n",
    "    \"dollars\", \"credit\", \"risk\", \"prescription\", \"generic\", \"drug\",\n",
    "    \"financial\", \"save\", \"dollar\", \"erotic\", \"million\", \"barrister\",\n",
    "    \"beneficiary\", \"easy\", \n",
    "    \"money back\", \"money\", \"credit card\")\n",
    "\n",
    "\n",
    "getMessageRecipients =\n",
    "  function(header)\n",
    "  {\n",
    "    c(if(\"To\" %in% names(header))  header[[\"To\"]] else character(0),\n",
    "      if(\"Cc\" %in% names(header))  header[[\"Cc\"]] else character(0),\n",
    "      if(\"Bcc\" %in% names(header)) header[[\"Bcc\"]] else character(0)\n",
    "    )\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcList = list(\n",
    "  isSpam =\n",
    "    expression(msg$isSpam)\n",
    "  ,\n",
    "  isRe =\n",
    "    function(msg) {\n",
    "      # Can have a Fwd: Re:  ... but we are not looking for this here.\n",
    "      # We may want to look at In-Reply-To field.\n",
    "      \"Subject\" %in% names(msg$header) && \n",
    "        length(grep(\"^[ \\t]*Re:\", msg$header[[\"Subject\"]])) > 0\n",
    "    }\n",
    "  ,\n",
    "  numLines =\n",
    "    function(msg) length(msg$body)\n",
    "  ,\n",
    "  bodyCharCt =\n",
    "    function(msg)\n",
    "      sum(nchar(msg$body))\n",
    "  ,\n",
    "  underscore =\n",
    "    function(msg) {\n",
    "      if(!\"Reply-To\" %in% names(msg$header))\n",
    "        return(FALSE)\n",
    "      \n",
    "      txt <- msg$header[[\"Reply-To\"]]\n",
    "      length(grep(\"_\", txt)) > 0  && \n",
    "        length(grep(\"[0-9A-Za-z]+\", txt)) > 0\n",
    "    }\n",
    "  ,\n",
    "  subExcCt = \n",
    "    function(msg) {\n",
    "      x = msg$header[\"Subject\"]\n",
    "      if(length(x) == 0 || sum(nchar(x)) == 0 || is.na(x))\n",
    "        return(NA)\n",
    "      \n",
    "      sum(nchar(gsub(\"[^!]\",\"\", x)))\n",
    "    }\n",
    "  ,\n",
    "  subQuesCt =\n",
    "    function(msg) {\n",
    "      x = msg$header[\"Subject\"]\n",
    "      if(length(x) == 0 || sum(nchar(x)) == 0 || is.na(x))\n",
    "        return(NA)\n",
    "      \n",
    "      sum(nchar(gsub(\"[^?]\",\"\", x)))\n",
    "    }\n",
    "  ,\n",
    "  numAtt = \n",
    "    function(msg) {\n",
    "      if (is.null(msg$attach)) return(0)\n",
    "      else nrow(msg$attach)\n",
    "    }\n",
    "   \n",
    "  ,\n",
    "  priority =\n",
    "    function(msg) {\n",
    "      ans <- FALSE\n",
    "      # Look for names X-Priority, Priority, X-Msmail-Priority\n",
    "      # Look for high any where in the value\n",
    "      ind = grep(\"priority\", tolower(names(msg$header)))\n",
    "      if (length(ind) > 0)  {\n",
    "        ans <- length(grep(\"high\", tolower(msg$header[ind]))) >0\n",
    "      }\n",
    "      ans\n",
    "    }\n",
    "  ,\n",
    "  numRec =\n",
    "    function(msg) {\n",
    "      # unique or not.\n",
    "      els = getMessageRecipients(msg$header)\n",
    "      \n",
    "      if(length(els) == 0)\n",
    "        return(NA)\n",
    "      \n",
    "      # Split each line by \",\"  and in each of these elements, look for\n",
    "      # the @ sign. This handles\n",
    "      tmp = sapply(strsplit(els, \",\"), function(x) grep(\"@\", x))\n",
    "      sum(sapply(tmp, length))\n",
    "    }\n",
    "  ,\n",
    "  perCaps =\n",
    "    function(msg)\n",
    "    {\n",
    "      body = paste(msg$body, collapse = \"\")\n",
    "      \n",
    "      # Return NA if the body of the message is \"empty\"\n",
    "      if(length(body) == 0 || nchar(body) == 0) return(NA)\n",
    "      \n",
    "      # Eliminate non-alpha characters and empty lines \n",
    "      body = gsub(\"[^[:alpha:]]\", \"\", body)\n",
    "      els = unlist(strsplit(body, \"\"))\n",
    "      ctCap = sum(els %in% LETTERS)\n",
    "      100 * ctCap / length(els)\n",
    "    }\n",
    "  ,\n",
    "  isInReplyTo =\n",
    "    function(msg)\n",
    "    {\n",
    "      \"In-Reply-To\" %in% names(msg$header)\n",
    "    }\n",
    "  ,\n",
    "  sortedRec =\n",
    "    function(msg)\n",
    "    {\n",
    "      ids = getMessageRecipients(msg$header)\n",
    "      all(sort(ids) == ids)\n",
    "    }\n",
    "  ,\n",
    "  subPunc =\n",
    "    function(msg)\n",
    "    {\n",
    "      if(\"Subject\" %in% names(msg$header)) {\n",
    "        el = gsub(\"['/.:@-]\", \"\", msg$header[\"Subject\"])\n",
    "        length(grep(\"[A-Za-z][[:punct:]]+[A-Za-z]\", el)) > 0\n",
    "      }\n",
    "      else\n",
    "        FALSE\n",
    "    },\n",
    "  hour =\n",
    "    function(msg)\n",
    "    {\n",
    "      date = msg$header[\"Date\"]\n",
    "      if ( is.null(date) ) return(NA)\n",
    "      # Need to handle that there may be only one digit in the hour\n",
    "      locate = regexpr(\"[0-2]?[0-9]:[0-5][0-9]:[0-5][0-9]\", date)\n",
    "      \n",
    "      if (locate < 0)\n",
    "        locate = regexpr(\"[0-2]?[0-9]:[0-5][0-9]\", date)\n",
    "      if (locate < 0) return(NA)\n",
    "      \n",
    "      hour = substring(date, locate, locate+1)\n",
    "      hour = as.numeric(gsub(\":\", \"\", hour))\n",
    "      \n",
    "      locate = regexpr(\"PM\", date)\n",
    "      if (locate > 0) hour = hour + 12\n",
    "      \n",
    "      locate = regexpr(\"[+-][0-2][0-9]00\", date)\n",
    "      if (locate < 0) offset = 0\n",
    "      else offset = as.numeric(substring(date, locate, locate + 2))\n",
    "      (hour - offset) %% 24\n",
    "    }\n",
    "  ,\n",
    "  multipartText =\n",
    "    function(msg)\n",
    "    {\n",
    "      if (is.null(msg$attach)) return(FALSE)\n",
    "      numAtt = nrow(msg$attach)\n",
    "      \n",
    "      types = \n",
    "        length(grep(\"(html|plain|text)\", msg$attach$aType)) > (numAtt/2)\n",
    "    }\n",
    "  ,\n",
    "  hasImages =\n",
    "    function(msg)\n",
    "    {\n",
    "      if (is.null(msg$attach)) return(FALSE)\n",
    "      \n",
    "      length(grep(\"^ *image\", tolower(msg$attach$aType))) > 0\n",
    "    }\n",
    "  ,\n",
    "  isPGPsigned =\n",
    "    function(msg)\n",
    "    {\n",
    "      if (is.null(msg$attach)) return(FALSE)\n",
    "      \n",
    "      length(grep(\"pgp\", tolower(msg$attach$aType))) > 0\n",
    "    },\n",
    "  perHTML =\n",
    "    function(msg)\n",
    "    {\n",
    "      if(! (\"Content-Type\" %in% names(msg$header))) return(0)\n",
    "      \n",
    "      el = tolower(msg$header[\"Content-Type\"]) \n",
    "      if (length(grep(\"html\", el)) == 0) return(0)\n",
    "      \n",
    "      els = gsub(\"[[:space:]]\", \"\", msg$body)\n",
    "      totchar = sum(nchar(els))\n",
    "      totplain = sum(nchar(gsub(\"<[^<]+>\", \"\", els )))\n",
    "      100 * (totchar - totplain)/totchar\n",
    "    },\n",
    "  subSpamWords =\n",
    "    function(msg)\n",
    "    {\n",
    "      if(\"Subject\" %in% names(msg$header))\n",
    "        length(grep(paste(SpamCheckWords, collapse = \"|\"), \n",
    "                    tolower(msg$header[\"Subject\"]))) > 0\n",
    "      else\n",
    "        NA\n",
    "    }\n",
    "  ,\n",
    "  subBlanks =\n",
    "    function(msg)\n",
    "    {\n",
    "      if(\"Subject\" %in% names(msg$header)) {\n",
    "        x = msg$header[\"Subject\"]\n",
    "        # should we count blank subject line as 0 or 1 or NA?\n",
    "        if (nchar(x) == 1) return(0)\n",
    "        else 100 *(1 - (nchar(gsub(\"[[:blank:]]\", \"\", x))/nchar(x)))\n",
    "      } else NA\n",
    "    }\n",
    "  ,\n",
    "  noHost =\n",
    "    function(msg)\n",
    "    {\n",
    "      # Or use partial matching.\n",
    "      idx = pmatch(\"Message-\", names(msg$header))\n",
    "      \n",
    "      if(is.na(idx)) return(NA)\n",
    "      \n",
    "      tmp = msg$header[idx]\n",
    "      return(length(grep(\".*@[^[:space:]]+\", tmp)) ==  0)\n",
    "    }\n",
    "  ,\n",
    "  numEnd =\n",
    "    function(msg)\n",
    "    {\n",
    "      # If we just do a grep(\"[0-9]@\",  )\n",
    "      # we get matches on messages that have a From something like\n",
    "      # \" \\\"marty66@aol.com\\\" <synjan@ecis.com>\"\n",
    "      # and the marty66 is the \"user's name\" not the login\n",
    "      # So we can be more precise if we want.\n",
    "      x = names(msg$header)\n",
    "      if ( !( \"From\" %in% x) ) return(NA)\n",
    "      login = gsub(\"^.*<\", \"\", msg$header[\"From\"])\n",
    "      if ( is.null(login) ) \n",
    "        login = gsub(\"^.*<\", \"\", msg$header[\"X-From\"])\n",
    "      if ( is.null(login) ) return(NA)\n",
    "      login = strsplit(login, \"@\")[[1]][1]\n",
    "      length(grep(\"[0-9]+$\", login)) > 0\n",
    "    },\n",
    "  isYelling =\n",
    "    function(msg)\n",
    "    {\n",
    "      if ( \"Subject\" %in% names(msg$header) ) {\n",
    "        el = gsub(\"[^[:alpha:]]\", \"\", msg$header[\"Subject\"])\n",
    "        if (nchar(el) > 0) nchar(gsub(\"[A-Z]\", \"\", el)) < 1\n",
    "        else FALSE\n",
    "      }\n",
    "      else\n",
    "        NA\n",
    "    },\n",
    "  forwards =\n",
    "    function(msg)\n",
    "    {\n",
    "      x = msg$body\n",
    "      if(length(x) == 0 || sum(nchar(x)) == 0)\n",
    "        return(NA)\n",
    "      \n",
    "      ans = length(grep(\"^[[:space:]]*>\", x))\n",
    "      100 * ans / length(x)\n",
    "    },\n",
    "  isOrigMsg =\n",
    "    function(msg)\n",
    "    {\n",
    "      x = msg$body\n",
    "      if(length(x) == 0) return(NA)\n",
    "      \n",
    "      length(grep(\"^[^[:alpha:]]*original[^[:alpha:]]+message[^[:alpha:]]*$\", \n",
    "                  tolower(x) ) ) > 0\n",
    "    },\n",
    "  isDear =\n",
    "    function(msg)\n",
    "    {\n",
    "      x = msg$body\n",
    "      if(length(x) == 0) return(NA)\n",
    "      \n",
    "      length(grep(\"^[[:blank:]]*dear +(sir|madam)\\\\>\", \n",
    "                  tolower(x))) > 0\n",
    "    },\n",
    "  isWrote =\n",
    "    function(msg)\n",
    "    {\n",
    "      x = msg$body\n",
    "      if(length(x) == 0) return(NA)\n",
    "      \n",
    "      length(grep(\"(wrote|schrieb|ecrit|escribe):\", tolower(x) )) > 0\n",
    "    },\n",
    "  avgWordLen =\n",
    "    function(msg)\n",
    "    {\n",
    "      txt = paste(msg$body, collapse = \" \")\n",
    "      if(length(txt) == 0 || sum(nchar(txt)) == 0) return(0)\n",
    "      \n",
    "      txt = gsub(\"[^[:alpha:]]\", \" \", txt)\n",
    "      words = unlist(strsplit(txt, \"[[:blank:]]+\"))\n",
    "      wordLens = nchar(words)\n",
    "      mean(wordLens[ wordLens > 0 ])\n",
    "    }\n",
    "  ,\n",
    "  numDlr =\n",
    "    function(msg)\n",
    "    {\n",
    "      x = paste(msg$body, collapse = \"\")\n",
    "      if(length(x) == 0 || sum(nchar(x)) == 0)\n",
    "        return(NA)\n",
    "      \n",
    "      nchar(gsub(\"[^$]\",\"\", x))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDerivedDF =\n",
    "function(email = emailStruct, operations = funcList, \n",
    "         verbose = FALSE)\n",
    "{\n",
    "  els = lapply(names(operations),\n",
    "               function(id) {\n",
    "                 if(verbose) print(id)\n",
    "                 e = operations[[id]]\n",
    "                 v = if(is.function(e)) \n",
    "                        sapply(email, e)\n",
    "                      else \n",
    "                        sapply(email, function(msg) eval(e))\n",
    "                 v\n",
    "         })\n",
    "\n",
    "   df = as.data.frame(els)\n",
    "   names(df) = names(operations)\n",
    "   invisible(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/hard_ham/00228.0eaef7857bbbf3ebf5edbbdae2b30493'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/hard_ham/0231.7c6cc716ce3f3bfad7130dd3c8d7b072'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/hard_ham/0250.7c6cc716ce3f3bfad7130dd3c8d7b072'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/spam/00136.faa39d8e816c70f23b4bb8758d8a74f0'\"Warning message in FUN(X[[i]], ...):\n",
      "\"incomplete final line found on './/messages/spam/0143.260a940290dcb61f9327b224a368d4af'\""
     ]
    }
   ],
   "source": [
    "emailStruct = mapply(processAllEmail, fullDirNames,\n",
    "                     isSpam = rep( c(FALSE, TRUE), 3:2))      \n",
    "emailStruct = unlist(emailStruct, recursive = FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>9348</li>\n",
       "\t<li>30</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 9348\n",
       "\\item 30\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 9348\n",
       "2. 30\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 9348   30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emailDF = createDerivedDF(emailStruct)\n",
    "dim(emailDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>isSpam</th><th scope=col>isRe</th><th scope=col>numLines</th><th scope=col>bodyCharCt</th><th scope=col>underscore</th><th scope=col>subExcCt</th><th scope=col>subQuesCt</th><th scope=col>numAtt</th><th scope=col>priority</th><th scope=col>numRec</th><th scope=col>...</th><th scope=col>subBlanks</th><th scope=col>noHost</th><th scope=col>numEnd</th><th scope=col>isYelling</th><th scope=col>forwards</th><th scope=col>isOrigMsg</th><th scope=col>isDear</th><th scope=col>isWrote</th><th scope=col>avgWordLen</th><th scope=col>numDlr</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>.//messages/easy_ham1</th><td>FALSE    </td><td> TRUE    </td><td>50       </td><td>1554     </td><td>FALSE    </td><td>0        </td><td>0        </td><td>0        </td><td>FALSE    </td><td>2        </td><td>...      </td><td>12.50000 </td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td> 0.000000</td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>4.376623 </td><td>3        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham2</th><td>FALSE    </td><td>FALSE    </td><td>26       </td><td> 873     </td><td>FALSE    </td><td>0        </td><td>0        </td><td>0        </td><td>FALSE    </td><td>1        </td><td>...      </td><td> 8.00000 </td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td> 0.000000</td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>4.555556 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham3</th><td>FALSE    </td><td>FALSE    </td><td>38       </td><td>1713     </td><td>FALSE    </td><td>0        </td><td>0        </td><td>0        </td><td>FALSE    </td><td>1        </td><td>...      </td><td> 8.00000 </td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td> 0.000000</td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>4.817164 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham4</th><td>FALSE    </td><td>FALSE    </td><td>32       </td><td>1095     </td><td>FALSE    </td><td>0        </td><td>0        </td><td>0        </td><td>FALSE    </td><td>0        </td><td>...      </td><td>18.91892 </td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td> 3.125000</td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>4.714286 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham5</th><td>FALSE    </td><td> TRUE    </td><td>31       </td><td>1021     </td><td>FALSE    </td><td>0        </td><td>0        </td><td>0        </td><td>FALSE    </td><td>1        </td><td>...      </td><td>15.21739 </td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td> 6.451613</td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>4.234940 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham6</th><td>FALSE    </td><td> TRUE    </td><td>25       </td><td> 718     </td><td>FALSE    </td><td>0        </td><td>0        </td><td>0        </td><td>FALSE    </td><td>1        </td><td>...      </td><td>15.21739 </td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>12.000000</td><td>FALSE    </td><td>FALSE    </td><td>FALSE    </td><td>3.956897 </td><td>0        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllll}\n",
       "  & isSpam & isRe & numLines & bodyCharCt & underscore & subExcCt & subQuesCt & numAtt & priority & numRec & ... & subBlanks & noHost & numEnd & isYelling & forwards & isOrigMsg & isDear & isWrote & avgWordLen & numDlr\\\\\n",
       "\\hline\n",
       "\t.//messages/easy\\_ham1 & FALSE     &  TRUE     & 50        & 1554      & FALSE     & 0         & 0         & 0         & FALSE     & 2         & ...       & 12.50000  & FALSE     & FALSE     & FALSE     &  0.000000 & FALSE     & FALSE     & FALSE     & 4.376623  & 3        \\\\\n",
       "\t.//messages/easy\\_ham2 & FALSE     & FALSE     & 26        &  873      & FALSE     & 0         & 0         & 0         & FALSE     & 1         & ...       &  8.00000  & FALSE     & FALSE     & FALSE     &  0.000000 & FALSE     & FALSE     & FALSE     & 4.555556  & 0        \\\\\n",
       "\t.//messages/easy\\_ham3 & FALSE     & FALSE     & 38        & 1713      & FALSE     & 0         & 0         & 0         & FALSE     & 1         & ...       &  8.00000  & FALSE     & FALSE     & FALSE     &  0.000000 & FALSE     & FALSE     & FALSE     & 4.817164  & 0        \\\\\n",
       "\t.//messages/easy\\_ham4 & FALSE     & FALSE     & 32        & 1095      & FALSE     & 0         & 0         & 0         & FALSE     & 0         & ...       & 18.91892  & FALSE     & FALSE     & FALSE     &  3.125000 & FALSE     & FALSE     & FALSE     & 4.714286  & 0        \\\\\n",
       "\t.//messages/easy\\_ham5 & FALSE     &  TRUE     & 31        & 1021      & FALSE     & 0         & 0         & 0         & FALSE     & 1         & ...       & 15.21739  & FALSE     & FALSE     & FALSE     &  6.451613 & FALSE     & FALSE     & FALSE     & 4.234940  & 0        \\\\\n",
       "\t.//messages/easy\\_ham6 & FALSE     &  TRUE     & 25        &  718      & FALSE     & 0         & 0         & 0         & FALSE     & 1         & ...       & 15.21739  & FALSE     & FALSE     & FALSE     & 12.000000 & FALSE     & FALSE     & FALSE     & 3.956897  & 0        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | isSpam | isRe | numLines | bodyCharCt | underscore | subExcCt | subQuesCt | numAtt | priority | numRec | ... | subBlanks | noHost | numEnd | isYelling | forwards | isOrigMsg | isDear | isWrote | avgWordLen | numDlr | \n",
       "|---|---|---|---|---|---|\n",
       "| .//messages/easy_ham1 | FALSE     |  TRUE     | 50        | 1554      | FALSE     | 0         | 0         | 0         | FALSE     | 2         | ...       | 12.50000  | FALSE     | FALSE     | FALSE     |  0.000000 | FALSE     | FALSE     | FALSE     | 4.376623  | 3         | \n",
       "| .//messages/easy_ham2 | FALSE     | FALSE     | 26        |  873      | FALSE     | 0         | 0         | 0         | FALSE     | 1         | ...       |  8.00000  | FALSE     | FALSE     | FALSE     |  0.000000 | FALSE     | FALSE     | FALSE     | 4.555556  | 0         | \n",
       "| .//messages/easy_ham3 | FALSE     | FALSE     | 38        | 1713      | FALSE     | 0         | 0         | 0         | FALSE     | 1         | ...       |  8.00000  | FALSE     | FALSE     | FALSE     |  0.000000 | FALSE     | FALSE     | FALSE     | 4.817164  | 0         | \n",
       "| .//messages/easy_ham4 | FALSE     | FALSE     | 32        | 1095      | FALSE     | 0         | 0         | 0         | FALSE     | 0         | ...       | 18.91892  | FALSE     | FALSE     | FALSE     |  3.125000 | FALSE     | FALSE     | FALSE     | 4.714286  | 0         | \n",
       "| .//messages/easy_ham5 | FALSE     |  TRUE     | 31        | 1021      | FALSE     | 0         | 0         | 0         | FALSE     | 1         | ...       | 15.21739  | FALSE     | FALSE     | FALSE     |  6.451613 | FALSE     | FALSE     | FALSE     | 4.234940  | 0         | \n",
       "| .//messages/easy_ham6 | FALSE     |  TRUE     | 25        |  718      | FALSE     | 0         | 0         | 0         | FALSE     | 1         | ...       | 15.21739  | FALSE     | FALSE     | FALSE     | 12.000000 | FALSE     | FALSE     | FALSE     | 3.956897  | 0         | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                      isSpam isRe  numLines bodyCharCt underscore subExcCt\n",
       ".//messages/easy_ham1 FALSE   TRUE 50       1554       FALSE      0       \n",
       ".//messages/easy_ham2 FALSE  FALSE 26        873       FALSE      0       \n",
       ".//messages/easy_ham3 FALSE  FALSE 38       1713       FALSE      0       \n",
       ".//messages/easy_ham4 FALSE  FALSE 32       1095       FALSE      0       \n",
       ".//messages/easy_ham5 FALSE   TRUE 31       1021       FALSE      0       \n",
       ".//messages/easy_ham6 FALSE   TRUE 25        718       FALSE      0       \n",
       "                      subQuesCt numAtt priority numRec ... subBlanks noHost\n",
       ".//messages/easy_ham1 0         0      FALSE    2      ... 12.50000  FALSE \n",
       ".//messages/easy_ham2 0         0      FALSE    1      ...  8.00000  FALSE \n",
       ".//messages/easy_ham3 0         0      FALSE    1      ...  8.00000  FALSE \n",
       ".//messages/easy_ham4 0         0      FALSE    0      ... 18.91892  FALSE \n",
       ".//messages/easy_ham5 0         0      FALSE    1      ... 15.21739  FALSE \n",
       ".//messages/easy_ham6 0         0      FALSE    1      ... 15.21739  FALSE \n",
       "                      numEnd isYelling forwards  isOrigMsg isDear isWrote\n",
       ".//messages/easy_ham1 FALSE  FALSE      0.000000 FALSE     FALSE  FALSE  \n",
       ".//messages/easy_ham2 FALSE  FALSE      0.000000 FALSE     FALSE  FALSE  \n",
       ".//messages/easy_ham3 FALSE  FALSE      0.000000 FALSE     FALSE  FALSE  \n",
       ".//messages/easy_ham4 FALSE  FALSE      3.125000 FALSE     FALSE  FALSE  \n",
       ".//messages/easy_ham5 FALSE  FALSE      6.451613 FALSE     FALSE  FALSE  \n",
       ".//messages/easy_ham6 FALSE  FALSE     12.000000 FALSE     FALSE  FALSE  \n",
       "                      avgWordLen numDlr\n",
       ".//messages/easy_ham1 4.376623   3     \n",
       ".//messages/easy_ham2 4.555556   0     \n",
       ".//messages/easy_ham3 4.817164   0     \n",
       ".//messages/easy_ham4 4.714286   0     \n",
       ".//messages/easy_ham5 4.234940   0     \n",
       ".//messages/easy_ham6 3.956897   0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(emailDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "setupRpart = function(data) {\n",
    "  logicalVars = which(sapply(data, is.logical))\n",
    "  facVars = lapply(data[ , logicalVars], \n",
    "                   function(x) {\n",
    "                      x = as.factor(x)\n",
    "                      levels(x) = c(\"F\", \"T\")\n",
    "                      x\n",
    "                   })\n",
    "  cbind(facVars, data[ , - logicalVars])\n",
    "}\n",
    "\n",
    "emailDFrp = setupRpart(emailDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>isSpam</th><th scope=col>isRe</th><th scope=col>underscore</th><th scope=col>priority</th><th scope=col>isInReplyTo</th><th scope=col>sortedRec</th><th scope=col>subPunc</th><th scope=col>multipartText</th><th scope=col>hasImages</th><th scope=col>isPGPsigned</th><th scope=col>...</th><th scope=col>subQuesCt</th><th scope=col>numAtt</th><th scope=col>numRec</th><th scope=col>perCaps</th><th scope=col>hour</th><th scope=col>perHTML</th><th scope=col>subBlanks</th><th scope=col>forwards</th><th scope=col>avgWordLen</th><th scope=col>numDlr</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>.//messages/easy_ham1</th><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>T        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>...      </td><td>0        </td><td>0        </td><td>2        </td><td>4.451039 </td><td>11       </td><td>0        </td><td>12.50000 </td><td> 0.000000</td><td>4.376623 </td><td>3        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham2</th><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>...      </td><td>0        </td><td>0        </td><td>1        </td><td>7.491289 </td><td>11       </td><td>0        </td><td> 8.00000 </td><td> 0.000000</td><td>4.555556 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham3</th><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>...      </td><td>0        </td><td>0        </td><td>1        </td><td>7.436096 </td><td>12       </td><td>0        </td><td> 8.00000 </td><td> 0.000000</td><td>4.817164 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham4</th><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>5.090909 </td><td>13       </td><td>0        </td><td>18.91892 </td><td> 3.125000</td><td>4.714286 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham5</th><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>...      </td><td>0        </td><td>0        </td><td>1        </td><td>6.116643 </td><td>13       </td><td>0        </td><td>15.21739 </td><td> 6.451613</td><td>4.234940 </td><td>0        </td></tr>\n",
       "\t<tr><th scope=row>.//messages/easy_ham6</th><td>F        </td><td>T        </td><td>F        </td><td>F        </td><td>T        </td><td>T        </td><td>F        </td><td>F        </td><td>F        </td><td>F        </td><td>...      </td><td>0        </td><td>0        </td><td>1        </td><td>7.625272 </td><td>13       </td><td>0        </td><td>15.21739 </td><td>12.000000</td><td>3.956897 </td><td>0        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllll}\n",
       "  & isSpam & isRe & underscore & priority & isInReplyTo & sortedRec & subPunc & multipartText & hasImages & isPGPsigned & ... & subQuesCt & numAtt & numRec & perCaps & hour & perHTML & subBlanks & forwards & avgWordLen & numDlr\\\\\n",
       "\\hline\n",
       "\t.//messages/easy\\_ham1 & F         & T         & F         & F         & T         & T         & F         & F         & F         & F         & ...       & 0         & 0         & 2         & 4.451039  & 11        & 0         & 12.50000  &  0.000000 & 4.376623  & 3        \\\\\n",
       "\t.//messages/easy\\_ham2 & F         & F         & F         & F         & F         & T         & F         & F         & F         & F         & ...       & 0         & 0         & 1         & 7.491289  & 11        & 0         &  8.00000  &  0.000000 & 4.555556  & 0        \\\\\n",
       "\t.//messages/easy\\_ham3 & F         & F         & F         & F         & F         & T         & F         & F         & F         & F         & ...       & 0         & 0         & 1         & 7.436096  & 12        & 0         &  8.00000  &  0.000000 & 4.817164  & 0        \\\\\n",
       "\t.//messages/easy\\_ham4 & F         & F         & F         & F         & F         & T         & F         & F         & F         & F         & ...       & 0         & 0         & 0         & 5.090909  & 13        & 0         & 18.91892  &  3.125000 & 4.714286  & 0        \\\\\n",
       "\t.//messages/easy\\_ham5 & F         & T         & F         & F         & F         & T         & F         & F         & F         & F         & ...       & 0         & 0         & 1         & 6.116643  & 13        & 0         & 15.21739  &  6.451613 & 4.234940  & 0        \\\\\n",
       "\t.//messages/easy\\_ham6 & F         & T         & F         & F         & T         & T         & F         & F         & F         & F         & ...       & 0         & 0         & 1         & 7.625272  & 13        & 0         & 15.21739  & 12.000000 & 3.956897  & 0        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | isSpam | isRe | underscore | priority | isInReplyTo | sortedRec | subPunc | multipartText | hasImages | isPGPsigned | ... | subQuesCt | numAtt | numRec | perCaps | hour | perHTML | subBlanks | forwards | avgWordLen | numDlr | \n",
       "|---|---|---|---|---|---|\n",
       "| .//messages/easy_ham1 | F         | T         | F         | F         | T         | T         | F         | F         | F         | F         | ...       | 0         | 0         | 2         | 4.451039  | 11        | 0         | 12.50000  |  0.000000 | 4.376623  | 3         | \n",
       "| .//messages/easy_ham2 | F         | F         | F         | F         | F         | T         | F         | F         | F         | F         | ...       | 0         | 0         | 1         | 7.491289  | 11        | 0         |  8.00000  |  0.000000 | 4.555556  | 0         | \n",
       "| .//messages/easy_ham3 | F         | F         | F         | F         | F         | T         | F         | F         | F         | F         | ...       | 0         | 0         | 1         | 7.436096  | 12        | 0         |  8.00000  |  0.000000 | 4.817164  | 0         | \n",
       "| .//messages/easy_ham4 | F         | F         | F         | F         | F         | T         | F         | F         | F         | F         | ...       | 0         | 0         | 0         | 5.090909  | 13        | 0         | 18.91892  |  3.125000 | 4.714286  | 0         | \n",
       "| .//messages/easy_ham5 | F         | T         | F         | F         | F         | T         | F         | F         | F         | F         | ...       | 0         | 0         | 1         | 6.116643  | 13        | 0         | 15.21739  |  6.451613 | 4.234940  | 0         | \n",
       "| .//messages/easy_ham6 | F         | T         | F         | F         | T         | T         | F         | F         | F         | F         | ...       | 0         | 0         | 1         | 7.625272  | 13        | 0         | 15.21739  | 12.000000 | 3.956897  | 0         | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                      isSpam isRe underscore priority isInReplyTo sortedRec\n",
       ".//messages/easy_ham1 F      T    F          F        T           T        \n",
       ".//messages/easy_ham2 F      F    F          F        F           T        \n",
       ".//messages/easy_ham3 F      F    F          F        F           T        \n",
       ".//messages/easy_ham4 F      F    F          F        F           T        \n",
       ".//messages/easy_ham5 F      T    F          F        F           T        \n",
       ".//messages/easy_ham6 F      T    F          F        T           T        \n",
       "                      subPunc multipartText hasImages isPGPsigned ... subQuesCt\n",
       ".//messages/easy_ham1 F       F             F         F           ... 0        \n",
       ".//messages/easy_ham2 F       F             F         F           ... 0        \n",
       ".//messages/easy_ham3 F       F             F         F           ... 0        \n",
       ".//messages/easy_ham4 F       F             F         F           ... 0        \n",
       ".//messages/easy_ham5 F       F             F         F           ... 0        \n",
       ".//messages/easy_ham6 F       F             F         F           ... 0        \n",
       "                      numAtt numRec perCaps  hour perHTML subBlanks forwards \n",
       ".//messages/easy_ham1 0      2      4.451039 11   0       12.50000   0.000000\n",
       ".//messages/easy_ham2 0      1      7.491289 11   0        8.00000   0.000000\n",
       ".//messages/easy_ham3 0      1      7.436096 12   0        8.00000   0.000000\n",
       ".//messages/easy_ham4 0      0      5.090909 13   0       18.91892   3.125000\n",
       ".//messages/easy_ham5 0      1      6.116643 13   0       15.21739   6.451613\n",
       ".//messages/easy_ham6 0      1      7.625272 13   0       15.21739  12.000000\n",
       "                      avgWordLen numDlr\n",
       ".//messages/easy_ham1 4.376623   3     \n",
       ".//messages/easy_ham2 4.555556   0     \n",
       ".//messages/easy_ham3 4.817164   0     \n",
       ".//messages/easy_ham4 4.714286   0     \n",
       ".//messages/easy_ham5 4.234940   0     \n",
       ".//messages/easy_ham6 3.956897   0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(emailDFrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART example data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "# train/test data split\n",
    "set.seed(418910)\n",
    "\n",
    "numEmail = length(isSpam)\n",
    "numSpam = sum(isSpam)\n",
    "numHam = numEmail - numSpam\n",
    "\n",
    "testSpamIdx = sample(numSpam, size = floor(numSpam/3))\n",
    "testHamIdx = sample(numHam, size = floor(numHam/3))\n",
    "\n",
    "testDF = \n",
    "  rbind( emailDFrp[ emailDFrp$isSpam == \"T\", ][testSpamIdx, ],\n",
    "         emailDFrp[emailDFrp$isSpam == \"F\", ][testHamIdx, ] )\n",
    "trainDF =\n",
    "  rbind( emailDFrp[emailDFrp$isSpam == \"T\", ][-testSpamIdx, ], \n",
    "         emailDFrp[emailDFrp$isSpam == \"F\", ][-testHamIdx, ])\n",
    "\n",
    "# train\n",
    "rpartFit = rpart(isSpam ~ ., data = trainDF, method = \"class\")\n",
    "\n",
    "# plot trained decision tree\n",
    "prp(rpartFit, extra = 1)\n",
    "\n",
    "# prediction\n",
    "predictions = predict(rpartFit, \n",
    "       newdata = testDF[, names(testDF) != \"isSpam\"],\n",
    "       type = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation for model optimization\n",
    "\n",
    "1. Change logical variables into numeric.\n",
    "2. Replace NaNs to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "setupRnum = function(data) {\n",
    "  logicalVars = which(sapply(data, is.logical))\n",
    "  facVars = lapply(data[ , logicalVars], \n",
    "                   function(x) {\n",
    "                      x = as.numeric(x)\n",
    "                   })\n",
    "  cbind(facVars, data[ , - logicalVars])\n",
    "}\n",
    "\n",
    "emailDFnum = setupRnum(emailDF)\n",
    "\n",
    "emailDFnum[is.na(emailDFnum)]<-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics function for Caret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'MLmetrics'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Recall\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(MLmetrics)\n",
    "f1 <- function(data, lev = NULL, model = NULL) {\n",
    "    f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])\n",
    "    p <- Precision(y_pred = data$pred, y_true = data$obs, positive = lev[1])\n",
    "    r <- Recall(y_pred = data$pred, y_true = data$obs, positive = lev[1])\n",
    "    a <- Accuracy(y_pred = data$pred, y_true = data$obs)\n",
    "    fp <-sum(data$pred==1 & data$obs==0)\n",
    "    fn <-sum(data$pred==0 & data$obs==1)\n",
    "    tn <-sum(data$pred==0 & data$obs==0)\n",
    "    tp <-sum(data$pred==1 & data$obs==1)\n",
    "    t1e<-fp/(fp+tn)\n",
    "    t2e<-fn/(fn+tp)\n",
    "    c(F1 = f1_val,\n",
    "    Precesion = p,\n",
    "    Recall = r,\n",
    "    Accuracy = a,\n",
    "    Type_I_error=t1e,\n",
    "    Type_II_error=t2e\n",
    "   )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART model: 3-fold CV w/ cp (pruning parameter) grid search<a name=\"cart\"></a>\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Attaching package: 'ggplot2'\n",
      "\n",
      "The following object is masked from 'package:NLP':\n",
      "\n",
      "    annotate\n",
      "\n",
      "\n",
      "Attaching package: 'caret'\n",
      "\n",
      "The following objects are masked from 'package:MLmetrics':\n",
      "\n",
      "    MAE, RMSE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "9348 samples\n",
       "  29 predictor\n",
       "   2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (3 fold) \n",
       "Summary of sample sizes: 6232, 6232, 6232 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp      F1         Precesion  Recall     Accuracy   Type_I_error\n",
       "  0.0000  0.9590466  0.9536237  0.9646094  0.9387035  0.03539059  \n",
       "  0.0005  0.9601377  0.9544103  0.9660481  0.9403081  0.03395195  \n",
       "  0.0010  0.9601851  0.9533877  0.9671990  0.9403081  0.03280104  \n",
       "  0.0015  0.9597741  0.9525893  0.9671990  0.9396662  0.03280104  \n",
       "  0.0020  0.9570529  0.9528863  0.9613005  0.9358151  0.03869947  \n",
       "  0.0025  0.9560202  0.9508395  0.9613005  0.9342105  0.03869947  \n",
       "  0.0030  0.9550755  0.9504394  0.9598619  0.9328199  0.04013811  \n",
       "  0.0035  0.9528765  0.9527657  0.9532441  0.9298246  0.04675586  \n",
       "  0.0040  0.9525112  0.9508208  0.9543951  0.9291827  0.04560495  \n",
       "  0.0045  0.9526140  0.9502928  0.9551144  0.9292897  0.04488563  \n",
       "  0.0050  0.9526140  0.9502928  0.9551144  0.9292897  0.04488563  \n",
       "  0.0055  0.9500170  0.9492365  0.9509423  0.9255456  0.04905769  \n",
       "  0.0060  0.9498159  0.9488449  0.9509423  0.9252246  0.04905769  \n",
       "  0.0065  0.9487535  0.9487568  0.9489282  0.9237270  0.05107179  \n",
       "  0.0070  0.9486161  0.9484787  0.9489282  0.9235131  0.05107179  \n",
       "  0.0075  0.9486161  0.9484787  0.9489282  0.9235131  0.05107179  \n",
       "  0.0080  0.9484868  0.9480803  0.9490721  0.9232991  0.05092792  \n",
       "  0.0085  0.9480187  0.9471716  0.9490721  0.9225503  0.05092792  \n",
       "  0.0090  0.9471543  0.9447802  0.9496475  0.9211596  0.05035247  \n",
       "  0.0095  0.9465685  0.9455614  0.9476334  0.9204108  0.05236657  \n",
       "  0.0100  0.9450535  0.9402714  0.9499353  0.9178434  0.05006474  \n",
       "  Type_II_error\n",
       "  0.1364205    \n",
       "  0.1343346    \n",
       "  0.1376721    \n",
       "  0.1401752    \n",
       "  0.1380893    \n",
       "  0.1443471    \n",
       "  0.1455987    \n",
       "  0.1380893    \n",
       "  0.1439299    \n",
       "  0.1455987    \n",
       "  0.1455987    \n",
       "  0.1481018    \n",
       "  0.1493534    \n",
       "  0.1493534    \n",
       "  0.1501877    \n",
       "  0.1501877    \n",
       "  0.1514393    \n",
       "  0.1543596    \n",
       "  0.1614518    \n",
       "  0.1585315    \n",
       "  0.1752190    \n",
       "\n",
       "F1 was used to select the optimal model using the largest value.\n",
       "The final value used for the model was cp = 0.001."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "val<-seq(from = 0, to=0.01, by=0.0005)\n",
    "cart_grid<-expand.grid(cp=val)\n",
    "train_control<-trainControl(method=\"cv\", number =3, savePredictions = 'final',summaryFunction = f1)\n",
    "model_rpart<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, method='rpart',tuneGrid = cart_grid,\n",
    "                         metric = 'F1')\n",
    "model_rpart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model: 3-fold CV w/ a grid search <a name=\"xg\"></a>\n",
    "\n",
    "[Booster Parameter](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/)\n",
    "\n",
    "- nrounds[default=100]: similar to number of trees\n",
    "- eta[default=0.3][range: (0,1)]: learning rate; the lower, the slower\n",
    "- gamma[default=0][range: (0,Inf)]: prevent overfitting; the higher, the higher the regularization\n",
    "- max_depth[default=6][range: (0,Inf)]: depth of tree\n",
    "- min_child_weight[default=1][range:(0,Inf)]: prevent overfitting by stopping the tree splitting\n",
    "- subsample[default=1][range: (0,1)]: controls the number of samples for each tree\n",
    "- colsample_bytree[default=1][range: (0,1)]: controls the number of features to a tree\n",
    "- lambda[default=0]: L2 regularization on wieghts\n",
    "- alpha[default=1]: L1 regularization on weights\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eXtreme Gradient Boosting \n",
       "\n",
       "9348 samples\n",
       "  29 predictor\n",
       "   2 classes: '0', '1' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (3 fold) \n",
       "Summary of sample sizes: 6232, 6232, 6232 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  eta   max_depth  gamma  min_child_weight  nrounds  F1         Precesion\n",
       "  0.01   7         0      1                 100      0.9569986  0.9410363\n",
       "  0.01   7         0      1                 150      0.9592776  0.9461422\n",
       "  0.01   7         0      1                 200      0.9612768  0.9494960\n",
       "  0.01   7         0      3                 100      0.9563969  0.9397420\n",
       "  0.01   7         0      3                 150      0.9585583  0.9442056\n",
       "  0.01   7         0      3                 200      0.9608316  0.9479384\n",
       "  0.01   7         0      5                 100      0.9552941  0.9381432\n",
       "  0.01   7         0      5                 150      0.9567803  0.9410165\n",
       "  0.01   7         0      5                 200      0.9588166  0.9449607\n",
       "  0.01   7         1      1                 100      0.9571670  0.9421632\n",
       "  0.01   7         1      1                 150      0.9593687  0.9457763\n",
       "  0.01   7         1      1                 200      0.9610485  0.9495880\n",
       "  0.01   7         1      3                 100      0.9558630  0.9385798\n",
       "  0.01   7         1      3                 150      0.9580823  0.9432766\n",
       "  0.01   7         1      3                 200      0.9602533  0.9476380\n",
       "  0.01   7         1      5                 100      0.9541615  0.9356920\n",
       "  0.01   7         1      5                 150      0.9565276  0.9401313\n",
       "  0.01   7         1      5                 200      0.9583841  0.9446688\n",
       "  0.01   7         3      1                 100      0.9573641  0.9426828\n",
       "  0.01   7         3      1                 150      0.9590907  0.9453729\n",
       "  0.01   7         3      1                 200      0.9614835  0.9498929\n",
       "  0.01   7         3      3                 100      0.9565311  0.9400023\n",
       "  0.01   7         3      3                 150      0.9587107  0.9440980\n",
       "  0.01   7         3      3                 200      0.9604008  0.9476474\n",
       "  0.01   7         3      5                 100      0.9546741  0.9372196\n",
       "  0.01   7         3      5                 150      0.9567855  0.9408977\n",
       "  0.01   7         3      5                 200      0.9579035  0.9438698\n",
       "  0.01   9         0      1                 100      0.9637729  0.9535598\n",
       "  0.01   9         0      1                 150      0.9650970  0.9556012\n",
       "  0.01   9         0      1                 200      0.9669896  0.9581896\n",
       "  0.01   9         0      3                 100      0.9614081  0.9485460\n",
       "  0.01   9         0      3                 150      0.9636327  0.9517666\n",
       "  0.01   9         0      3                 200      0.9659179  0.9555316\n",
       "  0.01   9         0      5                 100      0.9601940  0.9472633\n",
       "  0.01   9         0      5                 150      0.9630244  0.9520906\n",
       "  0.01   9         0      5                 200      0.9646295  0.9543832\n",
       "  0.01   9         1      1                 100      0.9633923  0.9537707\n",
       "  0.01   9         1      1                 150      0.9653136  0.9557414\n",
       "  0.01   9         1      1                 200      0.9667686  0.9583043\n",
       "  0.01   9         1      3                 100      0.9614713  0.9488072\n",
       "  0.01   9         1      3                 150      0.9634274  0.9513652\n",
       "  0.01   9         1      3                 200      0.9657182  0.9550067\n",
       "  0.01   9         1      5                 100      0.9593724  0.9457947\n",
       "  0.01   9         1      5                 150      0.9625650  0.9506262\n",
       "  0.01   9         1      5                 200      0.9642250  0.9534530\n",
       "  0.01   9         3      1                 100      0.9623378  0.9524012\n",
       "  0.01   9         3      1                 150      0.9638154  0.9543247\n",
       "  0.01   9         3      1                 200      0.9660789  0.9569544\n",
       "  0.01   9         3      3                 100      0.9608549  0.9492347\n",
       "  0.01   9         3      3                 150      0.9631048  0.9519668\n",
       "  0.01   9         3      3                 200      0.9649112  0.9548035\n",
       "  0.01   9         3      5                 100      0.9599735  0.9472397\n",
       "  0.01   9         3      5                 150      0.9619232  0.9500728\n",
       "  0.01   9         3      5                 200      0.9636339  0.9534058\n",
       "  0.01  11         0      1                 100      0.9671447  0.9582379\n",
       "  0.01  11         0      1                 150      0.9695740  0.9604921\n",
       "  0.01  11         0      1                 200      0.9716934  0.9632649\n",
       "  0.01  11         0      3                 100      0.9649344  0.9544743\n",
       "  0.01  11         0      3                 150      0.9675308  0.9580246\n",
       "  0.01  11         0      3                 200      0.9691564  0.9599588\n",
       "  0.01  11         0      5                 100      0.9618862  0.9526535\n",
       "  0.01  11         0      5                 150      0.9642014  0.9541329\n",
       "  0.01  11         0      5                 200      0.9663822  0.9568686\n",
       "  0.01  11         1      1                 100      0.9667213  0.9578206\n",
       "  0.01  11         1      1                 150      0.9688486  0.9601735\n",
       "  0.01  11         1      1                 200      0.9712609  0.9629690\n",
       "  0.01  11         1      3                 100      0.9652939  0.9547707\n",
       "  0.01  11         1      3                 150      0.9671062  0.9576180\n",
       "  0.01  11         1      3                 200      0.9687062  0.9601847\n",
       "  0.01  11         1      5                 100      0.9614299  0.9528664\n",
       "  0.01  11         1      5                 150      0.9644348  0.9537682\n",
       "  0.01  11         1      5                 200      0.9668096  0.9572902\n",
       "  0.01  11         3      1                 100      0.9652134  0.9566555\n",
       "  0.01  11         3      1                 150      0.9678860  0.9602359\n",
       "  0.01  11         3      1                 200      0.9695308  0.9617942\n",
       "  0.01  11         3      3                 100      0.9637401  0.9528206\n",
       "  0.01  11         3      3                 150      0.9664410  0.9575489\n",
       "  0.01  11         3      3                 200      0.9680564  0.9597448\n",
       "  0.01  11         3      5                 100      0.9616151  0.9521028\n",
       "  0.01  11         3      5                 150      0.9635731  0.9531676\n",
       "  0.01  11         3      5                 200      0.9661029  0.9564565\n",
       "  0.01  13         0      1                 100      0.9686230  0.9603157\n",
       "  0.01  13         0      1                 150      0.9725284  0.9646282\n",
       "  0.01  13         0      1                 200      0.9740471  0.9655277\n",
       "  0.01  13         0      3                 100      0.9645314  0.9551888\n",
       "  0.01  13         0      3                 150      0.9680649  0.9596217\n",
       "  0.01  13         0      3                 200      0.9710276  0.9634903\n",
       "  0.01  13         0      5                 100      0.9606181  0.9527770\n",
       "  0.01  13         0      5                 150      0.9651134  0.9573102\n",
       "  0.01  13         0      5                 200      0.9678809  0.9603709\n",
       "  0.01  13         1      1                 100      0.9681857  0.9601368\n",
       "  0.01  13         1      1                 150      0.9718312  0.9634068\n",
       "  0.01  13         1      1                 200      0.9735134  0.9661547\n",
       "  0.01  13         1      3                 100      0.9646473  0.9559696\n",
       "  0.01  13         1      3                 150      0.9677089  0.9592014\n",
       "  0.01  13         1      3                 200      0.9703302  0.9622511\n",
       "  0.01  13         1      5                 100      0.9604443  0.9532640\n",
       "  0.01  13         1      5                 150      0.9652498  0.9575796\n",
       "  0.01  13         1      5                 200      0.9678085  0.9603695\n",
       "  0.01  13         3      1                 100      0.9663772  0.9592527\n",
       "  0.01  13         3      1                 150      0.9695688  0.9628531\n",
       "  0.01  13         3      1                 200      0.9719885  0.9656602\n",
       "  0.01  13         3      3                 100      0.9641596  0.9551477\n",
       "  0.01  13         3      3                 150      0.9667825  0.9582097\n",
       "  0.01  13         3      3                 200      0.9688660  0.9618920\n",
       "  0.01  13         3      5                 100      0.9607744  0.9526698\n",
       "  0.01  13         3      5                 150      0.9640705  0.9555366\n",
       "  0.01  13         3      5                 200      0.9673137  0.9596760\n",
       "  0.10   7         0      1                 100      0.9770498  0.9710429\n",
       "  0.10   7         0      1                 150      0.9805954  0.9759436\n",
       "  0.10   7         0      1                 200      0.9841549  0.9808868\n",
       "  0.10   7         0      3                 100      0.9778330  0.9720135\n",
       "  0.10   7         0      3                 150      0.9803128  0.9756612\n",
       "  0.10   7         0      3                 200      0.9813699  0.9776019\n",
       "  0.10   7         0      5                 100      0.9762728  0.9699288\n",
       "  0.10   7         0      5                 150      0.9787221  0.9747509\n",
       "  0.10   7         0      5                 200      0.9797287  0.9756124\n",
       "  0.10   7         1      1                 100      0.9769659  0.9714363\n",
       "  0.10   7         1      1                 150      0.9798750  0.9755002\n",
       "  0.10   7         1      1                 200      0.9810240  0.9765051\n",
       "  0.10   7         1      3                 100      0.9772107  0.9705023\n",
       "  0.10   7         1      3                 150      0.9799583  0.9752361\n",
       "  0.10   7         1      3                 200      0.9805866  0.9766223\n",
       "  0.10   7         1      5                 100      0.9755523  0.9694772\n",
       "  0.10   7         1      5                 150      0.9780845  0.9737732\n",
       "  0.10   7         1      5                 200      0.9788648  0.9748938\n",
       "  0.10   7         3      1                 100      0.9746521  0.9674266\n",
       "  0.10   7         3      1                 150      0.9747920  0.9677052\n",
       "  0.10   7         3      1                 200      0.9747920  0.9677052\n",
       "  0.10   7         3      3                 100      0.9759513  0.9684522\n",
       "  0.10   7         3      3                 150      0.9761660  0.9687393\n",
       "  0.10   7         3      3                 200      0.9761660  0.9687393\n",
       "  0.10   7         3      5                 100      0.9750049  0.9679788\n",
       "  0.10   7         3      5                 150      0.9762544  0.9705732\n",
       "  0.10   7         3      5                 200      0.9762544  0.9705732\n",
       "  0.10   9         0      1                 100      0.9818007  0.9778896\n",
       "  0.10   9         0      1                 150      0.9846540  0.9814456\n",
       "  0.10   9         0      1                 200      0.9848641  0.9820039\n",
       "  0.10   9         0      3                 100      0.9801476  0.9765978\n",
       "  0.10   9         0      3                 150      0.9830057  0.9798577\n",
       "  0.10   9         0      3                 200      0.9839395  0.9807249\n",
       "  0.10   9         0      5                 100      0.9777939  0.9737653\n",
       "  0.10   9         0      5                 150      0.9802863  0.9768654\n",
       "  0.10   9         0      5                 200      0.9817929  0.9784378\n",
       "  0.10   9         1      1                 100      0.9815906  0.9774784\n",
       "  0.10   9         1      1                 150      0.9831536  0.9797335\n",
       "  0.10   9         1      1                 200      0.9835834  0.9803062\n",
       "  0.10   9         1      3                 100      0.9796619  0.9752104\n",
       "  0.10   9         1      3                 150      0.9819433  0.9781846\n",
       "  0.10   9         1      3                 200      0.9820108  0.9784604\n",
       "  0.10   9         1      5                 100      0.9780940  0.9735190\n",
       "  0.10   9         1      5                 150      0.9800093  0.9761788\n",
       "  0.10   9         1      5                 200      0.9803673  0.9766035\n",
       "  0.10   9         3      1                 100      0.9774036  0.9717338\n",
       "  0.10   9         3      1                 150      0.9774036  0.9717338\n",
       "  0.10   9         3      1                 200      0.9774036  0.9717338\n",
       "  0.10   9         3      3                 100      0.9765596  0.9703650\n",
       "  0.10   9         3      3                 150      0.9765596  0.9703650\n",
       "  0.10   9         3      3                 200      0.9765596  0.9703650\n",
       "  0.10   9         3      5                 100      0.9770268  0.9721049\n",
       "  0.10   9         3      5                 150      0.9770330  0.9718374\n",
       "  0.10   9         3      5                 200      0.9770330  0.9718374\n",
       "  0.10  11         0      1                 100      0.9837191  0.9808504\n",
       "  0.10  11         0      1                 150      0.9850795  0.9822826\n",
       "  0.10  11         0      1                 200      0.9855818  0.9828451\n",
       "  0.10  11         0      3                 100      0.9812126  0.9778752\n",
       "  0.10  11         0      3                 150      0.9839382  0.9807312\n",
       "  0.10  11         0      3                 200      0.9845100  0.9815768\n",
       "  0.10  11         0      5                 100      0.9791410  0.9755888\n",
       "  0.10  11         0      5                 150      0.9809265  0.9777130\n",
       "  0.10  11         0      5                 200      0.9826417  0.9799857\n",
       "  0.10  11         1      1                 100      0.9825006  0.9795692\n",
       "  0.10  11         1      1                 150      0.9836368  0.9814108\n",
       "  0.10  11         1      1                 200      0.9836368  0.9814108\n",
       "  0.10  11         1      3                 100      0.9813596  0.9780180\n",
       "  0.10  11         1      3                 150      0.9819953  0.9791456\n",
       "  0.10  11         1      3                 200      0.9820654  0.9792843\n",
       "  0.10  11         1      5                 100      0.9785810  0.9744723\n",
       "  0.10  11         1      5                 150      0.9809223  0.9779870\n",
       "  0.10  11         1      5                 200      0.9816346  0.9791168\n",
       "  0.10  11         3      1                 100      0.9770868  0.9726468\n",
       "  0.10  11         3      1                 150      0.9770868  0.9726468\n",
       "  0.10  11         3      1                 200      0.9770868  0.9726468\n",
       "  0.10  11         3      3                 100      0.9781573  0.9739257\n",
       "  0.10  11         3      3                 150      0.9781573  0.9739257\n",
       "  0.10  11         3      3                 200      0.9781573  0.9739257\n",
       "  0.10  11         3      5                 100      0.9765974  0.9716809\n",
       "  0.10  11         3      5                 150      0.9765974  0.9716809\n",
       "  0.10  11         3      5                 200      0.9765974  0.9716809\n",
       "  0.10  13         0      1                 100      0.9840753  0.9812811\n",
       "  0.10  13         0      1                 150      0.9847110  0.9824053\n",
       "  0.10  13         0      1                 200      0.9846335  0.9826774\n",
       "  0.10  13         0      3                 100      0.9824876  0.9802509\n",
       "  0.10  13         0      3                 150      0.9834240  0.9811180\n",
       "  0.10  13         0      3                 200      0.9837871  0.9812726\n",
       "  0.10  13         0      5                 100      0.9806298  0.9778391\n",
       "  0.10  13         0      5                 150      0.9828536  0.9802635\n",
       "  0.10  13         0      5                 200      0.9833531  0.9809732\n",
       "  0.10  13         1      1                 100      0.9842086  0.9819667\n",
       "  0.10  13         1      1                 150      0.9845729  0.9819866\n",
       "  0.10  13         1      1                 200      0.9845729  0.9819866\n",
       "  0.10  13         1      3                 100      0.9817739  0.9792707\n",
       "  0.10  13         1      3                 150      0.9830055  0.9800032\n",
       "  0.10  13         1      3                 200      0.9829347  0.9798613\n",
       "  0.10  13         1      5                 100      0.9791310  0.9759866\n",
       "  0.10  13         1      5                 150      0.9805587  0.9778298\n",
       "  0.10  13         1      5                 200      0.9805587  0.9778298\n",
       "  0.10  13         3      1                 100      0.9784164  0.9751478\n",
       "  0.10  13         3      1                 150      0.9784164  0.9751478\n",
       "  0.10  13         3      1                 200      0.9784164  0.9751478\n",
       "  0.10  13         3      3                 100      0.9788611  0.9751816\n",
       "  0.10  13         3      3                 150      0.9788611  0.9751816\n",
       "  0.10  13         3      3                 200      0.9788611  0.9751816\n",
       "  0.10  13         3      5                 100      0.9778582  0.9740462\n",
       "  0.10  13         3      5                 150      0.9778582  0.9740462\n",
       "  0.10  13         3      5                 200      0.9778582  0.9740462\n",
       "  0.30   7         0      1                 100      0.9846601  0.9814474\n",
       "  0.30   7         0      1                 150      0.9835683  0.9811319\n",
       "  0.30   7         0      1                 200      0.9841374  0.9819750\n",
       "  0.30   7         0      3                 100      0.9829455  0.9791997\n",
       "  0.30   7         0      3                 150      0.9831500  0.9798745\n",
       "  0.30   7         0      3                 200      0.9829494  0.9790465\n",
       "  0.30   7         0      5                 100      0.9817286  0.9777405\n",
       "  0.30   7         0      5                 150      0.9820854  0.9783130\n",
       "  0.30   7         0      5                 200      0.9822447  0.9774970\n",
       "  0.30   7         1      1                 100      0.9831618  0.9791892\n",
       "  0.30   7         1      1                 150      0.9831618  0.9791892\n",
       "  0.30   7         1      1                 200      0.9831618  0.9791892\n",
       "  0.30   7         1      3                 100      0.9804958  0.9774397\n",
       "  0.30   7         1      3                 150      0.9804958  0.9774397\n",
       "  0.30   7         1      3                 200      0.9804958  0.9774397\n",
       "  0.30   7         1      5                 100      0.9794452  0.9749231\n",
       "  0.30   7         1      5                 150      0.9794452  0.9749231\n",
       "  0.30   7         1      5                 200      0.9794452  0.9749231\n",
       "  0.30   7         3      1                 100      0.9741268  0.9677960\n",
       "  0.30   7         3      1                 150      0.9741268  0.9677960\n",
       "  0.30   7         3      1                 200      0.9741268  0.9677960\n",
       "  0.30   7         3      3                 100      0.9751902  0.9694884\n",
       "  0.30   7         3      3                 150      0.9751902  0.9694884\n",
       "  0.30   7         3      3                 200      0.9751902  0.9694884\n",
       "  0.30   7         3      5                 100      0.9750215  0.9701067\n",
       "  0.30   7         3      5                 150      0.9750215  0.9701067\n",
       "  0.30   7         3      5                 200      0.9750215  0.9701067\n",
       "  0.30   9         0      1                 100      0.9849451  0.9815981\n",
       "  0.30   9         0      1                 150      0.9841464  0.9814227\n",
       "  0.30   9         0      1                 200      0.9842882  0.9816990\n",
       "  0.30   9         0      3                 100      0.9833529  0.9806986\n",
       "  0.30   9         0      3                 150      0.9828587  0.9799927\n",
       "  0.30   9         0      3                 200      0.9832092  0.9806888\n",
       "  0.30   9         0      5                 100      0.9829922  0.9805571\n",
       "  0.30   9         0      5                 150      0.9830646  0.9805595\n",
       "  0.30   9         0      5                 200      0.9825732  0.9794259\n",
       "  0.30   9         1      1                 100      0.9835521  0.9818077\n",
       "  0.30   9         1      1                 150      0.9835521  0.9818077\n",
       "  0.30   9         1      1                 200      0.9835521  0.9818077\n",
       "  0.30   9         1      3                 100      0.9829277  0.9801393\n",
       "  0.30   9         1      3                 150      0.9829277  0.9801393\n",
       "  0.30   9         1      3                 200      0.9829277  0.9801393\n",
       "  0.30   9         1      5                 100      0.9800680  0.9768537\n",
       "  0.30   9         1      5                 150      0.9800680  0.9768537\n",
       "  0.30   9         1      5                 200      0.9800680  0.9768537\n",
       "  0.30   9         3      1                 100      0.9775956  0.9728331\n",
       "  0.30   9         3      1                 150      0.9775956  0.9728331\n",
       "  0.30   9         3      1                 200      0.9775956  0.9728331\n",
       "  0.30   9         3      3                 100      0.9756875  0.9701857\n",
       "  0.30   9         3      3                 150      0.9756875  0.9701857\n",
       "  0.30   9         3      3                 200      0.9756875  0.9701857\n",
       "  0.30   9         3      5                 100      0.9758908  0.9705692\n",
       "  0.30   9         3      5                 150      0.9758908  0.9705692\n",
       "  0.30   9         3      5                 200      0.9758908  0.9705692\n",
       "  0.30  11         0      1                 100      0.9847922  0.9821329\n",
       "  0.30  11         0      1                 150      0.9847878  0.9824081\n",
       "  0.30  11         0      1                 200      0.9847148  0.9824071\n",
       "  0.30  11         0      3                 100      0.9836387  0.9811331\n",
       "  0.30  11         0      3                 150      0.9835724  0.9807167\n",
       "  0.30  11         0      3                 200      0.9832784  0.9808461\n",
       "  0.30  11         0      5                 100      0.9816442  0.9785780\n",
       "  0.30  11         0      5                 150      0.9822937  0.9788775\n",
       "  0.30  11         0      5                 200      0.9825197  0.9786009\n",
       "  0.30  11         1      1                 100      0.9845060  0.9817076\n",
       "  0.30  11         1      1                 150      0.9845060  0.9817076\n",
       "  0.30  11         1      1                 200      0.9845060  0.9817076\n",
       "  0.30  11         1      3                 100      0.9830115  0.9795898\n",
       "  0.30  11         1      3                 150      0.9830115  0.9795898\n",
       "  0.30  11         1      3                 200      0.9830115  0.9795898\n",
       "  0.30  11         1      5                 100      0.9814995  0.9782893\n",
       "  0.30  11         1      5                 150      0.9814995  0.9782893\n",
       "  0.30  11         1      5                 200      0.9814995  0.9782893\n",
       "  0.30  11         3      1                 100      0.9779959  0.9742996\n",
       "  0.30  11         3      1                 150      0.9779959  0.9742996\n",
       "  0.30  11         3      1                 200      0.9779959  0.9742996\n",
       "  0.30  11         3      3                 100      0.9780727  0.9742006\n",
       "  0.30  11         3      3                 150      0.9780727  0.9742006\n",
       "  0.30  11         3      3                 200      0.9780727  0.9742006\n",
       "  0.30  11         3      5                 100      0.9768128  0.9719696\n",
       "  0.30  11         3      5                 150      0.9768128  0.9719696\n",
       "  0.30  11         3      5                 200      0.9768128  0.9719696\n",
       "  0.30  13         0      1                 100      0.9847020  0.9830894\n",
       "  0.30  13         0      1                 150      0.9844107  0.9830759\n",
       "  0.30  13         0      1                 200      0.9843393  0.9829338\n",
       "  0.30  13         0      3                 100      0.9834325  0.9804276\n",
       "  0.30  13         0      3                 150      0.9833612  0.9802827\n",
       "  0.30  13         0      3                 200      0.9829277  0.9801294\n",
       "  0.30  13         0      5                 100      0.9826408  0.9799916\n",
       "  0.30  13         0      5                 150      0.9826529  0.9792990\n",
       "  0.30  13         0      5                 200      0.9825745  0.9795656\n",
       "  0.30  13         1      1                 100      0.9842259  0.9808641\n",
       "  0.30  13         1      1                 150      0.9842259  0.9808641\n",
       "  0.30  13         1      1                 200      0.9842259  0.9808641\n",
       "  0.30  13         1      3                 100      0.9822192  0.9789996\n",
       "  0.30  13         1      3                 150      0.9822192  0.9789996\n",
       "  0.30  13         1      3                 200      0.9822192  0.9789996\n",
       "  0.30  13         1      5                 100      0.9817986  0.9780262\n",
       "  0.30  13         1      5                 150      0.9817986  0.9780262\n",
       "  0.30  13         1      5                 200      0.9817986  0.9780262\n",
       "  0.30  13         3      1                 100      0.9779016  0.9752535\n",
       "  0.30  13         3      1                 150      0.9779016  0.9752535\n",
       "  0.30  13         3      1                 200      0.9779016  0.9752535\n",
       "  0.30  13         3      3                 100      0.9784956  0.9750317\n",
       "  0.30  13         3      3                 150      0.9784956  0.9750317\n",
       "  0.30  13         3      3                 200      0.9784956  0.9750317\n",
       "  0.30  13         3      5                 100      0.9782679  0.9754242\n",
       "  0.30  13         3      5                 150      0.9782679  0.9754242\n",
       "  0.30  13         3      5                 200      0.9782679  0.9754242\n",
       "  Recall     Accuracy   Type_I_error  Type_II_error\n",
       "  0.9735290  0.9349593  0.02647101    0.17688778   \n",
       "  0.9728097  0.9385965  0.02719033    0.16061744   \n",
       "  0.9733851  0.9416988  0.02661488    0.15018773   \n",
       "  0.9736729  0.9339966  0.02632715    0.18105966   \n",
       "  0.9733851  0.9374198  0.02661488    0.16687526   \n",
       "  0.9741044  0.9409499  0.02589555    0.15519399   \n",
       "  0.9730974  0.9322850  0.02690260    0.18606592   \n",
       "  0.9730974  0.9346384  0.02690260    0.17688778   \n",
       "  0.9730974  0.9378477  0.02690260    0.16437213   \n",
       "  0.9726658  0.9352803  0.02733420    0.17313308   \n",
       "  0.9733851  0.9387035  0.02661488    0.16186900   \n",
       "  0.9728097  0.9413778  0.02719033    0.14977055   \n",
       "  0.9738167  0.9331408  0.02618328    0.18481435   \n",
       "  0.9733851  0.9366709  0.02661488    0.16979558   \n",
       "  0.9732413  0.9400941  0.02675874    0.15602837   \n",
       "  0.9733851  0.9304664  0.02661488    0.19399249   \n",
       "  0.9735290  0.9342105  0.02647101    0.17980809   \n",
       "  0.9725219  0.9372058  0.02747806    0.16520651   \n",
       "  0.9725219  0.9356012  0.02747806    0.17146433   \n",
       "  0.9732413  0.9382756  0.02675874    0.16312057   \n",
       "  0.9733851  0.9420197  0.02661488    0.14893617   \n",
       "  0.9736729  0.9342105  0.02632715    0.18022528   \n",
       "  0.9738167  0.9376337  0.02618328    0.16729245   \n",
       "  0.9735290  0.9403081  0.02647101    0.15602837   \n",
       "  0.9728097  0.9313222  0.02719033    0.18898623   \n",
       "  0.9732413  0.9346384  0.02675874    0.17730496   \n",
       "  0.9723781  0.9364570  0.02762192    0.16770964   \n",
       "  0.9742483  0.9455499  0.02575169    0.13767209   \n",
       "  0.9748238  0.9475824  0.02517623    0.13141427   \n",
       "  0.9759747  0.9504707  0.02402532    0.12348769   \n",
       "  0.9746799  0.9418057  0.02532010    0.15352524   \n",
       "  0.9758308  0.9452289  0.02416918    0.14351272   \n",
       "  0.9765501  0.9487591  0.02344986    0.13183146   \n",
       "  0.9735290  0.9399872  0.02647101    0.15727993   \n",
       "  0.9742483  0.9443731  0.02575169    0.14226116   \n",
       "  0.9751115  0.9468335  0.02488851    0.13516896   \n",
       "  0.9732413  0.9450150  0.02675874    0.13683771   \n",
       "  0.9751115  0.9479033  0.02488851    0.13099708   \n",
       "  0.9753992  0.9501498  0.02460078    0.12307050   \n",
       "  0.9745360  0.9419127  0.02546396    0.15269086   \n",
       "  0.9758308  0.9449080  0.02416918    0.14476429   \n",
       "  0.9766940  0.9484382  0.02330600    0.13350021   \n",
       "  0.9733851  0.9387035  0.02661488    0.16186900   \n",
       "  0.9748238  0.9436243  0.02517623    0.14685023   \n",
       "  0.9752554  0.9461917  0.02474464    0.13808928   \n",
       "  0.9725219  0.9434104  0.02747806    0.14100960   \n",
       "  0.9735290  0.9456568  0.02647101    0.13516896   \n",
       "  0.9753992  0.9490800  0.02460078    0.12724239   \n",
       "  0.9728097  0.9410569  0.02719033    0.15102211   \n",
       "  0.9745360  0.9444801  0.02546396    0.14267835   \n",
       "  0.9752554  0.9472614  0.02474464    0.13391740   \n",
       "  0.9730974  0.9396662  0.02690260    0.15727993   \n",
       "  0.9741044  0.9426615  0.02589555    0.14851898   \n",
       "  0.9741044  0.9453359  0.02589555    0.13808928   \n",
       "  0.9762624  0.9506846  0.02373759    0.12348769   \n",
       "  0.9788520  0.9543218  0.02114804    0.11681268   \n",
       "  0.9802906  0.9575310  0.01970939    0.10846892   \n",
       "  0.9756870  0.9472614  0.02431305    0.13516896   \n",
       "  0.9772695  0.9512195  0.02273054    0.12432207   \n",
       "  0.9785642  0.9536799  0.02143576    0.11848144   \n",
       "  0.9713710  0.9427685  0.02862897    0.14017522   \n",
       "  0.9745360  0.9461917  0.02546396    0.13600334   \n",
       "  0.9761185  0.9495079  0.02388146    0.12765957   \n",
       "  0.9758308  0.9500428  0.02416918    0.12473926   \n",
       "  0.9777011  0.9532520  0.02229895    0.11764706   \n",
       "  0.9797151  0.9568892  0.02028485    0.10930330   \n",
       "  0.9761185  0.9477963  0.02388146    0.13433458   \n",
       "  0.9768379  0.9505777  0.02316213    0.12557363   \n",
       "  0.9774133  0.9530381  0.02258668    0.11764706   \n",
       "  0.9702201  0.9421267  0.02977989    0.13934084   \n",
       "  0.9753992  0.9465126  0.02460078    0.13725490   \n",
       "  0.9765501  0.9501498  0.02344986    0.12640801   \n",
       "  0.9739606  0.9477963  0.02603942    0.12807676   \n",
       "  0.9756870  0.9518614  0.02431305    0.11722987   \n",
       "  0.9774133  0.9543218  0.02258668    0.11264080   \n",
       "  0.9749676  0.9454429  0.02503237    0.14017522   \n",
       "  0.9755431  0.9496149  0.02445691    0.12557363   \n",
       "  0.9765501  0.9520753  0.02344986    0.11889862   \n",
       "  0.9713710  0.9423406  0.02862897    0.14184397   \n",
       "  0.9742483  0.9452289  0.02575169    0.13892365   \n",
       "  0.9759747  0.9490800  0.02402532    0.12891114   \n",
       "  0.9771256  0.9529311  0.02287441    0.11722987   \n",
       "  0.9805783  0.9588147  0.01942167    0.10429704   \n",
       "  0.9827363  0.9610612  0.01726370    0.10179391   \n",
       "  0.9741044  0.9467266  0.02589555    0.13266583   \n",
       "  0.9766940  0.9520753  0.02330600    0.11931581   \n",
       "  0.9787081  0.9565682  0.02129190    0.10763454   \n",
       "  0.9686376  0.9409499  0.03136239    0.13934084   \n",
       "  0.9730974  0.9476893  0.02690260    0.12599082   \n",
       "  0.9755431  0.9518614  0.02445691    0.11681268   \n",
       "  0.9764063  0.9522893  0.02359373    0.11764706   \n",
       "  0.9804345  0.9577450  0.01956553    0.10805173   \n",
       "  0.9810099  0.9603124  0.01899007    0.09970797   \n",
       "  0.9735290  0.9469405  0.02647101    0.13016270   \n",
       "  0.9764063  0.9515404  0.02359373    0.12056738   \n",
       "  0.9785642  0.9554985  0.02143576    0.11138924   \n",
       "  0.9677744  0.9407360  0.03222558    0.13767209   \n",
       "  0.9730974  0.9479033  0.02690260    0.12515645   \n",
       "  0.9753992  0.9517544  0.02460078    0.11681268   \n",
       "  0.9736729  0.9496149  0.02632715    0.12015019   \n",
       "  0.9764063  0.9544288  0.02359373    0.10930330   \n",
       "  0.9784204  0.9580659  0.02157963    0.10095953   \n",
       "  0.9733851  0.9461917  0.02661488    0.13266583   \n",
       "  0.9755431  0.9501498  0.02445691    0.12348769   \n",
       "  0.9759747  0.9533590  0.02402532    0.11222361   \n",
       "  0.9690692  0.9411639  0.03093080    0.13975803   \n",
       "  0.9728097  0.9460847  0.02719033    0.13141427   \n",
       "  0.9751115  0.9510056  0.02488851    0.11889862   \n",
       "  0.9831679  0.9656611  0.01683211    0.08510638   \n",
       "  0.9853259  0.9710098  0.01467415    0.07050480   \n",
       "  0.9874838  0.9763586  0.01251618    0.05590321   \n",
       "  0.9837433  0.9668378  0.01625665    0.08218607   \n",
       "  0.9850381  0.9705819  0.01496188    0.07133917   \n",
       "  0.9851820  0.9721866  0.01481801    0.06549854   \n",
       "  0.9827363  0.9644844  0.01726370    0.08844389   \n",
       "  0.9827363  0.9682285  0.01726370    0.07384230   \n",
       "  0.9838872  0.9697261  0.01611279    0.07133917   \n",
       "  0.9825924  0.9655541  0.01740757    0.08385482   \n",
       "  0.9843188  0.9699401  0.01568120    0.07175636   \n",
       "  0.9856136  0.9716517  0.01438642    0.06883605   \n",
       "  0.9840311  0.9658751  0.01596893    0.08677514   \n",
       "  0.9847504  0.9700471  0.01524960    0.07259074   \n",
       "  0.9846065  0.9710098  0.01539347    0.06841886   \n",
       "  0.9817292  0.9634146  0.01827075    0.08969545   \n",
       "  0.9824486  0.9672657  0.01755143    0.07676262   \n",
       "  0.9828802  0.9684424  0.01711984    0.07342511   \n",
       "  0.9820170  0.9620240  0.01798302    0.09595327   \n",
       "  0.9820170  0.9622379  0.01798302    0.09511890   \n",
       "  0.9820170  0.9622379  0.01798302    0.09511890   \n",
       "  0.9835995  0.9639495  0.01640052    0.09303296   \n",
       "  0.9837433  0.9642704  0.01625665    0.09219858   \n",
       "  0.9837433  0.9642704  0.01625665    0.09219858   \n",
       "  0.9821608  0.9625588  0.01783916    0.09428452   \n",
       "  0.9820170  0.9644844  0.01798302    0.08635795   \n",
       "  0.9820170  0.9644844  0.01798302    0.08635795   \n",
       "  0.9857574  0.9728284  0.01424256    0.06466416   \n",
       "  0.9879154  0.9771074  0.01208459    0.05423446   \n",
       "  0.9877715  0.9774283  0.01222846    0.05256571   \n",
       "  0.9837433  0.9703680  0.01625665    0.06841886   \n",
       "  0.9861890  0.9746470  0.01381096    0.05882353   \n",
       "  0.9871961  0.9760377  0.01280391    0.05632040   \n",
       "  0.9818731  0.9668378  0.01812689    0.07676262   \n",
       "  0.9837433  0.9705819  0.01625665    0.06758448   \n",
       "  0.9851820  0.9728284  0.01481801    0.06299541   \n",
       "  0.9857574  0.9725075  0.01424256    0.06591573   \n",
       "  0.9866206  0.9748609  0.01337937    0.05924072   \n",
       "  0.9869084  0.9755028  0.01309164    0.05757196   \n",
       "  0.9841749  0.9696192  0.01582506    0.07259074   \n",
       "  0.9857574  0.9730424  0.01424256    0.06382979   \n",
       "  0.9856136  0.9731493  0.01438642    0.06299541   \n",
       "  0.9827363  0.9672657  0.01726370    0.07759700   \n",
       "  0.9838872  0.9701540  0.01611279    0.06967042   \n",
       "  0.9841749  0.9706889  0.01582506    0.06841886   \n",
       "  0.9831679  0.9661960  0.01683211    0.08302044   \n",
       "  0.9831679  0.9661960  0.01683211    0.08302044   \n",
       "  0.9831679  0.9661960  0.01683211    0.08302044   \n",
       "  0.9828802  0.9649123  0.01711984    0.08719232   \n",
       "  0.9828802  0.9649123  0.01711984    0.08719232   \n",
       "  0.9828802  0.9649123  0.01711984    0.08719232   \n",
       "  0.9820170  0.9656611  0.01798302    0.08176888   \n",
       "  0.9823047  0.9656611  0.01769530    0.08260325   \n",
       "  0.9823047  0.9656611  0.01769530    0.08260325   \n",
       "  0.9866206  0.9757167  0.01337937    0.05590321   \n",
       "  0.9879154  0.9777493  0.01208459    0.05173133   \n",
       "  0.9883470  0.9784981  0.01165300    0.05006258   \n",
       "  0.9846065  0.9719726  0.01539347    0.06466416   \n",
       "  0.9871961  0.9760377  0.01280391    0.05632040   \n",
       "  0.9874838  0.9768935  0.01251618    0.05381727   \n",
       "  0.9827363  0.9688703  0.01726370    0.07133917   \n",
       "  0.9841749  0.9715447  0.01582506    0.06508135   \n",
       "  0.9853259  0.9741121  0.01467415    0.05840634   \n",
       "  0.9854697  0.9738982  0.01453028    0.05965791   \n",
       "  0.9859013  0.9756098  0.01409869    0.05423446   \n",
       "  0.9859013  0.9756098  0.01409869    0.05423446   \n",
       "  0.9847504  0.9721866  0.01524960    0.06424698   \n",
       "  0.9848943  0.9731493  0.01510574    0.06090947   \n",
       "  0.9848943  0.9732563  0.01510574    0.06049228   \n",
       "  0.9827363  0.9680145  0.01726370    0.07467668   \n",
       "  0.9838872  0.9715447  0.01611279    0.06424698   \n",
       "  0.9841749  0.9726145  0.01582506    0.06090947   \n",
       "  0.9815854  0.9657681  0.01841462    0.08010013   \n",
       "  0.9815854  0.9657681  0.01841462    0.08010013   \n",
       "  0.9815854  0.9657681  0.01841462    0.08010013   \n",
       "  0.9824486  0.9673727  0.01755143    0.07634543   \n",
       "  0.9824486  0.9673727  0.01755143    0.07634543   \n",
       "  0.9824486  0.9673727  0.01755143    0.07634543   \n",
       "  0.9815854  0.9650193  0.01841462    0.08302044   \n",
       "  0.9815854  0.9650193  0.01841462    0.08302044   \n",
       "  0.9815854  0.9650193  0.01841462    0.08302044   \n",
       "  0.9869084  0.9762516  0.01309164    0.05465165   \n",
       "  0.9870522  0.9772144  0.01294778    0.05131414   \n",
       "  0.9866206  0.9771074  0.01337937    0.05047977   \n",
       "  0.9847504  0.9738982  0.01524960    0.05757196   \n",
       "  0.9857574  0.9752888  0.01424256    0.05506884   \n",
       "  0.9863329  0.9758237  0.01366710    0.05465165   \n",
       "  0.9834556  0.9711168  0.01654438    0.06466416   \n",
       "  0.9854697  0.9744330  0.01453028    0.05757196   \n",
       "  0.9857574  0.9751819  0.01424256    0.05548602   \n",
       "  0.9864768  0.9764656  0.01352323    0.05256571   \n",
       "  0.9871961  0.9770004  0.01280391    0.05256571   \n",
       "  0.9871961  0.9770004  0.01280391    0.05256571   \n",
       "  0.9843188  0.9728284  0.01568120    0.06049228   \n",
       "  0.9860452  0.9746470  0.01395483    0.05840634   \n",
       "  0.9860452  0.9745400  0.01395483    0.05882353   \n",
       "  0.9823047  0.9688703  0.01769530    0.07008761   \n",
       "  0.9833118  0.9710098  0.01668825    0.06466416   \n",
       "  0.9833118  0.9710098  0.01668825    0.06466416   \n",
       "  0.9817292  0.9678006  0.01827075    0.07259074   \n",
       "  0.9817292  0.9678006  0.01827075    0.07259074   \n",
       "  0.9817292  0.9678006  0.01827075    0.07259074   \n",
       "  0.9825924  0.9684424  0.01740757    0.07259074   \n",
       "  0.9825924  0.9684424  0.01740757    0.07259074   \n",
       "  0.9825924  0.9684424  0.01740757    0.07259074   \n",
       "  0.9817292  0.9669448  0.01827075    0.07592824   \n",
       "  0.9817292  0.9669448  0.01827075    0.07592824   \n",
       "  0.9817292  0.9669448  0.01827075    0.07592824   \n",
       "  0.9879154  0.9771074  0.01208459    0.05423446   \n",
       "  0.9860452  0.9755028  0.01395483    0.05506884   \n",
       "  0.9863329  0.9763586  0.01366710    0.05256571   \n",
       "  0.9867645  0.9745400  0.01323551    0.06090947   \n",
       "  0.9864768  0.9748609  0.01352323    0.05882353   \n",
       "  0.9869084  0.9745400  0.01309164    0.06132666   \n",
       "  0.9857574  0.9727214  0.01424256    0.06508135   \n",
       "  0.9859013  0.9732563  0.01409869    0.06341260   \n",
       "  0.9870522  0.9734703  0.01294778    0.06591573   \n",
       "  0.9871961  0.9748609  0.01280391    0.06090947   \n",
       "  0.9871961  0.9748609  0.01280391    0.06090947   \n",
       "  0.9871961  0.9748609  0.01280391    0.06090947   \n",
       "  0.9835995  0.9709029  0.01640052    0.06591573   \n",
       "  0.9835995  0.9709029  0.01640052    0.06591573   \n",
       "  0.9835995  0.9709029  0.01640052    0.06591573   \n",
       "  0.9840311  0.9692982  0.01596893    0.07342511   \n",
       "  0.9840311  0.9692982  0.01596893    0.07342511   \n",
       "  0.9840311  0.9692982  0.01596893    0.07342511   \n",
       "  0.9805783  0.9612751  0.01942167    0.09470171   \n",
       "  0.9805783  0.9612751  0.01942167    0.09470171   \n",
       "  0.9805783  0.9612751  0.01942167    0.09470171   \n",
       "  0.9810099  0.9628798  0.01899007    0.08969545   \n",
       "  0.9810099  0.9628798  0.01899007    0.08969545   \n",
       "  0.9810099  0.9628798  0.01899007    0.08969545   \n",
       "  0.9800029  0.9626658  0.01999712    0.08760951   \n",
       "  0.9800029  0.9626658  0.01999712    0.08760951   \n",
       "  0.9800029  0.9626658  0.01999712    0.08760951   \n",
       "  0.9883470  0.9775353  0.01165300    0.05381727   \n",
       "  0.9869084  0.9763586  0.01309164    0.05423446   \n",
       "  0.9869084  0.9765725  0.01309164    0.05340008   \n",
       "  0.9860452  0.9751819  0.01395483    0.05632040   \n",
       "  0.9857574  0.9744330  0.01424256    0.05840634   \n",
       "  0.9857574  0.9749679  0.01424256    0.05632040   \n",
       "  0.9854697  0.9746470  0.01453028    0.05673759   \n",
       "  0.9856136  0.9747540  0.01438642    0.05673759   \n",
       "  0.9857574  0.9740051  0.01424256    0.06007509   \n",
       "  0.9853259  0.9755028  0.01467415    0.05298290   \n",
       "  0.9853259  0.9755028  0.01467415    0.05298290   \n",
       "  0.9853259  0.9755028  0.01467415    0.05298290   \n",
       "  0.9857574  0.9745400  0.01424256    0.05798915   \n",
       "  0.9857574  0.9745400  0.01424256    0.05798915   \n",
       "  0.9857574  0.9745400  0.01424256    0.05798915   \n",
       "  0.9833118  0.9702610  0.01668825    0.06758448   \n",
       "  0.9833118  0.9702610  0.01668825    0.06758448   \n",
       "  0.9833118  0.9702610  0.01668825    0.06758448   \n",
       "  0.9824486  0.9665169  0.01755143    0.07968294   \n",
       "  0.9824486  0.9665169  0.01755143    0.07968294   \n",
       "  0.9824486  0.9665169  0.01755143    0.07968294   \n",
       "  0.9812977  0.9636286  0.01870234    0.08760951   \n",
       "  0.9812977  0.9636286  0.01870234    0.08760951   \n",
       "  0.9812977  0.9636286  0.01870234    0.08760951   \n",
       "  0.9812977  0.9639495  0.01870234    0.08635795   \n",
       "  0.9812977  0.9639495  0.01870234    0.08635795   \n",
       "  0.9812977  0.9639495  0.01870234    0.08635795   \n",
       "  0.9874838  0.9773214  0.01251618    0.05214852   \n",
       "  0.9871961  0.9773214  0.01280391    0.05131414   \n",
       "  0.9870522  0.9772144  0.01294778    0.05131414   \n",
       "  0.9861890  0.9756098  0.01381096    0.05506884   \n",
       "  0.9864768  0.9755028  0.01352323    0.05632040   \n",
       "  0.9857574  0.9750749  0.01424256    0.05590321   \n",
       "  0.9847504  0.9726145  0.01524960    0.06257822   \n",
       "  0.9857574  0.9735772  0.01424256    0.06174385   \n",
       "  0.9864768  0.9738982  0.01352323    0.06257822   \n",
       "  0.9873400  0.9768935  0.01266005    0.05340008   \n",
       "  0.9873400  0.9768935  0.01266005    0.05340008   \n",
       "  0.9873400  0.9768935  0.01266005    0.05340008   \n",
       "  0.9864768  0.9746470  0.01352323    0.05965791   \n",
       "  0.9864768  0.9746470  0.01352323    0.05965791   \n",
       "  0.9864768  0.9746470  0.01352323    0.05965791   \n",
       "  0.9847504  0.9724005  0.01524960    0.06341260   \n",
       "  0.9847504  0.9724005  0.01524960    0.06341260   \n",
       "  0.9847504  0.9724005  0.01524960    0.06341260   \n",
       "  0.9817292  0.9671588  0.01827075    0.07509387   \n",
       "  0.9817292  0.9671588  0.01827075    0.07509387   \n",
       "  0.9817292  0.9671588  0.01827075    0.07509387   \n",
       "  0.9820170  0.9672657  0.01798302    0.07551106   \n",
       "  0.9820170  0.9672657  0.01798302    0.07551106   \n",
       "  0.9820170  0.9672657  0.01798302    0.07551106   \n",
       "  0.9817292  0.9653402  0.01827075    0.08218607   \n",
       "  0.9817292  0.9653402  0.01827075    0.08218607   \n",
       "  0.9817292  0.9653402  0.01827075    0.08218607   \n",
       "  0.9863329  0.9772144  0.01366710    0.04922820   \n",
       "  0.9857574  0.9767865  0.01424256    0.04922820   \n",
       "  0.9857574  0.9766795  0.01424256    0.04964539   \n",
       "  0.9864768  0.9752888  0.01352323    0.05715478   \n",
       "  0.9864768  0.9751819  0.01352323    0.05757196   \n",
       "  0.9857574  0.9745400  0.01424256    0.05798915   \n",
       "  0.9853259  0.9741121  0.01467415    0.05840634   \n",
       "  0.9860452  0.9741121  0.01395483    0.06049228   \n",
       "  0.9856136  0.9740051  0.01438642    0.05965791   \n",
       "  0.9876277  0.9764656  0.01237232    0.05590321   \n",
       "  0.9876277  0.9764656  0.01237232    0.05590321   \n",
       "  0.9876277  0.9764656  0.01237232    0.05590321   \n",
       "  0.9854697  0.9734703  0.01453028    0.06132666   \n",
       "  0.9854697  0.9734703  0.01453028    0.06132666   \n",
       "  0.9854697  0.9734703  0.01453028    0.06132666   \n",
       "  0.9856136  0.9728284  0.01438642    0.06424698   \n",
       "  0.9856136  0.9728284  0.01438642    0.06424698   \n",
       "  0.9856136  0.9728284  0.01438642    0.06424698   \n",
       "  0.9805783  0.9670518  0.01942167    0.07217355   \n",
       "  0.9805783  0.9670518  0.01942167    0.07217355   \n",
       "  0.9805783  0.9670518  0.01942167    0.07217355   \n",
       "  0.9820170  0.9679076  0.01798302    0.07300793   \n",
       "  0.9820170  0.9679076  0.01798302    0.07300793   \n",
       "  0.9820170  0.9679076  0.01798302    0.07300793   \n",
       "  0.9811538  0.9675866  0.01884621    0.07175636   \n",
       "  0.9811538  0.9675866  0.01884621    0.07175636   \n",
       "  0.9811538  0.9675866  0.01884621    0.07175636   \n",
       "\n",
       "Tuning parameter 'colsample_bytree' was held constant at a value of 1\n",
       "\n",
       "Tuning parameter 'subsample' was held constant at a value of 1\n",
       "F1 was used to select the optimal model using the largest value.\n",
       "The final values used for the model were nrounds = 200, max_depth = 11, eta\n",
       " = 0.1, gamma = 0, colsample_bytree = 1, min_child_weight = 1 and subsample = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(xgboost)\n",
    "xgb_grid<-expand.grid(nrounds = c(100,150,200), max_depth = c(7,9,11,13), \n",
    "                      eta = c(0.01,0.1,0.3), gamma=c(0,1,3), \n",
    "                      min_child_weight=c(1,3,5),\n",
    "                      colsample_bytree=1,  subsample=1)\n",
    "\n",
    "train_control<-trainControl(method=\"cv\", number=3, savePredictions = 'final',summaryFunction = f1)\n",
    "model_xgb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control,method='xgbTree',tuneGrid = xgb_grid,\n",
    "                       metric = 'F1')\n",
    "model_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost w/ Additional LLR Feature <a name=\"xgllr\"></a>\n",
    "\n",
    "\n",
    "[Back to top](#method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Fold1</dt>\n",
       "\t\t<dd>6232</dd>\n",
       "\t<dt>Fold2</dt>\n",
       "\t\t<dd>6232</dd>\n",
       "\t<dt>Fold3</dt>\n",
       "\t\t<dd>6232</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Fold1] 6232\n",
       "\\item[Fold2] 6232\n",
       "\\item[Fold3] 6232\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Fold1\n",
       ":   6232Fold2\n",
       ":   6232Fold3\n",
       ":   6232\n",
       "\n"
      ],
      "text/plain": [
       "Fold1 Fold2 Fold3 \n",
       " 6232  6232  6232 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create k-fold index\n",
    "set.seed(418910)\n",
    "cv_folds <- createFolds(as.factor(emailDFnum$isSpam), k=3, list=TRUE, returnTrain = TRUE)\n",
    "lengths(cv_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "9348"
      ],
      "text/latex": [
       "9348"
      ],
      "text/markdown": [
       "9348"
      ],
      "text/plain": [
       "[1] 9348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized words from email body text\n",
    "length(msgWordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>9348</li>\n",
       "\t<li>30</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 9348\n",
       "\\item 30\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 9348\n",
       "2. 30\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 9348   30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 29 features from email metadata and simple text mining\n",
    "emailDFnum0 = setupRnum(emailDF)\n",
    "\n",
    "dim(emailDFnum0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-error:0.004653 \n",
      "[2]\ttrain-error:0.005616 \n",
      "[3]\ttrain-error:0.004172 \n",
      "[4]\ttrain-error:0.004172 \n",
      "[5]\ttrain-error:0.004172 \n",
      "[6]\ttrain-error:0.004172 \n",
      "[7]\ttrain-error:0.004172 \n",
      "[8]\ttrain-error:0.003851 \n",
      "[9]\ttrain-error:0.003851 \n",
      "[10]\ttrain-error:0.003851 \n",
      "[11]\ttrain-error:0.003370 \n",
      "[12]\ttrain-error:0.003049 \n",
      "[13]\ttrain-error:0.003049 \n",
      "[14]\ttrain-error:0.002728 \n",
      "[15]\ttrain-error:0.002728 \n",
      "[16]\ttrain-error:0.001926 \n",
      "[17]\ttrain-error:0.002246 \n",
      "[18]\ttrain-error:0.001605 \n",
      "[19]\ttrain-error:0.001605 \n",
      "[20]\ttrain-error:0.001605 \n",
      "[21]\ttrain-error:0.001444 \n",
      "[22]\ttrain-error:0.001444 \n",
      "[23]\ttrain-error:0.001284 \n",
      "[24]\ttrain-error:0.001284 \n",
      "[25]\ttrain-error:0.000642 \n",
      "[26]\ttrain-error:0.000642 \n",
      "[27]\ttrain-error:0.000642 \n",
      "[28]\ttrain-error:0.000642 \n",
      "[29]\ttrain-error:0.000642 \n",
      "[30]\ttrain-error:0.000642 \n",
      "[31]\ttrain-error:0.000642 \n",
      "[32]\ttrain-error:0.000642 \n",
      "[33]\ttrain-error:0.000642 \n",
      "[34]\ttrain-error:0.000642 \n",
      "[35]\ttrain-error:0.000642 \n",
      "[36]\ttrain-error:0.000642 \n",
      "[37]\ttrain-error:0.000642 \n",
      "[38]\ttrain-error:0.000642 \n",
      "[39]\ttrain-error:0.000642 \n",
      "[40]\ttrain-error:0.000642 \n",
      "[41]\ttrain-error:0.000642 \n",
      "[42]\ttrain-error:0.000642 \n",
      "[43]\ttrain-error:0.000481 \n",
      "[44]\ttrain-error:0.000481 \n",
      "[45]\ttrain-error:0.000321 \n",
      "[46]\ttrain-error:0.000321 \n",
      "[47]\ttrain-error:0.000321 \n",
      "[48]\ttrain-error:0.000321 \n",
      "[49]\ttrain-error:0.000321 \n",
      "[50]\ttrain-error:0.000321 \n",
      "[51]\ttrain-error:0.000321 \n",
      "[52]\ttrain-error:0.000321 \n",
      "[53]\ttrain-error:0.000321 \n",
      "[54]\ttrain-error:0.000321 \n",
      "[55]\ttrain-error:0.000321 \n",
      "[56]\ttrain-error:0.000321 \n",
      "[57]\ttrain-error:0.000321 \n",
      "[58]\ttrain-error:0.000321 \n",
      "[59]\ttrain-error:0.000321 \n",
      "[60]\ttrain-error:0.000321 \n",
      "[61]\ttrain-error:0.000321 \n",
      "[62]\ttrain-error:0.000321 \n",
      "[63]\ttrain-error:0.000321 \n",
      "[64]\ttrain-error:0.000321 \n",
      "[65]\ttrain-error:0.000321 \n",
      "[66]\ttrain-error:0.000321 \n",
      "[67]\ttrain-error:0.000321 \n",
      "[68]\ttrain-error:0.000321 \n",
      "[69]\ttrain-error:0.000321 \n",
      "[70]\ttrain-error:0.000321 \n",
      "[71]\ttrain-error:0.000321 \n",
      "[72]\ttrain-error:0.000321 \n",
      "[73]\ttrain-error:0.000321 \n",
      "[74]\ttrain-error:0.000321 \n",
      "[75]\ttrain-error:0.000321 \n",
      "[76]\ttrain-error:0.000321 \n",
      "[77]\ttrain-error:0.000160 \n",
      "[78]\ttrain-error:0.000160 \n",
      "[79]\ttrain-error:0.000321 \n",
      "[80]\ttrain-error:0.000160 \n",
      "[81]\ttrain-error:0.000321 \n",
      "[82]\ttrain-error:0.000160 \n",
      "[83]\ttrain-error:0.000160 \n",
      "[84]\ttrain-error:0.000160 \n",
      "[85]\ttrain-error:0.000160 \n",
      "[86]\ttrain-error:0.000160 \n",
      "[87]\ttrain-error:0.000160 \n",
      "[88]\ttrain-error:0.000160 \n",
      "[89]\ttrain-error:0.000160 \n",
      "[90]\ttrain-error:0.000160 \n",
      "[91]\ttrain-error:0.000160 \n",
      "[92]\ttrain-error:0.000160 \n",
      "[93]\ttrain-error:0.000160 \n",
      "[94]\ttrain-error:0.000160 \n",
      "[95]\ttrain-error:0.000160 \n",
      "[96]\ttrain-error:0.000160 \n",
      "[97]\ttrain-error:0.000160 \n",
      "[98]\ttrain-error:0.000000 \n",
      "[99]\ttrain-error:0.000000 \n",
      "[100]\ttrain-error:0.000000 \n",
      "[101]\ttrain-error:0.000000 \n",
      "[102]\ttrain-error:0.000000 \n",
      "[103]\ttrain-error:0.000000 \n",
      "[104]\ttrain-error:0.000000 \n",
      "[105]\ttrain-error:0.000000 \n",
      "[106]\ttrain-error:0.000000 \n",
      "[107]\ttrain-error:0.000000 \n",
      "[108]\ttrain-error:0.000000 \n",
      "[109]\ttrain-error:0.000000 \n",
      "[110]\ttrain-error:0.000000 \n",
      "[111]\ttrain-error:0.000000 \n",
      "[112]\ttrain-error:0.000000 \n",
      "[113]\ttrain-error:0.000000 \n",
      "[114]\ttrain-error:0.000000 \n",
      "[115]\ttrain-error:0.000000 \n",
      "[116]\ttrain-error:0.000000 \n",
      "[117]\ttrain-error:0.000000 \n",
      "[118]\ttrain-error:0.000000 \n",
      "[119]\ttrain-error:0.000000 \n",
      "[120]\ttrain-error:0.000000 \n",
      "[121]\ttrain-error:0.000000 \n",
      "[122]\ttrain-error:0.000000 \n",
      "[123]\ttrain-error:0.000000 \n",
      "[124]\ttrain-error:0.000000 \n",
      "[125]\ttrain-error:0.000000 \n",
      "[126]\ttrain-error:0.000000 \n",
      "[127]\ttrain-error:0.000000 \n",
      "[128]\ttrain-error:0.000000 \n",
      "[129]\ttrain-error:0.000000 \n",
      "[130]\ttrain-error:0.000000 \n",
      "[131]\ttrain-error:0.000000 \n",
      "[132]\ttrain-error:0.000000 \n",
      "[133]\ttrain-error:0.000000 \n",
      "[134]\ttrain-error:0.000000 \n",
      "[135]\ttrain-error:0.000000 \n",
      "[136]\ttrain-error:0.000000 \n",
      "[137]\ttrain-error:0.000000 \n",
      "[138]\ttrain-error:0.000000 \n",
      "[139]\ttrain-error:0.000000 \n",
      "[140]\ttrain-error:0.000000 \n",
      "[141]\ttrain-error:0.000000 \n",
      "[142]\ttrain-error:0.000000 \n",
      "[143]\ttrain-error:0.000000 \n",
      "[144]\ttrain-error:0.000000 \n",
      "[145]\ttrain-error:0.000000 \n",
      "[146]\ttrain-error:0.000000 \n",
      "[147]\ttrain-error:0.000000 \n",
      "[148]\ttrain-error:0.000000 \n",
      "[149]\ttrain-error:0.000000 \n",
      "[150]\ttrain-error:0.000000 \n",
      "[151]\ttrain-error:0.000000 \n",
      "[152]\ttrain-error:0.000000 \n",
      "[153]\ttrain-error:0.000000 \n",
      "[154]\ttrain-error:0.000000 \n",
      "[155]\ttrain-error:0.000000 \n",
      "[156]\ttrain-error:0.000000 \n",
      "[157]\ttrain-error:0.000000 \n",
      "[158]\ttrain-error:0.000000 \n",
      "[159]\ttrain-error:0.000000 \n",
      "[160]\ttrain-error:0.000000 \n",
      "[161]\ttrain-error:0.000000 \n",
      "[162]\ttrain-error:0.000000 \n",
      "[163]\ttrain-error:0.000000 \n",
      "[164]\ttrain-error:0.000000 \n",
      "[165]\ttrain-error:0.000000 \n",
      "[166]\ttrain-error:0.000000 \n",
      "[167]\ttrain-error:0.000000 \n",
      "[168]\ttrain-error:0.000000 \n",
      "[169]\ttrain-error:0.000000 \n",
      "[170]\ttrain-error:0.000000 \n",
      "[171]\ttrain-error:0.000000 \n",
      "[172]\ttrain-error:0.000000 \n",
      "[173]\ttrain-error:0.000000 \n",
      "[174]\ttrain-error:0.000000 \n",
      "[175]\ttrain-error:0.000000 \n",
      "[176]\ttrain-error:0.000000 \n",
      "[177]\ttrain-error:0.000000 \n",
      "[178]\ttrain-error:0.000000 \n",
      "[179]\ttrain-error:0.000000 \n",
      "[180]\ttrain-error:0.000000 \n",
      "[181]\ttrain-error:0.000000 \n",
      "[182]\ttrain-error:0.000000 \n",
      "[183]\ttrain-error:0.000000 \n",
      "[184]\ttrain-error:0.000000 \n",
      "[185]\ttrain-error:0.000000 \n",
      "[186]\ttrain-error:0.000000 \n",
      "[187]\ttrain-error:0.000000 \n",
      "[188]\ttrain-error:0.000000 \n",
      "[189]\ttrain-error:0.000000 \n",
      "[190]\ttrain-error:0.000000 \n",
      "[191]\ttrain-error:0.000000 \n",
      "[192]\ttrain-error:0.000000 \n",
      "[193]\ttrain-error:0.000000 \n",
      "[194]\ttrain-error:0.000000 \n",
      "[195]\ttrain-error:0.000000 \n",
      "[196]\ttrain-error:0.000000 \n",
      "[197]\ttrain-error:0.000000 \n",
      "[198]\ttrain-error:0.000000 \n",
      "[199]\ttrain-error:0.000000 \n",
      "[200]\ttrain-error:0.000000 \n",
      "[1]\ttrain-error:0.002728 \n",
      "[2]\ttrain-error:0.003370 \n",
      "[3]\ttrain-error:0.001926 \n",
      "[4]\ttrain-error:0.001444 \n",
      "[5]\ttrain-error:0.001926 \n",
      "[6]\ttrain-error:0.001605 \n",
      "[7]\ttrain-error:0.001444 \n",
      "[8]\ttrain-error:0.001444 \n",
      "[9]\ttrain-error:0.001444 \n",
      "[10]\ttrain-error:0.001284 \n",
      "[11]\ttrain-error:0.001444 \n",
      "[12]\ttrain-error:0.001284 \n",
      "[13]\ttrain-error:0.001284 \n",
      "[14]\ttrain-error:0.001284 \n",
      "[15]\ttrain-error:0.001284 \n",
      "[16]\ttrain-error:0.001284 \n",
      "[17]\ttrain-error:0.001284 \n",
      "[18]\ttrain-error:0.001284 \n",
      "[19]\ttrain-error:0.001284 \n",
      "[20]\ttrain-error:0.001444 \n",
      "[21]\ttrain-error:0.001444 \n",
      "[22]\ttrain-error:0.001444 \n",
      "[23]\ttrain-error:0.001444 \n",
      "[24]\ttrain-error:0.001284 \n",
      "[25]\ttrain-error:0.001284 \n",
      "[26]\ttrain-error:0.001284 \n",
      "[27]\ttrain-error:0.001284 \n",
      "[28]\ttrain-error:0.000642 \n",
      "[29]\ttrain-error:0.000642 \n",
      "[30]\ttrain-error:0.000642 \n",
      "[31]\ttrain-error:0.000642 \n",
      "[32]\ttrain-error:0.000642 \n",
      "[33]\ttrain-error:0.000481 \n",
      "[34]\ttrain-error:0.000321 \n",
      "[35]\ttrain-error:0.000321 \n",
      "[36]\ttrain-error:0.000321 \n",
      "[37]\ttrain-error:0.000321 \n",
      "[38]\ttrain-error:0.000321 \n",
      "[39]\ttrain-error:0.000321 \n",
      "[40]\ttrain-error:0.000321 \n",
      "[41]\ttrain-error:0.000321 \n",
      "[42]\ttrain-error:0.000321 \n",
      "[43]\ttrain-error:0.000321 \n",
      "[44]\ttrain-error:0.000321 \n",
      "[45]\ttrain-error:0.000321 \n",
      "[46]\ttrain-error:0.000321 \n",
      "[47]\ttrain-error:0.000321 \n",
      "[48]\ttrain-error:0.000321 \n",
      "[49]\ttrain-error:0.000321 \n",
      "[50]\ttrain-error:0.000321 \n",
      "[51]\ttrain-error:0.000321 \n",
      "[52]\ttrain-error:0.000321 \n",
      "[53]\ttrain-error:0.000321 \n",
      "[54]\ttrain-error:0.000321 \n",
      "[55]\ttrain-error:0.000321 \n",
      "[56]\ttrain-error:0.000160 \n",
      "[57]\ttrain-error:0.000160 \n",
      "[58]\ttrain-error:0.000160 \n",
      "[59]\ttrain-error:0.000160 \n",
      "[60]\ttrain-error:0.000160 \n",
      "[61]\ttrain-error:0.000160 \n",
      "[62]\ttrain-error:0.000160 \n",
      "[63]\ttrain-error:0.000160 \n",
      "[64]\ttrain-error:0.000160 \n",
      "[65]\ttrain-error:0.000160 \n",
      "[66]\ttrain-error:0.000160 \n",
      "[67]\ttrain-error:0.000160 \n",
      "[68]\ttrain-error:0.000160 \n",
      "[69]\ttrain-error:0.000160 \n",
      "[70]\ttrain-error:0.000160 \n",
      "[71]\ttrain-error:0.000160 \n",
      "[72]\ttrain-error:0.000160 \n",
      "[73]\ttrain-error:0.000160 \n",
      "[74]\ttrain-error:0.000160 \n",
      "[75]\ttrain-error:0.000160 \n",
      "[76]\ttrain-error:0.000160 \n",
      "[77]\ttrain-error:0.000160 \n",
      "[78]\ttrain-error:0.000160 \n",
      "[79]\ttrain-error:0.000160 \n",
      "[80]\ttrain-error:0.000160 \n",
      "[81]\ttrain-error:0.000160 \n",
      "[82]\ttrain-error:0.000160 \n",
      "[83]\ttrain-error:0.000160 \n",
      "[84]\ttrain-error:0.000160 \n",
      "[85]\ttrain-error:0.000160 \n",
      "[86]\ttrain-error:0.000160 \n",
      "[87]\ttrain-error:0.000160 \n",
      "[88]\ttrain-error:0.000160 \n",
      "[89]\ttrain-error:0.000160 \n",
      "[90]\ttrain-error:0.000160 \n",
      "[91]\ttrain-error:0.000160 \n",
      "[92]\ttrain-error:0.000160 \n",
      "[93]\ttrain-error:0.000160 \n",
      "[94]\ttrain-error:0.000160 \n",
      "[95]\ttrain-error:0.000160 \n",
      "[96]\ttrain-error:0.000160 \n",
      "[97]\ttrain-error:0.000160 \n",
      "[98]\ttrain-error:0.000160 \n",
      "[99]\ttrain-error:0.000160 \n",
      "[100]\ttrain-error:0.000160 \n",
      "[101]\ttrain-error:0.000160 \n",
      "[102]\ttrain-error:0.000160 \n",
      "[103]\ttrain-error:0.000160 \n",
      "[104]\ttrain-error:0.000160 \n",
      "[105]\ttrain-error:0.000000 \n",
      "[106]\ttrain-error:0.000160 \n",
      "[107]\ttrain-error:0.000160 \n",
      "[108]\ttrain-error:0.000160 \n",
      "[109]\ttrain-error:0.000160 \n",
      "[110]\ttrain-error:0.000000 \n",
      "[111]\ttrain-error:0.000000 \n",
      "[112]\ttrain-error:0.000000 \n",
      "[113]\ttrain-error:0.000000 \n",
      "[114]\ttrain-error:0.000000 \n",
      "[115]\ttrain-error:0.000000 \n",
      "[116]\ttrain-error:0.000000 \n",
      "[117]\ttrain-error:0.000000 \n",
      "[118]\ttrain-error:0.000000 \n",
      "[119]\ttrain-error:0.000000 \n",
      "[120]\ttrain-error:0.000000 \n",
      "[121]\ttrain-error:0.000000 \n",
      "[122]\ttrain-error:0.000000 \n",
      "[123]\ttrain-error:0.000000 \n",
      "[124]\ttrain-error:0.000000 \n",
      "[125]\ttrain-error:0.000000 \n",
      "[126]\ttrain-error:0.000000 \n",
      "[127]\ttrain-error:0.000000 \n",
      "[128]\ttrain-error:0.000000 \n",
      "[129]\ttrain-error:0.000000 \n",
      "[130]\ttrain-error:0.000000 \n",
      "[131]\ttrain-error:0.000000 \n",
      "[132]\ttrain-error:0.000000 \n",
      "[133]\ttrain-error:0.000000 \n",
      "[134]\ttrain-error:0.000000 \n",
      "[135]\ttrain-error:0.000000 \n",
      "[136]\ttrain-error:0.000000 \n",
      "[137]\ttrain-error:0.000000 \n",
      "[138]\ttrain-error:0.000000 \n",
      "[139]\ttrain-error:0.000000 \n",
      "[140]\ttrain-error:0.000000 \n",
      "[141]\ttrain-error:0.000000 \n",
      "[142]\ttrain-error:0.000000 \n",
      "[143]\ttrain-error:0.000000 \n",
      "[144]\ttrain-error:0.000000 \n",
      "[145]\ttrain-error:0.000000 \n",
      "[146]\ttrain-error:0.000000 \n",
      "[147]\ttrain-error:0.000000 \n",
      "[148]\ttrain-error:0.000000 \n",
      "[149]\ttrain-error:0.000000 \n",
      "[150]\ttrain-error:0.000000 \n",
      "[151]\ttrain-error:0.000000 \n",
      "[152]\ttrain-error:0.000000 \n",
      "[153]\ttrain-error:0.000000 \n",
      "[154]\ttrain-error:0.000000 \n",
      "[155]\ttrain-error:0.000000 \n",
      "[156]\ttrain-error:0.000000 \n",
      "[157]\ttrain-error:0.000000 \n",
      "[158]\ttrain-error:0.000000 \n",
      "[159]\ttrain-error:0.000000 \n",
      "[160]\ttrain-error:0.000000 \n",
      "[161]\ttrain-error:0.000000 \n",
      "[162]\ttrain-error:0.000000 \n",
      "[163]\ttrain-error:0.000000 \n",
      "[164]\ttrain-error:0.000000 \n",
      "[165]\ttrain-error:0.000000 \n",
      "[166]\ttrain-error:0.000000 \n",
      "[167]\ttrain-error:0.000000 \n",
      "[168]\ttrain-error:0.000000 \n",
      "[169]\ttrain-error:0.000000 \n",
      "[170]\ttrain-error:0.000000 \n",
      "[171]\ttrain-error:0.000000 \n",
      "[172]\ttrain-error:0.000000 \n",
      "[173]\ttrain-error:0.000000 \n",
      "[174]\ttrain-error:0.000000 \n",
      "[175]\ttrain-error:0.000000 \n",
      "[176]\ttrain-error:0.000000 \n",
      "[177]\ttrain-error:0.000000 \n",
      "[178]\ttrain-error:0.000000 \n",
      "[179]\ttrain-error:0.000000 \n",
      "[180]\ttrain-error:0.000000 \n",
      "[181]\ttrain-error:0.000000 \n",
      "[182]\ttrain-error:0.000000 \n",
      "[183]\ttrain-error:0.000000 \n",
      "[184]\ttrain-error:0.000000 \n",
      "[185]\ttrain-error:0.000000 \n",
      "[186]\ttrain-error:0.000000 \n",
      "[187]\ttrain-error:0.000000 \n",
      "[188]\ttrain-error:0.000000 \n",
      "[189]\ttrain-error:0.000000 \n",
      "[190]\ttrain-error:0.000000 \n",
      "[191]\ttrain-error:0.000000 \n",
      "[192]\ttrain-error:0.000000 \n",
      "[193]\ttrain-error:0.000000 \n",
      "[194]\ttrain-error:0.000000 \n",
      "[195]\ttrain-error:0.000000 \n",
      "[196]\ttrain-error:0.000000 \n",
      "[197]\ttrain-error:0.000000 \n",
      "[198]\ttrain-error:0.000000 \n",
      "[199]\ttrain-error:0.000000 \n",
      "[200]\ttrain-error:0.000000 \n",
      "[1]\ttrain-error:0.004172 \n",
      "[2]\ttrain-error:0.004332 \n",
      "[3]\ttrain-error:0.004172 \n",
      "[4]\ttrain-error:0.004172 \n",
      "[5]\ttrain-error:0.004012 \n",
      "[6]\ttrain-error:0.004012 \n",
      "[7]\ttrain-error:0.003530 \n",
      "[8]\ttrain-error:0.003370 \n",
      "[9]\ttrain-error:0.003209 \n",
      "[10]\ttrain-error:0.003209 \n",
      "[11]\ttrain-error:0.003209 \n",
      "[12]\ttrain-error:0.003209 \n",
      "[13]\ttrain-error:0.002888 \n",
      "[14]\ttrain-error:0.002567 \n",
      "[15]\ttrain-error:0.002567 \n",
      "[16]\ttrain-error:0.002407 \n",
      "[17]\ttrain-error:0.002246 \n",
      "[18]\ttrain-error:0.002246 \n",
      "[19]\ttrain-error:0.002246 \n",
      "[20]\ttrain-error:0.001926 \n",
      "[21]\ttrain-error:0.001926 \n",
      "[22]\ttrain-error:0.001444 \n",
      "[23]\ttrain-error:0.001284 \n",
      "[24]\ttrain-error:0.001284 \n",
      "[25]\ttrain-error:0.001123 \n",
      "[26]\ttrain-error:0.000802 \n",
      "[27]\ttrain-error:0.000802 \n",
      "[28]\ttrain-error:0.000802 \n",
      "[29]\ttrain-error:0.000802 \n",
      "[30]\ttrain-error:0.000802 \n",
      "[31]\ttrain-error:0.000642 \n",
      "[32]\ttrain-error:0.000642 \n",
      "[33]\ttrain-error:0.000642 \n",
      "[34]\ttrain-error:0.000642 \n",
      "[35]\ttrain-error:0.000642 \n",
      "[36]\ttrain-error:0.000642 \n",
      "[37]\ttrain-error:0.000642 \n",
      "[38]\ttrain-error:0.000321 \n",
      "[39]\ttrain-error:0.000321 \n",
      "[40]\ttrain-error:0.000321 \n",
      "[41]\ttrain-error:0.000321 \n",
      "[42]\ttrain-error:0.000321 \n",
      "[43]\ttrain-error:0.000321 \n",
      "[44]\ttrain-error:0.000321 \n",
      "[45]\ttrain-error:0.000321 \n",
      "[46]\ttrain-error:0.000321 \n",
      "[47]\ttrain-error:0.000321 \n",
      "[48]\ttrain-error:0.000321 \n",
      "[49]\ttrain-error:0.000321 \n",
      "[50]\ttrain-error:0.000321 \n",
      "[51]\ttrain-error:0.000321 \n",
      "[52]\ttrain-error:0.000321 \n",
      "[53]\ttrain-error:0.000321 \n",
      "[54]\ttrain-error:0.000321 \n",
      "[55]\ttrain-error:0.000321 \n",
      "[56]\ttrain-error:0.000321 \n",
      "[57]\ttrain-error:0.000321 \n",
      "[58]\ttrain-error:0.000321 \n",
      "[59]\ttrain-error:0.000321 \n",
      "[60]\ttrain-error:0.000321 \n",
      "[61]\ttrain-error:0.000321 \n",
      "[62]\ttrain-error:0.000321 \n",
      "[63]\ttrain-error:0.000321 \n",
      "[64]\ttrain-error:0.000321 \n",
      "[65]\ttrain-error:0.000321 \n",
      "[66]\ttrain-error:0.000160 \n",
      "[67]\ttrain-error:0.000160 \n",
      "[68]\ttrain-error:0.000160 \n",
      "[69]\ttrain-error:0.000160 \n",
      "[70]\ttrain-error:0.000160 \n",
      "[71]\ttrain-error:0.000160 \n",
      "[72]\ttrain-error:0.000160 \n",
      "[73]\ttrain-error:0.000160 \n",
      "[74]\ttrain-error:0.000160 \n",
      "[75]\ttrain-error:0.000000 \n",
      "[76]\ttrain-error:0.000000 \n",
      "[77]\ttrain-error:0.000000 \n",
      "[78]\ttrain-error:0.000000 \n",
      "[79]\ttrain-error:0.000000 \n",
      "[80]\ttrain-error:0.000000 \n",
      "[81]\ttrain-error:0.000000 \n",
      "[82]\ttrain-error:0.000000 \n",
      "[83]\ttrain-error:0.000000 \n",
      "[84]\ttrain-error:0.000000 \n",
      "[85]\ttrain-error:0.000000 \n",
      "[86]\ttrain-error:0.000000 \n",
      "[87]\ttrain-error:0.000000 \n",
      "[88]\ttrain-error:0.000000 \n",
      "[89]\ttrain-error:0.000000 \n",
      "[90]\ttrain-error:0.000000 \n",
      "[91]\ttrain-error:0.000000 \n",
      "[92]\ttrain-error:0.000000 \n",
      "[93]\ttrain-error:0.000000 \n",
      "[94]\ttrain-error:0.000000 \n",
      "[95]\ttrain-error:0.000000 \n",
      "[96]\ttrain-error:0.000000 \n",
      "[97]\ttrain-error:0.000000 \n",
      "[98]\ttrain-error:0.000000 \n",
      "[99]\ttrain-error:0.000000 \n",
      "[100]\ttrain-error:0.000000 \n",
      "[101]\ttrain-error:0.000000 \n",
      "[102]\ttrain-error:0.000000 \n",
      "[103]\ttrain-error:0.000000 \n",
      "[104]\ttrain-error:0.000000 \n",
      "[105]\ttrain-error:0.000000 \n",
      "[106]\ttrain-error:0.000000 \n",
      "[107]\ttrain-error:0.000000 \n",
      "[108]\ttrain-error:0.000000 \n",
      "[109]\ttrain-error:0.000000 \n",
      "[110]\ttrain-error:0.000000 \n",
      "[111]\ttrain-error:0.000000 \n",
      "[112]\ttrain-error:0.000000 \n",
      "[113]\ttrain-error:0.000000 \n",
      "[114]\ttrain-error:0.000000 \n",
      "[115]\ttrain-error:0.000000 \n",
      "[116]\ttrain-error:0.000000 \n",
      "[117]\ttrain-error:0.000000 \n",
      "[118]\ttrain-error:0.000000 \n",
      "[119]\ttrain-error:0.000000 \n",
      "[120]\ttrain-error:0.000000 \n",
      "[121]\ttrain-error:0.000000 \n",
      "[122]\ttrain-error:0.000000 \n",
      "[123]\ttrain-error:0.000000 \n",
      "[124]\ttrain-error:0.000000 \n",
      "[125]\ttrain-error:0.000000 \n",
      "[126]\ttrain-error:0.000000 \n",
      "[127]\ttrain-error:0.000000 \n",
      "[128]\ttrain-error:0.000000 \n",
      "[129]\ttrain-error:0.000000 \n",
      "[130]\ttrain-error:0.000000 \n",
      "[131]\ttrain-error:0.000000 \n",
      "[132]\ttrain-error:0.000000 \n",
      "[133]\ttrain-error:0.000000 \n",
      "[134]\ttrain-error:0.000000 \n",
      "[135]\ttrain-error:0.000000 \n",
      "[136]\ttrain-error:0.000000 \n",
      "[137]\ttrain-error:0.000000 \n",
      "[138]\ttrain-error:0.000000 \n",
      "[139]\ttrain-error:0.000000 \n",
      "[140]\ttrain-error:0.000000 \n",
      "[141]\ttrain-error:0.000000 \n",
      "[142]\ttrain-error:0.000000 \n",
      "[143]\ttrain-error:0.000000 \n",
      "[144]\ttrain-error:0.000000 \n",
      "[145]\ttrain-error:0.000000 \n",
      "[146]\ttrain-error:0.000000 \n",
      "[147]\ttrain-error:0.000000 \n",
      "[148]\ttrain-error:0.000000 \n",
      "[149]\ttrain-error:0.000000 \n",
      "[150]\ttrain-error:0.000000 \n",
      "[151]\ttrain-error:0.000000 \n",
      "[152]\ttrain-error:0.000000 \n",
      "[153]\ttrain-error:0.000000 \n",
      "[154]\ttrain-error:0.000000 \n",
      "[155]\ttrain-error:0.000000 \n",
      "[156]\ttrain-error:0.000000 \n",
      "[157]\ttrain-error:0.000000 \n",
      "[158]\ttrain-error:0.000000 \n",
      "[159]\ttrain-error:0.000000 \n",
      "[160]\ttrain-error:0.000000 \n",
      "[161]\ttrain-error:0.000000 \n",
      "[162]\ttrain-error:0.000000 \n",
      "[163]\ttrain-error:0.000000 \n",
      "[164]\ttrain-error:0.000000 \n",
      "[165]\ttrain-error:0.000000 \n",
      "[166]\ttrain-error:0.000000 \n",
      "[167]\ttrain-error:0.000000 \n",
      "[168]\ttrain-error:0.000000 \n",
      "[169]\ttrain-error:0.000000 \n",
      "[170]\ttrain-error:0.000000 \n",
      "[171]\ttrain-error:0.000000 \n",
      "[172]\ttrain-error:0.000000 \n",
      "[173]\ttrain-error:0.000000 \n",
      "[174]\ttrain-error:0.000000 \n",
      "[175]\ttrain-error:0.000000 \n",
      "[176]\ttrain-error:0.000000 \n",
      "[177]\ttrain-error:0.000000 \n",
      "[178]\ttrain-error:0.000000 \n",
      "[179]\ttrain-error:0.000000 \n",
      "[180]\ttrain-error:0.000000 \n",
      "[181]\ttrain-error:0.000000 \n",
      "[182]\ttrain-error:0.000000 \n",
      "[183]\ttrain-error:0.000000 \n",
      "[184]\ttrain-error:0.000000 \n",
      "[185]\ttrain-error:0.000000 \n",
      "[186]\ttrain-error:0.000000 \n",
      "[187]\ttrain-error:0.000000 \n",
      "[188]\ttrain-error:0.000000 \n",
      "[189]\ttrain-error:0.000000 \n",
      "[190]\ttrain-error:0.000000 \n",
      "[191]\ttrain-error:0.000000 \n",
      "[192]\ttrain-error:0.000000 \n",
      "[193]\ttrain-error:0.000000 \n",
      "[194]\ttrain-error:0.000000 \n",
      "[195]\ttrain-error:0.000000 \n",
      "[196]\ttrain-error:0.000000 \n",
      "[197]\ttrain-error:0.000000 \n",
      "[198]\ttrain-error:0.000000 \n",
      "[199]\ttrain-error:0.000000 \n",
      "[200]\ttrain-error:0.000000 \n"
     ]
    }
   ],
   "source": [
    "# 3-fold cross-validation\n",
    "\n",
    "for (i in 1:3){\n",
    "\n",
    "    # email text mining\n",
    "    testMsgWords = msgWordsList[- cv_folds[[i]]]\n",
    "    trainMsgWords = msgWordsList[cv_folds[[i]]]\n",
    "\n",
    "    testIsSpam = isSpam[- cv_folds[[i]]]\n",
    "    trainIsSpam = isSpam[cv_folds[[i]]]\n",
    "\n",
    "    # Log Likelihood Ratio w/ Naive Bayes\n",
    "    trainTable = computeFreqs(trainMsgWords, trainIsSpam)\n",
    "    testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)\n",
    "    trainLLR = sapply(trainMsgWords, computeMsgLLR, trainTable)\n",
    "\n",
    "    # email number features\n",
    "    emailDFnumTest = emailDFnum0[- cv_folds[[i]],]\n",
    "    emailDFnumTrain = emailDFnum0[cv_folds[[i]],]\n",
    "\n",
    "    emailDFnumTest$LLR = testLLR\n",
    "    emailDFnumTrain$LLR = trainLLR\n",
    "\n",
    "    # replace NaNs with zeros\n",
    "    emailDFnumTest[is.na(emailDFnumTest)]<-0\n",
    "    emailDFnumTrain[is.na(emailDFnumTrain)]<-0\n",
    "\n",
    "    # convert list to matrix\n",
    "    emailDFnumTest = matrix(unlist(emailDFnumTest), ncol = 31, byrow=FALSE)\n",
    "    emailDFnumTrain = matrix(unlist(emailDFnumTrain), ncol = 31, byrow=FALSE)\n",
    "\n",
    "    # split label from data\n",
    "    testX = emailDFnumTest[,-1]\n",
    "    testY =  emailDFnumTest[,1]\n",
    "    trainX = emailDFnumTrain[,-1]\n",
    "    trainY =  emailDFnumTrain[,1]\n",
    "\n",
    "    # training\n",
    "    model_xgb <- xgboost(data = trainX, label = trainY, \n",
    "                   max_depth = 11, eta = 0.1,nrounds = 200,\n",
    "                   objective = \"binary:logistic\")\n",
    "\n",
    "\n",
    "    # predict\n",
    "    pred <- predict(model_xgb, testX, objective='binary:logistics')   # prediction\n",
    "    pred_y <- as.numeric(pred > 0.5)                                  # binary conversion\n",
    "\n",
    "\n",
    "    # Output collection from iterations\n",
    "    if (i==1){\n",
    "        Metrics31 = metrics(pred_y, testY, 'NA')\n",
    "        }else{\n",
    "            Metrics31 = rbind(Metrics31, metrics(pred_y, testY, 'NA'))\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-fold cross-validation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Precision</th><th scope=col>Recall</th><th scope=col>F1</th><th scope=col>Accuracy</th><th scope=col>Type_I_Error</th><th scope=col>Type_II_Error</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9924147  </td><td>0.9824781  </td><td>0.9874214  </td><td>0.9935815  </td><td>0.002589555</td><td>0.01752190 </td></tr>\n",
       "\t<tr><td>0.9811794  </td><td>0.9787234  </td><td>0.9799499  </td><td>0.9897304  </td><td>0.006473889</td><td>0.02127660 </td></tr>\n",
       "\t<tr><td>1.0000000  </td><td>0.9737171  </td><td>0.9866836  </td><td>0.9932606  </td><td>0.000000000</td><td>0.02628285 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " Precision & Recall & F1 & Accuracy & Type\\_I\\_Error & Type\\_II\\_Error\\\\\n",
       "\\hline\n",
       "\t 0.9924147   & 0.9824781   & 0.9874214   & 0.9935815   & 0.002589555 & 0.01752190 \\\\\n",
       "\t 0.9811794   & 0.9787234   & 0.9799499   & 0.9897304   & 0.006473889 & 0.02127660 \\\\\n",
       "\t 1.0000000   & 0.9737171   & 0.9866836   & 0.9932606   & 0.000000000 & 0.02628285 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Precision | Recall | F1 | Accuracy | Type_I_Error | Type_II_Error | \n",
       "|---|---|---|\n",
       "| 0.9924147   | 0.9824781   | 0.9874214   | 0.9935815   | 0.002589555 | 0.01752190  | \n",
       "| 0.9811794   | 0.9787234   | 0.9799499   | 0.9897304   | 0.006473889 | 0.02127660  | \n",
       "| 1.0000000   | 0.9737171   | 0.9866836   | 0.9932606   | 0.000000000 | 0.02628285  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Precision Recall    F1        Accuracy  Type_I_Error Type_II_Error\n",
       "1 0.9924147 0.9824781 0.9874214 0.9935815 0.002589555  0.01752190   \n",
       "2 0.9811794 0.9787234 0.9799499 0.9897304 0.006473889  0.02127660   \n",
       "3 1.0000000 0.9737171 0.9866836 0.9932606 0.000000000  0.02628285   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Metrics31[,-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average output metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Mean</th><th scope=col>StDev</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Precision</th><td>0.991</td><td>0.009</td></tr>\n",
       "\t<tr><th scope=row>Recall</th><td>0.978</td><td>0.004</td></tr>\n",
       "\t<tr><th scope=row>F1</th><td>0.985</td><td>0.004</td></tr>\n",
       "\t<tr><th scope=row>Accuracy</th><td>0.992</td><td>0.002</td></tr>\n",
       "\t<tr><th scope=row>Type_I_Error</th><td>0.003</td><td>0.003</td></tr>\n",
       "\t<tr><th scope=row>Type_II_Error</th><td>0.022</td><td>0.004</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Mean & StDev\\\\\n",
       "\\hline\n",
       "\tPrecision & 0.991 & 0.009\\\\\n",
       "\tRecall & 0.978 & 0.004\\\\\n",
       "\tF1 & 0.985 & 0.004\\\\\n",
       "\tAccuracy & 0.992 & 0.002\\\\\n",
       "\tType\\_I\\_Error & 0.003 & 0.003\\\\\n",
       "\tType\\_II\\_Error & 0.022 & 0.004\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Mean | StDev | \n",
       "|---|---|---|---|---|---|\n",
       "| Precision | 0.991 | 0.009 | \n",
       "| Recall | 0.978 | 0.004 | \n",
       "| F1 | 0.985 | 0.004 | \n",
       "| Accuracy | 0.992 | 0.002 | \n",
       "| Type_I_Error | 0.003 | 0.003 | \n",
       "| Type_II_Error | 0.022 | 0.004 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "              Mean  StDev\n",
       "Precision     0.991 0.009\n",
       "Recall        0.978 0.004\n",
       "F1            0.985 0.004\n",
       "Accuracy      0.992 0.002\n",
       "Type_I_Error  0.003 0.003\n",
       "Type_II_Error 0.022 0.004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_metrics31 = cbind(apply(Metrics31[,-7], 2, mean), apply(Metrics31[,-7], 2, sd))\n",
    "colnames(avg_metrics31) = c('Mean','StDev')\n",
    "round(avg_metrics31,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of all models <a name=\"resultAll\"></a>\n",
    "\n",
    "[Back to Result](#Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>F1</th><th scope=col>precision</th><th scope=col>Recall</th><th scope=col>Accuracy</th><th scope=col>Type_I_Error</th><th scope=col>Type_II_Error</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Naïve_BOW</th><td>0.963</td><td>0.972</td><td>0.953</td><td>0.981</td><td>0.009</td><td>0.047</td></tr>\n",
       "\t<tr><th scope=row>CART</th><td>0.960</td><td>0.953</td><td>0.967</td><td>0.940</td><td>0.033</td><td>0.138</td></tr>\n",
       "\t<tr><th scope=row>XGBoost</th><td>0.986</td><td>0.982</td><td>0.989</td><td>0.978</td><td>0.011</td><td>0.054</td></tr>\n",
       "\t<tr><th scope=row>XGBoost_BOW</th><td>0.985</td><td>0.991</td><td>0.978</td><td>0.992</td><td>0.003</td><td>0.022</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & F1 & precision & Recall & Accuracy & Type\\_I\\_Error & Type\\_II\\_Error\\\\\n",
       "\\hline\n",
       "\tNaïve\\_BOW & 0.963 & 0.972 & 0.953 & 0.981 & 0.009 & 0.047\\\\\n",
       "\tCART & 0.960 & 0.953 & 0.967 & 0.940 & 0.033 & 0.138\\\\\n",
       "\tXGBoost & 0.986 & 0.982 & 0.989 & 0.978 & 0.011 & 0.054\\\\\n",
       "\tXGBoost\\_BOW & 0.985 & 0.991 & 0.978 & 0.992 & 0.003 & 0.022\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | F1 | precision | Recall | Accuracy | Type_I_Error | Type_II_Error | \n",
       "|---|---|---|---|\n",
       "| Naïve_BOW | 0.963 | 0.972 | 0.953 | 0.981 | 0.009 | 0.047 | \n",
       "| CART | 0.960 | 0.953 | 0.967 | 0.940 | 0.033 | 0.138 | \n",
       "| XGBoost | 0.986 | 0.982 | 0.989 | 0.978 | 0.011 | 0.054 | \n",
       "| XGBoost_BOW | 0.985 | 0.991 | 0.978 | 0.992 | 0.003 | 0.022 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            F1    precision Recall Accuracy Type_I_Error Type_II_Error\n",
       "Naïve_BOW   0.963 0.972     0.953  0.981    0.009        0.047        \n",
       "CART        0.960 0.953     0.967  0.940    0.033        0.138        \n",
       "XGBoost     0.986 0.982     0.989  0.978    0.011        0.054        \n",
       "XGBoost_BOW 0.985 0.991     0.978  0.992    0.003        0.022        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance = data.frame(\n",
    "    F1 = c(0.963, 0.960, 0.986, 0.985),\n",
    "    precision = c(0.972, 0.953, 0.982, 0.991),\n",
    "    Recall = c(0.953, 0.967, 0.989, 0.978),\n",
    "    Accuracy = c(0.981, 0.940, 0.978, 0.992),\n",
    "    Type_I_Error = c(0.009, 0.033, 0.011, 0.003),\n",
    "    Type_II_Error = c(0.047, 0.138, 0.054, 0.022)\n",
    ")\n",
    "\n",
    "rownames(performance) = c('Naïve_BOW', 'CART', 'XGBoost', 'XGBoost_BOW')\n",
    "performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [conda env:mro]",
   "language": "R",
   "name": "conda-env-mro-r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
