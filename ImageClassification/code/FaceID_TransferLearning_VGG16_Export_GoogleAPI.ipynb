{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning - Export Keras model for Google Cloud ML API\n",
    "\n",
    "\n",
    "8/2/2018\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "\n",
    "\n",
    "**Environment:**\n",
    "\n",
    "- Python 3.6\n",
    "- Keras-gpu 2.2.0\n",
    "- matplotlib 2.2.2\n",
    "- pillow 5.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications import VGG16\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image   # for load_image\n",
    "import tensorflow as tf\n",
    "\n",
    "vgg_conv = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze layers from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000002A48C2DAB00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A4934FD748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A49351CE10> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002A49351CB00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A4934FD9E8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A494503F28> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002A49451EB70> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A494534978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A49455B630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A49454CC50> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002A494581A90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A494591A58> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A4945BE828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A4945AAF28> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002A4945D4F98> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A4945F9470> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A49460C6D8> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002A494627E80> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002A49463A630> True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                11275     \n",
      "=================================================================\n",
      "Total params: 40,417,099\n",
      "Trainable params: 32,781,835\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras import models\n",
    "# from keras import layers\n",
    "# from keras import optimizers\n",
    " \n",
    "targetClassNumber = 11   \n",
    "    \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(targetClassNumber, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 images belonging to 11 classes.\n",
      "Found 22 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = './train'\n",
    "validation_dir = './validation'\n",
    "image_size = 224\n",
    " \n",
    "# nTrain = 600\n",
    "# nVal = 150\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# import numpy as np\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 20\n",
    "val_batchsize = 20\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save image labels\n",
    "\n",
    "### as a pickle file to look up during prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Dictionary : \n",
      " {'Benjamin': 0, 'Damarcus': 1, 'Frank': 2, 'Lokesh': 3, 'Misael': 4, 'Sohail': 5, 'aaron': 6, 'albert': 7, 'brian': 8, 'mooyoung-lee': 9, 'vivek': 10}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Label dictionary of train images\n",
    "class_dictionary = train_generator.class_indices\n",
    "print('Train Label Dictionary : \\n',class_dictionary)\n",
    "\n",
    "with open('labels.pickle', 'wb') as f:\n",
    "    pickle.dump(class_dictionary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a TensorBoard Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = keras.callbacks.TensorBoard(\n",
    "#     log_dir='logs',\n",
    "#     histogram_freq=5,\n",
    "#     write_graph=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "#               metrics=['acc'])\n",
    "# # Train the model\n",
    "# history = model.fit_generator(\n",
    "#       train_generator,\n",
    "#       steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "#       epochs=24,\n",
    "#       validation_data=validation_generator,\n",
    "#       validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "#       verbose=1)\n",
    " \n",
    "# Save the model\n",
    "# model.save('faceID_VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "2/1 [====================================] - 7s 4s/step - loss: 5.2684 - acc: 0.0535\n",
      "Epoch 2/24\n",
      "2/1 [====================================] - 1s 496ms/step - loss: 2.7792 - acc: 0.1250\n",
      "Epoch 3/24\n",
      "2/1 [====================================] - 1s 628ms/step - loss: 2.4184 - acc: 0.2589\n",
      "Epoch 4/24\n",
      "2/1 [====================================] - 1s 572ms/step - loss: 2.1956 - acc: 0.3124\n",
      "Epoch 5/24\n",
      "2/1 [====================================] - 1s 496ms/step - loss: 2.0947 - acc: 0.1965\n",
      "Epoch 6/24\n",
      "2/1 [====================================] - 1s 498ms/step - loss: 1.6163 - acc: 0.4196\n",
      "Epoch 7/24\n",
      "2/1 [====================================] - 1s 598ms/step - loss: 1.5104 - acc: 0.5268\n",
      "Epoch 8/24\n",
      "2/1 [====================================] - 1s 589ms/step - loss: 1.1207 - acc: 0.5357\n",
      "Epoch 9/24\n",
      "2/1 [====================================] - 1s 555ms/step - loss: 1.4552 - acc: 0.4821\n",
      "Epoch 10/24\n",
      "2/1 [====================================] - 1s 522ms/step - loss: 0.8491 - acc: 0.8392\n",
      "Epoch 11/24\n",
      "2/1 [====================================] - 1s 573ms/step - loss: 0.5731 - acc: 0.8572\n",
      "Epoch 12/24\n",
      "2/1 [====================================] - 1s 539ms/step - loss: 0.4758 - acc: 0.9017\n",
      "Epoch 13/24\n",
      "2/1 [====================================] - 1s 586ms/step - loss: 0.2672 - acc: 0.9375\n",
      "Epoch 14/24\n",
      "2/1 [====================================] - 1s 570ms/step - loss: 0.1215 - acc: 1.0000\n",
      "Epoch 15/24\n",
      "2/1 [====================================] - 1s 496ms/step - loss: 0.1797 - acc: 0.9107\n",
      "Epoch 16/24\n",
      "2/1 [====================================] - 1s 563ms/step - loss: 0.6576 - acc: 0.8392\n",
      "Epoch 17/24\n",
      "2/1 [====================================] - 1s 494ms/step - loss: 0.3350 - acc: 0.9107\n",
      "Epoch 18/24\n",
      "2/1 [====================================] - 1s 584ms/step - loss: 0.1160 - acc: 1.0000\n",
      "Epoch 19/24\n",
      "2/1 [====================================] - 1s 587ms/step - loss: 0.3645 - acc: 0.8570\n",
      "Epoch 20/24\n",
      "2/1 [====================================] - 1s 548ms/step - loss: 0.3957 - acc: 0.9107\n",
      "Epoch 21/24\n",
      "2/1 [====================================] - 1s 539ms/step - loss: 0.1920 - acc: 0.9285\n",
      "Epoch 22/24\n",
      "2/1 [====================================] - 1s 493ms/step - loss: 0.1202 - acc: 0.9732\n",
      "Epoch 23/24\n",
      "2/1 [====================================] - 1s 516ms/step - loss: 0.0654 - acc: 1.0000\n",
      "Epoch 24/24\n",
      "2/1 [====================================] - 1s 603ms/step - loss: 0.1142 - acc: 0.9732\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=24,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Compatible Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'faceID_VGG16_gMLapi\\\\saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'faceID_VGG16_gMLapi\\\\saved_model.pb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_builder = tf.saved_model.builder.SavedModelBuilder(\"faceID_VGG16_gMLapi\")\n",
    "\n",
    "inputs = {\n",
    "    'input': tf.saved_model.utils.build_tensor_info(model.input)\n",
    "}\n",
    "outputs = {\n",
    "    'earnings': tf.saved_model.utils.build_tensor_info(model.output)\n",
    "}\n",
    "\n",
    "signature_def = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    ")\n",
    "\n",
    "# save structure and weight\n",
    "model_builder.add_meta_graph_and_variables(\n",
    "    K.get_session(),\n",
    "    tags=[tf.saved_model.tag_constants.SERVING],\n",
    "    signature_def_map={\n",
    "        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "    }\n",
    ")\n",
    "\n",
    "model_builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
